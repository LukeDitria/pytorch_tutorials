{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dg0l8uzpFLUp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "import copy\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 64\n",
    "lr = 2e-5\n",
    "\n",
    "train_epoch = 100\n",
    "\n",
    "# data_loader\n",
    "img_size = 32\n",
    "\n",
    "data_set_root = \"../../datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Use a GPU if avaliable </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QwzLOOlx6KxE"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "gpu_indx  = 0\n",
    "device = torch.device(gpu_indx if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(img_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=([0.5]), std=([0.5]))\n",
    "                                ])\n",
    "\n",
    "trainset = datasets.MNIST(data_set_root, train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "un8vZ4VHFLUt"
   },
   "source": [
    "<h3> Generator </h3>\n",
    "Our Generator is a simple transpose convolution network, it takes in a vector as an input and up samples it multiple times until it is the desired size.<br>\n",
    "NOTE: We use a 3D tensor for the input instead of a 1D vector as it is easier to just use transpose convolution layers for everything instead of having to reshape the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xP4N0Mr8FLUv"
   },
   "outputs": [],
   "source": [
    "class ConditionalNorm2d(nn.Module):\n",
    "    def __init__(self, channels, num_features, norm_type=\"gn\"):\n",
    "        super(ConditionalNorm2d, self).__init__()\n",
    "        self.channels = channels\n",
    "        if norm_type == \"bn\":\n",
    "            self.norm = nn.BatchNorm2d(channels, affine=False, eps=1e-4)\n",
    "        elif norm_type == \"gn\":\n",
    "            self.norm = nn.GroupNorm(8, channels, affine=False, eps=1e-4)\n",
    "        else:\n",
    "            raise ValueError(\"Normalisation type not recognised.\")\n",
    "\n",
    "        self.fcw = nn.Linear(num_features, channels)\n",
    "        self.fcb = nn.Linear(num_features, channels)\n",
    "\n",
    "    def forward(self, x, features):\n",
    "        out = self.norm(x)\n",
    "        w = self.fcw(features)\n",
    "        b = self.fcb(features)\n",
    "\n",
    "        out = w.view(-1, self.channels, 1, 1) * out + b.view(-1, self.channels, 1, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResDown(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual down sampling block for the encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_in, channel_out, kernel_size=3, num_features=128):\n",
    "        super(ResDown, self).__init__()\n",
    "        self.norm1 = ConditionalNorm2d(channel_in, num_features=num_features)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_out, kernel_size, 2, kernel_size // 2)\n",
    "        self.norm2 = ConditionalNorm2d(channel_out, num_features=num_features)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channel_out, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 2, kernel_size // 2)\n",
    "\n",
    "        self.act_fnc = nn.ELU()\n",
    "\n",
    "    def forward(self, x, features):\n",
    "        x = self.act_fnc(self.norm1(x, features))\n",
    "\n",
    "        skip = self.conv3(x)\n",
    "\n",
    "        x = self.act_fnc(self.norm2(self.conv1(x), features))\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x + skip\n",
    "\n",
    "\n",
    "class ResUp(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual up sampling block for the decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_in, channel_out, kernel_size=3, scale_factor=2, num_features=128):\n",
    "        super(ResUp, self).__init__()\n",
    "        self.norm1 = ConditionalNorm2d(channel_in, num_features=num_features)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_in, kernel_size, 1, kernel_size // 2)\n",
    "        self.norm2 = ConditionalNorm2d(channel_in, num_features=num_features)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "\n",
    "        self.up_nn = nn.Upsample(scale_factor=scale_factor, mode=\"nearest\")\n",
    "\n",
    "        self.act_fnc = nn.ELU()\n",
    "\n",
    "    def forward(self, x, features):\n",
    "        x = self.up_nn(self.act_fnc(self.norm1(x, features)))\n",
    "\n",
    "        skip = self.conv3(x)\n",
    "\n",
    "        x = self.act_fnc(self.norm2(self.conv1(x), features))\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x + skip\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_in, channel_out, kernel_size=3, num_features=128):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.norm1 = ConditionalNorm2d(channel_in, num_features=num_features)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_in, kernel_size, 1, kernel_size // 2)\n",
    "        self.norm2 = ConditionalNorm2d(channel_in, num_features=num_features)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "\n",
    "        if not channel_in == channel_out:\n",
    "            self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "\n",
    "        self.act_fnc = nn.ELU()\n",
    "\n",
    "    def forward(self, x_in, features):\n",
    "        x = self.act_fnc(self.norm1(x_in, features))\n",
    "\n",
    "        if self.conv3 is not None:\n",
    "            skip = self.conv3(x)\n",
    "        else:\n",
    "            skip = x_in\n",
    "\n",
    "        x = self.act_fnc(self.norm2(self.conv1(x), features))\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x + skip\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, ch=64, blocks=(1, 2, 4), num_features=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_in = nn.Conv2d(channels, blocks[0] * ch, 3, 1, 1)\n",
    "\n",
    "        widths_in = list(blocks)\n",
    "        widths_out = list(blocks[1:]) + [blocks[-1]]\n",
    "\n",
    "        self.layer_blocks = nn.ModuleList([])\n",
    "\n",
    "        for w_in, w_out in zip(widths_in, widths_out):\n",
    "            self.layer_blocks.append(ResDown(w_in * ch, w_out * ch, num_features=num_features))\n",
    "\n",
    "        self.block_out = ResBlock(w_out * ch, w_out * ch, num_features=num_features)\n",
    "\n",
    "        self.act_fnc = nn.ELU()\n",
    "\n",
    "    def forward(self, x, index_features):\n",
    "        x = self.conv_in(x)\n",
    "        skip_list = [x]\n",
    "\n",
    "        for block in self.layer_blocks:\n",
    "            x = block(x, index_features)\n",
    "            skip_list.append(x)\n",
    "        \n",
    "        x = self.block_out(x, index_features)\n",
    "        return x, skip_list\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder block\n",
    "    Built to be a mirror of the encoder block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, ch=64, blocks=(1, 2, 4), num_features=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        widths_out = list(blocks)[::-1]\n",
    "        widths_in = (list(blocks[1:]) + [blocks[-1]])[::-1]\n",
    "\n",
    "        self.block_in = ResBlock(blocks[-1] * ch, blocks[-1] * ch, num_features=num_features)\n",
    "        \n",
    "        self.layer_blocks = nn.ModuleList([])\n",
    "\n",
    "        for w_in, w_out in zip(widths_in, widths_out):\n",
    "            self.layer_blocks.append(ResUp(w_in * ch * 2, w_out * ch, num_features=num_features))\n",
    "\n",
    "        self.conv_out = nn.Conv2d(blocks[0] * ch * 2, channels, 3, 1, 1)\n",
    "        self.act_fnc = nn.ELU()\n",
    "\n",
    "    def forward(self, x_in, skip_list, index_features):\n",
    "        x = self.block_in(x_in, index_features)\n",
    "        \n",
    "        for block in self.layer_blocks:\n",
    "            skip = skip_list.pop()\n",
    "            x = torch.cat((x, skip), 1)\n",
    "            x = block(x, index_features)\n",
    "            \n",
    "        skip = skip_list.pop()\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        return self.conv_out(x)\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet network, uses the above encoder and decoder blocks\n",
    "    \"\"\"\n",
    "    def __init__(self, channel_in=3, ch=32, blocks=(1, 2, 4), timesteps=20, num_features=128):\n",
    "        super(Unet, self).__init__()\n",
    "        self.index_emb = nn.Embedding(timesteps, num_features)\n",
    "        self.encoder = Encoder(channel_in, ch=ch, blocks=blocks, num_features=num_features)\n",
    "        self.decoder = Decoder(channel_in, ch=ch, blocks=blocks, num_features=num_features)\n",
    "\n",
    "    def forward(self, x, index):\n",
    "        index_features = self.index_emb(index)\n",
    "        bottleneck, skip_list = self.encoder(x, index_features)\n",
    "        recon_img = self.decoder(bottleneck, skip_list, index_features)\n",
    "        return recon_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_alphas_bar(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, steps, steps)\n",
    "    alphas_bar = torch.cos(((x / steps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_bar = alphas_bar / alphas_bar[0]\n",
    "    return alphas_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_from_x0(curr_img, img_pred, alpha):\n",
    "    return (curr_img - alpha.sqrt() * img_pred)/((1 - alpha).sqrt() + 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_diffuse(diffusion_model, sample_in, total_steps):\n",
    "    diffusion_model.eval()\n",
    "    bs = sample_in.shape[0]\n",
    "    alphas = torch.flip(cosine_alphas_bar(total_steps), (0,)).to(device)\n",
    "    random_sample = copy.deepcopy(sample_in)\n",
    "    with torch.no_grad():\n",
    "        for i in trange(total_steps - 1):\n",
    "            index = (i * torch.ones(bs, device=sample_in.device)).long()\n",
    "\n",
    "            img_output = diffusion_model(random_sample, index)\n",
    "\n",
    "            noise = noise_from_x0(random_sample, img_output, alphas[i])\n",
    "            x0 = img_output\n",
    "\n",
    "            rep1 = alphas[i].sqrt() * x0 + (1 - alphas[i]).sqrt() * noise\n",
    "            rep2 = alphas[i + 1].sqrt() * x0 + (1 - alphas[i + 1]).sqrt() * noise\n",
    "\n",
    "            random_sample += rep2 - rep1\n",
    "\n",
    "        index = ((total_steps - 1) * torch.ones(bs, device=sample_in.device)).long()\n",
    "        img_output = diffusion_model(random_sample, index)\n",
    "\n",
    "    return img_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader itterable object\n",
    "dataiter = iter(train_loader)\n",
    "# Sample from the itterable object\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9820,
     "status": "ok",
     "timestamp": 1570750148112,
     "user": {
      "displayName": "Yunyan Xing",
      "photoUrl": "",
      "userId": "15587527606127278468"
     },
     "user_tz": -660
    },
    "id": "FL0LtpmwFLVA",
    "outputId": "9f67f2bd-db73-4b7d-be3e-365776d09ede"
   },
   "outputs": [],
   "source": [
    "timesteps = 50\n",
    "\n",
    "# network\n",
    "u_net = Unet(channel_in=images.shape[1], ch=32, blocks=(1, 2, 4), timesteps=timesteps).to(device)\n",
    "\n",
    "#A fixed latent noise vector so we can see the improvement over the epochs\n",
    "fixed_latent_noise = torch.randn(images.shape[0], 1, img_size, img_size).to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(u_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gbwhfk6AFLVD"
   },
   "outputs": [],
   "source": [
    "loss_log = []\n",
    "mean_loss = 0\n",
    "\n",
    "alphas = torch.flip(cosine_alphas_bar(timesteps), (0,)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(alphas.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = images.shape[0]\n",
    "images = images.to(device)\n",
    "\n",
    "rand_index = torch.randint(timesteps, (bs, ), device=device)\n",
    "\n",
    "alpha_batch = alphas[rand_index].reshape(bs, 1, 1, 1)\n",
    "\n",
    "noise_input = alpha_batch.sqrt() * images + (1 - alpha_batch).sqrt() * fixed_latent_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(torch.clamp(noise_input, -1, 1)[:16].cpu(), 8, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1021,
     "status": "error",
     "timestamp": 1570754566661,
     "user": {
      "displayName": "Yunyan Xing",
      "photoUrl": "",
      "userId": "15587527606127278468"
     },
     "user_tz": -660
    },
    "id": "IVBfpjOkFLVH",
    "outputId": "adfcd384-2571-4071-8c30-ad2f66a5f2f1"
   },
   "outputs": [],
   "source": [
    "pbar = trange(train_epoch, leave=False, desc=\"Epoch\")    \n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Loss: %.4f' % (mean_loss/len(train_loader)))\n",
    "    mean_loss = 0\n",
    "\n",
    "    for num_iter, (images, labels) in enumerate(tqdm(train_loader, leave=False)):\n",
    "\n",
    "        images = images.to(device)\n",
    "        \n",
    "        #the size of the current minibatch\n",
    "        bs = images.shape[0]\n",
    "\n",
    "        rand_index = torch.randint(timesteps, (bs, ), device=device)\n",
    "        random_sample = torch.randn_like(images)\n",
    "        alpha_batch = alphas[rand_index].reshape(bs, 1, 1, 1)\n",
    "        \n",
    "        noise_input = alpha_batch.sqrt() * images + (1 - alpha_batch).sqrt() * random_sample\n",
    "\n",
    "        img_pred = u_net(noise_input, rand_index)\n",
    "        \n",
    "        loss = F.l1_loss(img_pred, images)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #log the generator training loss\n",
    "        loss_log.append(loss.item())\n",
    "        mean_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(img_pred.detach().cpu(), nrow=16, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1570754571230,
     "user": {
      "displayName": "Yunyan Xing",
      "photoUrl": "",
      "userId": "15587527606127278468"
     },
     "user_tz": -660
    },
    "id": "a753TY7Z6KxV",
    "outputId": "6b7ebf6d-1354-4776-fe13-9b6f7e18eeae"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_log[1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1197,
     "status": "ok",
     "timestamp": 1570754573085,
     "user": {
      "displayName": "Yunyan Xing",
      "photoUrl": "",
      "userId": "15587527606127278468"
     },
     "user_tz": -660
    },
    "id": "VdN0meY8FLVK",
    "outputId": "1c97838e-e848-4016-863d-b14d9ddc2b8d"
   },
   "outputs": [],
   "source": [
    "fake_sample = cold_diffuse(u_net, fixed_latent_noise, total_steps=timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(fake_sample.detach().cpu(), nrow=16, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DCGAN_MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
