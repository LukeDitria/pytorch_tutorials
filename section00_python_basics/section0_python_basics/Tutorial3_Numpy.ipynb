{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/NumPy_logo.svg/775px-NumPy_logo.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Numpy and python </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Introduction </h2> <br>\n",
    "NumPy (Numerical python) is a very popular Python library with a huge number of functions and utilities.\n",
    "Later when we introduce Pytorch (The Deep Learning Framework used in this unit) we will see that there are many similarities between it and Numpy. In this way Numpy is our stepping stone between basic Python and Pytorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Python recap </h3> <br>\n",
    "Lets use pure Python to perform some matrix opperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create some \"Matrices\" as lists of lists  \n",
    "\n",
    "#3x3\n",
    "W = [[1, 1, 1],\n",
    "     [1.5, 1.5, 1.5],\n",
    "     [2, 2, 2]]\n",
    "\n",
    "#3x1\n",
    "x = [[6], [7], [8]]\n",
    "#3x1\n",
    "b = [[1], [1], [1]]\n",
    "\n",
    "#Variable to store output\n",
    "#3x1\n",
    "y = [[0], [0], [0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now compute Wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First lets compute W*x\n",
    "for i in range(len(W)):\n",
    "    for j in range(len(x)):\n",
    "        y[i][0] += W[i][j] * x[j][0]\n",
    "        \n",
    "#now lets add b\n",
    "for i in range(len(y)):\n",
    "    y[i][0] += b[i][0]\n",
    "    \n",
    "#print out the result\n",
    "print(\"Output:\\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Introducing.... NumPy! </h3> <br>\n",
    "See how tedious matrix operations are with pure python!?<br> Instead of trying to create our own functions (which is often a waste of time as other implementations will often be much better than our own) let's use Numpy to do the same thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we must import the numpy library! \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can transform our list of lists into a \"numpy array\" by using the function \"array\"\n",
    "W_np = np.array(W)\n",
    "\n",
    "x_np = np.array(x)\n",
    "\n",
    "#lets use the function \"ones\" to create an array of ones!\n",
    "b_np = np.ones((3, 1))\n",
    "\n",
    "#Lets now compute Wx + b using these numpy variables!\n",
    "output = np.matmul(W_np, x_np) + b_np\n",
    "\n",
    "#print out the result\n",
    "print(\"Output:\\n\", output)\n",
    "print(\"Output shape:\\n\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Tensors aka Multi-Dimensional Arrays </h2> <br>\n",
    "We've seen how we can create a \"matrix\" using np.array and a list of lists, lets have a little bit of a closer look at these multidemensional \"arrays\" also known as \"Tensors\"\n",
    "<br>\n",
    "eg:<br>\n",
    "A \"Scalar\" is a 0D Tensor<br>\n",
    "A \"Vector\" is a 1D Tensor<br>\n",
    "A \"Matrix\" is a 2D Tensor etc etc<br>\n",
    "An understanding of Tensors will be essential when we move to pytorch!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets use numpy's \"random\" function (a part of the np.random module) to create a 3D Tensor!\n",
    "T = np.random.random((2,3,4))\n",
    "#Lets print it out!\n",
    "print(\"Our 3D Tensor:\\n\", T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above print out we can visualise what our 3D tensor looks like, we can pretend (for this Tensor) that it is just 2, 3x4 matrices stacked together. This interpretation can be useful when we move to higher dimensional Tensors later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Basic Element-wise Operations </h2> <br>\n",
    "Lets see how we can perform some basic \"Element-wise\" operations on our numpy arrays (aka our Tensors)<br>\n",
    "Note: By \"Element wise\" we mean that the operation is applied independently to every value, as opposed to something like a matrix operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a 1D Tensor using \"arange\"\n",
    "y = np.arange(10)\n",
    "#this will create a \"Vector\" of numbers from 0 to 10\n",
    "print(\"Our 1D Tensor:\\n\",y)\n",
    "\n",
    "#We can perform normal python scalar arithmetic on numpy arrays\n",
    "print(\"\\nScalar Multiplication:\\n\",y * 10)\n",
    "print(\"Addition and Square:\\n\",(y + 1)**2)\n",
    "print(\"Addition:\\n\",y + y)\n",
    "print(\"Addition and division:\\n\",y / (y + 1))\n",
    "\n",
    "#We can use a combination of numpy functions and normal python arithmetic\n",
    "print(\"\\nPower and square root:\\n\",np.sqrt(y**2))\n",
    "\n",
    "#Numpy arrays are objects and have functions\n",
    "print(\"\\nY -\\n Min:%.2f\\n Max:%.2f\\n Standard Deviation:%.2f\\n Sum:%.2f\" %(y.min(), y.max(), y.std(), y.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Matrix opperations </h2> <br>\n",
    "Now let's use some Numpy functions to perform some real matrix opperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Matrix-1\n",
    "matrix_1 = np.random.random((3,3))\n",
    "#Create Matrix-2\n",
    "matrix_2 = np.random.random((3,3))\n",
    "\n",
    "#Add the 2 Matrices\n",
    "print(\"Addition:\\n\",np.add(matrix_1,matrix_2))\n",
    "\n",
    "#Subtraction\n",
    "print(\"Subtraction:\\n\",np.subtract(matrix_1,matrix_2))\n",
    "\n",
    "#Multiplication\n",
    "print(\"Multiplication:\\n\",np.matmul(matrix_1,matrix_2))\n",
    "\n",
    "print(\"\\nFor Matrix 1:\")\n",
    "#Calculate its inverse\n",
    "print(\"The inverse is:\\n\",np.linalg.inv(matrix_1))\n",
    "\n",
    "#Transpose the matrix\n",
    "print(\"The Transpose is:\\n\", matrix_1.T)\n",
    "\n",
    "#Calculate the Determinant\n",
    "print(\"The Determinant is:\\n\", np.linalg.det(matrix_1))\n",
    "\n",
    "#Print the Trace\n",
    "print(\"The Trace is:\\n\", matrix_1.trace())\n",
    "\n",
    "#Calculate the Rank\n",
    "print(\"The Rank is:\\n\", np.linalg.matrix_rank(matrix_1))\n",
    "\n",
    "# Calculate the Eigenvalues and Eigenvectors of that Matrix\n",
    "eigenvalues ,eigenvectors=np.linalg.eig(matrix_1)\n",
    "\n",
    "print(\"The Eigenvalues are:\\n\",eigenvalues)\n",
    "print(\"The Eigenvectors are:\\n\",eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Tensor Manipulation </h2>\n",
    "<b>Being able to index and change the shape of Tensors is one of the most important skills that you will need to learn going forward!<br>\n",
    "I cannot overstate the importance of developing an intuative understanding of the following!!</b>\n",
    "<br>\n",
    "<h3> Indexing </h3> <br>\n",
    "Just like with lists, indexing matrices (or Tensors) is very important, we can do the same with numpy arrays!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Vectors! </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a vector\n",
    "vector = np.array([ 1,2,3,4,5,6 ])\n",
    "print(\"Our Vector:\\n\",vector)\n",
    "\n",
    "#Select all elements of a vector\n",
    "print(\"All elements:\\n\",vector[:])\n",
    "\n",
    "#Select everything up to and including the 3rd element\n",
    "print(\"Within a range:\\n\",vector[:3])\n",
    "\n",
    "#Select the everything after the 3rd element\n",
    "print(\"Another range:\\n\",vector[3:])\n",
    "\n",
    "#Select the last element\n",
    "print(\"The last element:\\n\",vector[-1])\n",
    "\n",
    "#Select 3rd element of Vector\n",
    "#INDEXING STARTS AT 0\n",
    "print(\"3rd Element:\\n\",vector[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Matrix! </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Matrix\n",
    "matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(\"Our Matrix:\\n\",matrix)\n",
    "\n",
    "#Select 2nd row 2nd column\n",
    "print(\"A single element:\\n\",matrix[1,1])\n",
    "\n",
    "#Select the first 2 rows and all the columns of the matrix\n",
    "print(\"Index rows with all columns:\\n\",matrix[:2,:])\n",
    "\n",
    "#Select all rows and the 2nd column of the matrix\n",
    "print(\"Index columns with all rows:\\n\",matrix[:,:2])\n",
    "\n",
    "#Select rows 1 and 2 and column 2 and 3 of the matrix\n",
    "print(\"Index columns and rows:\\n\",matrix[:2,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Describing Tensors </h3> <br>\n",
    "Let's see how we can view the characteristics of our Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a large 3D Tensor\n",
    "Tensor = np.random.randint(0, 10, (3, 5, 5))\n",
    "\n",
    "#View the Number of elements in every dimension\n",
    "print(\"The Tensors shape is:\", Tensor.shape)\n",
    "\n",
    "#View the number of elements in total\n",
    "print(\"The Tensors size is:\", Tensor.size)\n",
    "\n",
    "#View the number of Dimensions(2 in this case)\n",
    "print(\"There are %d Dimensions\" %(Tensor.ndim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Reshaping </h3> <br>\n",
    "We can change a Tensor to one of the same size (same number of elements) but a different shape by using some Numpy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Origional Tensor:\\n\", Tensor)\n",
    "\n",
    "#We can also use the Flatten method to convert to a 1D Tensor\n",
    "print(\"Flatten to a 1D Tensor:\\n\",Tensor.flatten())\n",
    "\n",
    "#Let us reshape our Tensor to a 2D Tensor\n",
    "print(\"Reshape to 3x25:\\n\", Tensor.reshape(3,25))\n",
    "\n",
    "#Here the -1 tells Numpy to put as many elements as it needs here in order to maintain the given dimention sizes\n",
    "#AKA \"I don't care the size of this dimention as long as the first one is 3\"\n",
    "print(\"Reshape to 5xwhatever:\\n\",Tensor.reshape(5,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plotting with matplotlib</h3>\n",
    "We've already seen the matplotlib library and used it to plot out lists of data. The matplotlib library works very closely with numpy and can plot out numpy arrays  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sine wave using numpy functions and matplotlib\n",
    "#create an array of 1000 points from 0 - 100\n",
    "x = np.linspace(0, 100, 1000)\n",
    "#calculate the sine of the points\n",
    "y = np.sin(x)\n",
    "#create a line plot\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create an image as a 2D array\n",
    "img = np.zeros((60, 60))\n",
    "#Set some points to 1\n",
    "img[10:20, (15, 45)] = 1\n",
    "img[35:45, (15, 45)] = 1\n",
    "img[45, 15:46] = 1\n",
    "\n",
    "#imshow can display numpy arrays as colour (3D array - HxWxC) or grayscale images (2D array - HxW)!\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Broadcasting </h2>\n",
    "<b>Important to know!</b> <br>\n",
    "Broadcasting is a powerful tool that lets us perform element wise matrix or vector operations across higher dimensional Tensors. <br>\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Thanh_Dang_Diep/publication/326377197/figure/fig1/AS:647925902368772@1531488981904/Broadcasting-in-NumPy.png)\n",
    "\n",
    "Lets see what we mean by this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create 2 differently shaped 2D Tensors (Matrices)\n",
    "Tensor1 = np.random.randint(0, 10, (1, 4))\n",
    "Tensor2 = np.random.randint(0, 10, (2, 1))\n",
    "\n",
    "print(\"Tensor 1:\\n\", Tensor1)\n",
    "print(\"With shape:\\n\", Tensor1.shape)\n",
    "\n",
    "print(\"\\nTensor 2:\\n\", Tensor2)\n",
    "print(\"With shape:\\n\", Tensor2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from high school days that there is no way we can perform a normal matrix addition on these two matrices, so when we try Numpy should give us an error right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor3 = np.add(Tensor1, Tensor2)\n",
    "\n",
    "print(\"The resulting Tensor:\\n\", Tensor3)\n",
    "print(\"The resulting shape is:\\n\", Tensor3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT!?! A 1x4 Matrix added a 2x1? resulting in a 2x4 Matrix, What did Numpy do here?<br>\n",
    "Well, as suggested Numpy is NOT performing a normal Matrix addition. Instead Numpy is performing a broadcast operation, THEN a Matrix addition. <br>\n",
    "So then, what is Broadcasting? <br>\n",
    "Let's look again at the shape of those two 2D Tensors and the resulting Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tensor 1 shape:\\n\", Tensor1.shape)\n",
    "print(\"Tensor 2 shape:\\n\", Tensor2.shape)\n",
    "print(\"Resulting Tensor shape:\\n\", Tensor3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the resulting shape of the Tensor addition seems to come from the larger dimensions of the multiplication<br>\n",
    "1x<b>4</b>+<b>2</b>x1 = <b>4x2</b> <br>\n",
    "During the \"Broadcast\" operation Numpy \"repeats\" (Broadcasts) dimensions of the two Tensors so that they are the same shape, and then performs the addition. <br>\n",
    "Let's do this manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat entries of dim0 2 times\n",
    "Tensor1_repeated = Tensor1.repeat(2, 0)\n",
    "#Repeat entries of dim1 4 times\n",
    "Tensor2_repeated = Tensor2.repeat(4, 1)\n",
    "\n",
    "print(\"Tensor 1:\\n\", Tensor1_repeated)\n",
    "print(\"Tensor 1 shape:\\n\", Tensor1_repeated.shape)\n",
    "\n",
    "print(\"\\nTensor 2:\\n\", Tensor2_repeated)\n",
    "print(\"Tensor 2 shape:\\n\", Tensor2_repeated.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've repeated the SMALLER corresponding dimension to that of the larger dimension.<br>\n",
    "NOTE: You can only broadcast Tensors if dimensions are indices multiples of each other!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets add the two resulting Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor3_repeated = np.add(Tensor1_repeated, Tensor2_repeated)\n",
    "print(\"Tensor 3:\\n\", Tensor3_repeated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the resulting Tensor is the same as the result from the initial addition!<br>\n",
    "NOTE: Numpy does not actually repeat the stored memory when performing the operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Broadcasting use case</b><br>\n",
    "Lets see WHY we would use broadcasting with an example.<br>\n",
    "Letâ€™s create a \"dataset\" of 100 data points, each datapoint will be a vector of 10 values (lets imagine each value could represent some measured \"quantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a random dataset\n",
    "data = np.random.randint(0, 100, (100, 10))\n",
    "print(\"Data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use broadcasting to normalise every datapoint (i) independently for every quantity (q) aka we need to find the mean and standard deviation of every \"quantity\" (q) and the perform a normalisation<br>\n",
    "\\begin{equation*}\n",
    "Xnorm_i^q = \\frac{(x_i^q -\\mu^q)}{\\sigma^q}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the mean across the 0th dimension\n",
    "#Aka find the mean of each quantity over the 100 datapoint\n",
    "#This should give us 10 means\n",
    "mean_vector = data.mean(0).reshape(1, 10)\n",
    "#Find the standard deviation across the 0th dimension\n",
    "std_vector = data.std(0).reshape(1, 10)\n",
    "\n",
    "print(\"Quantity mean:\\n\", mean_vector)\n",
    "print(\"Quantity std:\\n\", std_vector.round(2))\n",
    "\n",
    "print(\"mean shape:\\n\", mean_vector.shape)\n",
    "print(\"std shape:\\n\", std_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perform the normalisation on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm = (data-mean_vector)/std_vector\n",
    "print(\"Normalised dataset  shape:\\n\", data_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using our knowledge of broadcasting we don't have to use loops and indexing to perform operations!<br>\n",
    "This is incredibly useful as matrix (Tensor) operations are MUCH faster than iterative loops and MUCH MUCH faster when we start to use GPUs and operations are performed in parallel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
