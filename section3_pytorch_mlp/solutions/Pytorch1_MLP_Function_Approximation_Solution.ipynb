{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> Approximating a Region of the Sine Function </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will approximate a region of the sine function with a neural network to get a sense of how architecture and hyperparameters affect neural network performance. We will be introducing Pytorch Neural Network structure, Pytorch datasets and dataloaders and optimizers\n",
    "\n",
    "<img src=\"../data/sine_wave.gif\" width=\"1200\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pytorch Datasets and Dataloaders </h2>\n",
    "<b> *Basics only, we will cover more detail later on, but for now...</b><br>\n",
    "Pytorch has a huge number of functionalities that make training our neural networks very easy! One of those functionalities is the Pytorch dataset and dataloader (they are real life-savers!). The \"dataset\" class is an object that \"stores\" our dataset either directly (it loads all the data in the initialisation function) or indirectly (it loads the image paths during the initialisation function and only loads them when it needs to - for large image-based datasets this is usually the only way to do it).We will see how we can create our own Pytorch dataset soon!<br>\n",
    "These datasets then are used to create a \"dataloader\" object that is \"iterable\". The Pytorch dataloader will take our dataset and randomly shuffle it (if we tell it to), it will also divide the dataset into \"mini-batches\" which are groups of datapoints of a fixed size (the batch size). Our Neural Network is then trained through a single step of GD on this mini-batch. As we iterate through the dataloader, the dataloader will pass us a new unique mini-batch until the whole dataset has been passed to us. One whole loop through the dataset is called an \"epoch\", during every epoch the dataset is re-shuffled so the mini-batches are all random. This random sampling of the dataset and training on mini-batches (instead of performing GD on the whole dataset) is called Stochastic Gradient Descent (SGD)<br>\n",
    "Note: If the whole dataset does not evenly divide into mini batches then in the last iterator we will just be passed whatever is left over!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating a Pytorch dataset </h3>\n",
    "The dataset we will be creating will be points from a \"noisey\" sine wave.<br>\n",
    "The Pytorch dataset class has three essential parts:<br>\n",
    "The __init__ function (as most Python classes do)<br>\n",
    "The __getitem__ function (this is called during every iteration)<br>\n",
    "The __len__ function (this must return the length of the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"SineDataset\" class by importing the Pytorch Dataset class\n",
    "class SineDataset(Dataset):\n",
    "    \"\"\" Data noisey sinewave dataset\n",
    "        num_datapoints - the number of datapoints you want\n",
    "    \"\"\"\n",
    "    def __init__(self, num_datapoints):\n",
    "        # Lets generate the noisy sinewave points\n",
    "        \n",
    "        # Create \"num_datapoints\" worth of random x points using a uniform distribution (0-1) using torch.rand\n",
    "        # Then scale and shift the points to be between -9 and 9\n",
    "        self.x_data = torch.rand(num_datapoints,1)*18 - 9 \n",
    "        \n",
    "        # Calculate the sin of all data points in the x vector and the scale amplitude\n",
    "        self.y_data = (torch.sin(self.x_data))/2.5\n",
    "        \n",
    "        # Add some gaussein noise to each datapoint using torch.randn_like\n",
    "        # Note:torch.randn_like will generate a tensor of gaussein noise the same size \n",
    "        # and type as the provided tensor\n",
    "        self.y_data += torch.randn_like(self.y_data)/20\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # This function is called by the dataLOADER class whenever it wants a new mini-batch\n",
    "        # The dataLOADER class will pass the dataSET and number of datapoint indexes (mini-batch of indexes)\n",
    "        # It is up to the dataSET's __getitem__ function to output the corresponding input datapoints \n",
    "        #AND the corresponding labels\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "        # Note:Pytorch will actually pass the __getitem__ function one index at a time\n",
    "        # If you use multiple dataLOADER \"workers\" multiple __getitem__ calls will be made in parallel\n",
    "        # (Pytorch will spawn multiple threads)\n",
    "\n",
    "    def __len__(self):\n",
    "        # We also need to specify a \"length\" function, Python will use this fuction whenever\n",
    "        # You use the Python len(function)\n",
    "        # We need to define it so the dataLOADER knows how big the dataSET is!\n",
    "        return self.x_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our dataset, lets create an instance of it for training and testing and then create  dataloaders to make it easy to iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x_train = 30000   # the number of training datapoints\n",
    "n_x_test = 8000     # the number of testing datapoints\n",
    "batch_size = 16\n",
    "\n",
    "# Create an instance of the SineDataset for both the training and test set\n",
    "dataset_train = SineDataset(n_x_train)\n",
    "dataset_test  = SineDataset(n_x_test)\n",
    "\n",
    "# Now we need to pass the dataSET to the Pytorch dataLOADER class along with some other arguments\n",
    "# batch_size - the size of our mini-batches\n",
    "# shuffle - whether or not we want to shuffle the dataset\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualise the dataset we've created!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "plt.scatter(dataset_train.x_data, dataset_train.y_data, s=0.2)\n",
    "# Note:see here how we can just directly access the data from the dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Neural Network Architecture</h2>\n",
    "<b> Non-Linear function approximators! </b> <br>\n",
    "\n",
    "Up until now we have only created a single linear layer with an input layer and an output layer. In this section we will start to create multi-layered networks with many \"hidden\" layers separated by \"activation functions\" that give our networks \"non-linearities\". If we didn't have these activation functions and simply stacked layers together, our network would be no better than a single linear layer! Why? Because multiple sequential \"linear transformations\" can be modeled with just a single linear transformation. This is easiest to understand with matrix multiplications (which is exactly what happens inside a linear layer).<br>\n",
    "\n",
    "$M_o = M_i*M_1*M_2*M_3*M_4*M_5$<br>\n",
    "Is the same as<br>\n",
    "$M_o = M_i*M_T$<br>\n",
    "Where<br>\n",
    "$M_T = M_1*M_2*M_3*M_4*M_5$<br>\n",
    "\n",
    "Aka multiplication with several matrices can be simplified to multiplication with a single matrix.<br>\n",
    "\n",
    "So what are these nonlinear activation functions that turn our simple linear models into a power \"nonlinear function approximator\"? Some common examples are:<br>\n",
    "1. relu\n",
    "2. sigmoid\n",
    "3. tanh\n",
    "\n",
    "Simply put they are \"nonlinear\" functions, the simplest of which is the \"rectified linear unit\" (relu) which is \"piecewise non-linear\".\n",
    "\n",
    "NOTE: The term \"layer\" most commonly refers to the inputs or outputs of the weight matrix or activations functions and not the linear layer or activation layer themselves. Output layers in between two \"linear layers\" are called \"hidden layers\". You can imagine them \"inside\" the neural network with us only being able to see the input and output layers. To confuse things even further the outputs of activation functions are also commonly called \"activations\"\n",
    "\n",
    "Why do we want a linear function approximator? Because many processes, tasks, systems in the real world are non-linear. \"Linear\" in basic terms refers to any process that takes inputs, scales them and sums them together to get an output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pytorch nn.Module</h3>\n",
    "Now we can define a Pytorch model to be trained!<br>\n",
    "To do so we use the Pytorch nn.Module class as the base for defining our network. Just like the dataset class, this class has a number of important functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our network class by using the nn.module\n",
    "class ShallowLinear(nn.Module):\n",
    "    '''\n",
    "    A simple, general purpose, fully connected network\n",
    "    '''\n",
    "    # Here we initialise our network and define all the layers we need\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        # Perform initialization of the pytorch superclass, this will allow us to inherit \n",
    "        # functions from the nn.Module class\n",
    "        super(ShallowLinear, self).__init__()\n",
    "        # Define the linear layers we will need\n",
    "        # Note: the output of one layer must be the same size \n",
    "        # as the input to the next!\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear4 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This function is an important one and we must create it or pytorch will give us an error!\n",
    "        # This function defines the \"forward pass\" of our neural network\n",
    "        # and will be called when we simply call our network class\n",
    "        # aka we can do net(input) instead of net.forward(input)\n",
    "        \n",
    "        # Lets define the sqeuence of events for our forward pass!\n",
    "        x = self.linear1(x) # hidden layer\n",
    "        x = torch.tanh(x)   # activation function\n",
    "        \n",
    "        x = self.linear2(x) # hidden layer\n",
    "        x = torch.tanh(x)   # activation function\n",
    "        \n",
    "        x = self.linear3(x) # hidden layer\n",
    "        x = torch.tanh(x)   # activation function\n",
    "\n",
    "        # No activation function on the output!!\n",
    "        x = self.linear4(x) # output layer\n",
    "        \n",
    "        # Note we re-use the variable x as we don't care about overwriting it \n",
    "        # though in later labs we will want to use earlier hidden layers\n",
    "        # later in our network!\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters,  model and optimizer\n",
    "\n",
    "Here we define the following parameters for training:\n",
    "\n",
    "- batch size (which has already been defined)\n",
    "- learning rate\n",
    "- number of training epochs\n",
    "- optimizer\n",
    "- loss function\n",
    "\n",
    "Ideally, numeric parameters would be tested empirically with an exhaustive search. When testing manually, It is recommended to maximize the model fit with one parameter at a time to avoid confounding your results. \n",
    "\n",
    "Try these learning rates:\n",
    "- 5e-2, 1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5\n",
    "\n",
    "Try these optimizers:\n",
    "- `optim.SGD(shallow_model.parameters(), lr=learning_rate)`\n",
    "- `optim.Adam(shallow_model.parameters(), lr=learning_rate)`\n",
    "\n",
    "[Youtube: Optimizers - EXPLAINED! by CodeEmporium](https://youtu.be/mdKjMPmcWjY?si=VtjuF_QlPHAzj2Sx)\n",
    "\n",
    "See the pytorch documentation pages for an extensive list of options:\n",
    "- Optimizers: http://pytorch.org/docs/master/optim.html#algorithms\n",
    "- Loss: http://pytorch.org/docs/master/nn.html#id46\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "learning_rate = 5e-4\n",
    "nepochs = 10\n",
    "\n",
    "# Create model\n",
    "shallow_model = ShallowLinear(input_size=1, output_size=1, hidden_size=64)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(shallow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.MSELoss()  # mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate training, plot testing results\n",
    "Here we put all the previous methods together to train and test the model. This problem is an unusual one in that our loss is the best quantitative metric of the model performance. Classification problems require further analysis of true/false positives/negatives.\n",
    "\n",
    "Rerun this cell several times without editing any parameters. Is the result the same?\n",
    "\n",
    "Try a larger batch size, how is the training time affected?\n",
    "\n",
    "Look at the slope and noise level of the loss plot. Does it look like the training converged on a local minimum?\n",
    "\n",
    "Try some different hyperparameters and see how accurate you can get your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create two lists to store the loss values from training and testing\n",
    "training_loss_logger = []\n",
    "testing_loss_logger = []\n",
    "# Note: create them outside of the train/test cell so they don't get overwritten \n",
    "# if we want to run the cell again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main train/test cell\n",
    "# This will run one epoch of training and then one epoch of testing nepochs times\n",
    "\n",
    "for epoch in trange(nepochs, desc=\"Epochs\", leave=False):\n",
    "    \n",
    "    # Perform training Loop!\n",
    "    for x, y in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "\n",
    "        # Run forward calculation\n",
    "        y_predict = shallow_model(x)\n",
    "\n",
    "        # Compute loss.\n",
    "        loss = loss_fn(y_predict, y)\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable weights\n",
    "        # of the model)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the loss so we can visualise the training plot later\n",
    "        training_loss_logger.append(loss.item())\n",
    "\n",
    "    # Perform a test Loop!\n",
    "    with torch.no_grad():\n",
    "        test_loss_accum = 0\n",
    "        for i, (x, y) in enumerate(tqdm(data_loader_test, desc=\"Testing\", leave=False)):\n",
    "\n",
    "            # Run forward calculation\n",
    "            y_predict = shallow_model(x)\n",
    "            # Compute loss.\n",
    "            loss = loss_fn(y_predict, y)\n",
    "            # Log the loss so we can visualise the training plot later\n",
    "            testing_loss_logger.append(loss.item())\n",
    "            test_loss_accum += loss\n",
    "            \n",
    "        test_loss_accum /= (i + 1)\n",
    "        \n",
    "print(\"Epoch [%d/%d], Average Test Loss %.4f\" %(epoch, nepochs, test_loss_accum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lets visualise our results!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "plt.plot(np.linspace(0, nepochs, len(training_loss_logger)), training_loss_logger)\n",
    "plt.plot(np.linspace(0, nepochs, len(testing_loss_logger)), testing_loss_logger)\n",
    "title = plt.title(\"Training and Testing Loss Vs Epoch\")\n",
    "legend = plt.legend([\"Training\", \"Testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = []\n",
    "# Perform a test Loop!\n",
    "with torch.no_grad():\n",
    "    for x, y in data_loader_test:\n",
    "        # Run forward calculation\n",
    "        y_predict = shallow_model(x)\n",
    "        test_outputs.append(y_predict.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the testdata against the model's predicted outputs, how accurate is it?\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "plt.scatter(dataset_test.x_data, dataset_test.y_data, s=0.2)\n",
    "plt.scatter(dataset_test.x_data, test_outputs, s=0.2)\n",
    "plt.text(-9, 0.44, \"- Prediction\", color=\"orange\", fontsize=16)\n",
    "plt.text(-9, 0.48, \"- Sine wave (with noise)\", color=\"blue\", fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
