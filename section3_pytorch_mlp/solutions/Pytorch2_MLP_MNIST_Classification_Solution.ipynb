{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> PyTorch Classification with MLP </h1>\n",
    "Lets see how we can train our first neural network using the Pytorch funcunalities we have previously seen! In this notebook we will be training a Multilayer Perceptron (MLP) with the MNIST dataset. We will see how to use Pytorch inbuilt datasets, how to construct a MLP using the Pytorch nn.module class and how to construct a training and testing loop to perform stochastic gradient descent (SGD).\n",
    "\n",
    "<img src=\"../data/MNIST.gif\" width=\"700\" align=\"center\">\n",
    "\n",
    "Animation of MNIST digits and a MLP's activations changing via learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Download the MNIST Train and Test set </h2>\n",
    "The MNIST dataset is a large database of handwritten digits that is commonly used for training and testing in the field of machine learning, it consists of 60,000 training images and 10,000 testing images as well as the corresponding digit class (0-9) (it has moved out of fashion these days because it is \"too easy\" to learn though it is still used at times as a \"proof of concept\").  <br>\n",
    "Pytorch has constructed a number of \"dataset\" classes that will automatically download various datasets making it very easy for us to train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "#Create a train and test dataset using the Pytorch MNIST dataloader class\n",
    "train = MNIST('../data', train=True,  download=True, transform=transforms.ToTensor())\n",
    "test  = MNIST('../data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "#Using the Pytorch dataloader class and the Pytorch datasets we with create itterable dataloader objects\n",
    "train_loader = dataloader.DataLoader(train, shuffle=True, batch_size=batch_size, num_workers=0, pin_memory=False) \n",
    "test_loader = dataloader.DataLoader(test, shuffle=False, batch_size=batch_size, num_workers=0, pin_memory=False)\n",
    "\n",
    "#NOTE:num_workers is the number of extra threads the dataloader will spawn to load the data from file, \n",
    "#you will rarely need more than 4 \n",
    "#NOTE!!! ON WINDOWS THERE CAN BE ISSUES WITH HAVING MORE THAN 0 WORKERS!! IF YOUR TRAINING LOOP STALLS AND DOES\n",
    "#NOTHING SET num_workers TO 0!\n",
    "\n",
    "#NOTE:pin_memory is only useful if you are training with a GPU!!!! If it is True then the GPU will pre-allocate\n",
    "#memory for the NEXT batch so the CPU-GPU transfer can be handled by the DMA controller freeing up the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set device to GPU_indx if GPU is avaliable\n",
    "#GPU index is set by the NVIDIA Drivers if you have only one GPU then it should always be 0\n",
    "GPU_indx = 0\n",
    "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Visualize a few training samples </h3>\n",
    "Lets visualise that mini-batches that the dataloader gives us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can create an itterater using the dataloaders and take a random sample \n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"The input data shape is :\\n\", images.shape)\n",
    "print(\"The target output data shape is :\\n\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that (as specified) our mini-batch is 256. The dataloader has passed us a 4D Tensor as input data, the first dimension (d0) is known as the \"batch dimension\" (B) the other three are the image dimensions (CxHxW). We can this of this 4D Tensor as a stack of 256, 1 channel, 28x28 images.<br>\n",
    "The image labels are a 1D Tensor, 1 single scalar value per image (per mini-batch \"instance\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(images, 32)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our Neural Network Model \n",
    "We define our model using the torch.nn.Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a simple MLP network similar to the sine wave approximator\n",
    "class Simple_MLP(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Simple_MLP, self).__init__()\n",
    "        #We will use 4 linear layers\n",
    "        #The input to the model is 784 (28x28 - the image size)\n",
    "        #and the should be num_classes outputs\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #the data we pass the model is a batch of single channel images\n",
    "        #with shape BSx1x28x28 we need to flatten it to BSx784\n",
    "        #To use it in a linear layer\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        #We will use a relu activation function for this network! (F.relu)\n",
    "        #NOTE F.relu is the \"functional\" version of the activation function!\n",
    "        #nn.ReLU is a class constructor of a \"ReLU\" object\n",
    "        #These two things are the same for MOST purposes!\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create the model and define the Loss and Optimizer</h3>\n",
    "Since this is a classification task, we will use Cross Entropy Loss. We define our criterion using Cross Entropy Loss \n",
    "\n",
    "[torch.nn.CrossEntropyLoss](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n",
    "\n",
    "Just like in the sine wave approximation, experiment with different optimizers and hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our model\n",
    "model = Simple_MLP(10).to(device)\n",
    "#Create our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#Define our loss funcition and optimizer\n",
    "lr = 1e-1\n",
    "optimizer = optim.SGD(model.parameters(), lr)\n",
    "#Number of Epochs\n",
    "n_epochs = 10\n",
    "\n",
    "#We can print out our model structure\n",
    "print(model)\n",
    "#Note: this is only the order in which the layers were defined NOT the path of the forward pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create a function that will train the network for one epoch </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, loss_logger):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        outputs = model(data.to(device))\n",
    "        loss = criterion(outputs, target.to(device))\n",
    "        \n",
    "        optimizer.zero_grad()   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_logger.append(loss.item())\n",
    "\n",
    "    return loss_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create a function that will evaluate our network's performance on the test set </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, loss_logger):\n",
    "    with torch.no_grad():\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
    "            outputs = model(data.to(device))\n",
    "            \n",
    "            #Calculate the accuracy of the model\n",
    "            #you'll need to accumulate the accuracy over multiple steps\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == target.to(device)).sum().item()\n",
    "            total_predictions += target.shape[0]\n",
    "            \n",
    "            #calculate the loss\n",
    "            loss = criterion(outputs, target.to(device))\n",
    "            loss_logger.append(loss.item())\n",
    "            \n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        return loss_logger, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model for N epochs\n",
    "We call our training and testing functions in a loop, while keeping track of the losses and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss  = []\n",
    "test_acc   = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epochs):\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, train_loss)\n",
    "    test_loss, acc = test_model(model, test_loader, criterion, test_loss)\n",
    "    test_acc.append(acc)\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Epoch: [%d/%d], Test Accuracy: %.2f\" % (i+1, n_epochs, acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Test Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.plot(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
