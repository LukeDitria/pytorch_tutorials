{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> PyTorch Classification with MLP </h1>\n",
    "Lets see how we can train our first neural network using the Pytorch funcunalities we have previously seen! In this notebook we will be training a Multilayer Perceptron (MLP) with the MNIST dataset. We will see how to use Pytorch inbuilt datasets, how to construct a MLP using the Pytorch nn.module class and how to construct a training and testing loop to perform stochastic gradient descent (SGD).\n",
    "\n",
    "<img src=\"../data/MNIST.gif\" width=\"700\" align=\"center\">\n",
    "\n",
    "Animation of MNIST digits and a MLP's activations changing via learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Classification </h2>\n",
    "In this Notebook we are performing \"classification\", that is we want our model to predict what group a given input belongs to after being trained with a large number of examples. In our case we are using MNIST, a data-set of small black and white hand written digits. We want our model to predict what digit (0-9) is in the image!\n",
    "\n",
    "[Classification Explained](https://www.datacamp.com/blog/classification-machine-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Download the MNIST Train and Test set </h2>\n",
    "The MNIST dataset is a large database of handwritten digits that is commonly used for training and testing in the field of machine learning, it consists of 60,000 training images and 10,000 testing images as well as the corresponding digit class (0-9) (it has moved out of fashion these days because it is \"too easy\" to learn though it is still used at times as a \"proof of concept\").  <br>\n",
    "Pytorch has constructed a number of \"dataset\" classes that will automatically download various datasets making it very easy for us to train our models. We will look more closely at using Pytorch datasets in a later lab.\n",
    "\n",
    "[Pytorch Datasets](https://pytorch.org/vision/stable/datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of our mini-batches\n",
    "batch_size = 256\n",
    "\n",
    "# Create a train and test dataset using the Pytorch MNIST dataloader class\n",
    "# NOTE: IF YOU DO NOT HAVE THE LATEST VERSION OF torchvision YOU WILL NEED TO DOWNLOAD THE MNIST DATASET\n",
    "# FIRST AS THE LINK THE OLD PYTORCH MNIST DATASET HAS DOES NOT WORK! \n",
    "# SEE BELOW BLOCK OF CODE!\n",
    "train = MNIST('../data', train=True,  download=True, transform=transforms.ToTensor())\n",
    "test  = MNIST('../data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Using the Pytorch dataloader class and the Pytorch datasets we with create itterable dataloader objects\n",
    "train_loader = dataloader.DataLoader(train, shuffle=True, batch_size=batch_size, num_workers=0, pin_memory=False) \n",
    "test_loader = dataloader.DataLoader(test, shuffle=False, batch_size=batch_size, num_workers=0, pin_memory=False)\n",
    "\n",
    "# NOTE:num_workers is the number of extra threads the dataloader will spawn to load the data from file, \n",
    "# you will rarely need more than 4 \n",
    "# NOTE!!! ON WINDOWS THERE CAN BE ISSUES WITH HAVING MORE THAN 0 WORKERS!! IF YOUR TRAINING LOOP STALLS AND DOES\n",
    "# NOTHING SET num_workers TO 0!\n",
    "\n",
    "# NOTE:pin_memory is only useful if you are training with a GPU!!!! If it is True then the GPU will pre-allocate\n",
    "# memory for the NEXT batch so the CPU-GPU transfer can be handled by the DMA controller freeing up the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IF YOU ARE USING AN OLD VERSION OF PYTORCH\n",
    "# from torchvision.datasets.utils import download_and_extract_archive\n",
    "# import os\n",
    "\n",
    "# url_base = 'https://ossci-datasets.s3.amazonaws.com/mnist/'\n",
    "\n",
    "# resources = [\n",
    "#     (\"train-images-idx3-ubyte.gz\", \"f68b3c2dcbeaaa9fbdd348bbdeb94873\"),\n",
    "#     (\"train-labels-idx1-ubyte.gz\", \"d53e105ee54ea40749a09fcbcd1e9432\"),\n",
    "#     (\"t10k-images-idx3-ubyte.gz\", \"9fb629c4189551a2d022fa330f9573f3\"),\n",
    "#     (\"t10k-labels-idx1-ubyte.gz\", \"ec29112dd5afa0611ce80d1b7f02629c\")]\n",
    "\n",
    "\n",
    "# def download():\n",
    "#     os.makedirs(\"./data/MNIST/raw\", exist_ok=True)\n",
    "\n",
    "#     # download files\n",
    "#     for filename, md5 in resources:\n",
    "#         url = \"{}{}\".format(url_base, filename)\n",
    "#         print(\"Downloading {}\".format(url))\n",
    "        \n",
    "#         download_and_extract_archive(\n",
    "#             url, download_root=\"./data/MNIST/raw\",\n",
    "#             filename=filename,\n",
    "#             md5=md5\n",
    "#         )\n",
    "\n",
    "        \n",
    "# download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU_indx if GPU is avaliable\n",
    "GPU_indx = 0\n",
    "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Visualise a few training samples </h3>\n",
    "Lets visualise that mini-batches that the dataloader gives us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create an itterater using the dataloaders and take a random sample \n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"The input data shape is :\\n\", images.shape)\n",
    "print(\"The target output data shape is :\\n\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that (as specified) our mini-batch is 256. The dataloader has passed us a 4D Tensor as input data, the first dimension (d0) is known as the \"batch dimension\" (B) the other three are the image dimensions (CxHxW). We can this of this 4D Tensor as a stack of 256, 1 channel, 28x28 images.<br>\n",
    "The image labels are a 1D Tensor, 1 single scalar value per image (per mini-batch \"instance\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(images, 32)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our Neural Network Model \n",
    "We define our model using the torch.nn.Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a simple MLP network similar to the sine wave approximator\n",
    "class Simple_MLP(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Simple_MLP, self).__init__()\n",
    "        # We will use 4 linear layers\n",
    "        # The input to the model is 784 (28x28 - the image size)\n",
    "        # and the should be num_classes outputs\n",
    "        \n",
    "        # TO DO-  hidded layer size 512\n",
    "        # TO DO-  hidded layer size 256\n",
    "        # TO DO-  hidded layer size 128\n",
    "        # TO DO-  output layer size num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # The data we pass the model is a batch of single channel images\n",
    "        # with shape BSx1x28x28 we need to flatten it to BSx784\n",
    "        # To use it in a linear layer\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # We will use a relu activation function for this network! (F.relu)\n",
    "        # NOTE F.relu is the \"functional\" version of the activation function!\n",
    "        # nn.ReLU is a class constructor of a \"ReLU\" object\n",
    "        \n",
    "        # These two things are the same for MOST purposes!\n",
    "        # TO DO, layer and then activation function\n",
    "        # TO DO, layer and then activation function\n",
    "        # TO DO, layer and then activation function\n",
    "        # TO DO, add layer\n",
    "        return # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create the model and define the Loss and Optimizer</h3>\n",
    "Since this is a classification task, we will use Cross Entropy Loss. We define our criterion using Cross Entropy Loss \n",
    "\n",
    "[torch.nn.CrossEntropyLoss](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n",
    "\n",
    "[Killer Combo: Softmax and Cross Entropy by Paolo Perrotta\n",
    "](https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba)\n",
    "\n",
    "Just like in the sine wave approximation, experiment with different optimizers and hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our model\n",
    "model = # TO DO create a model with 10 classes\n",
    "#Create our loss function\n",
    "criterion = # TO DO add the Cross Entropy loss Function\n",
    "#Define our loss funcition and optimizer\n",
    "lr = # TO DO\n",
    "optimizer = # TO DO\n",
    "# Number of Epochs\n",
    "n_epochs = # TO DO\n",
    "\n",
    "# We can print out our model structure\n",
    "print(model)\n",
    "# Note: this is only the order in which the layers were defined NOT the path of the forward pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create a function that will train the network for one epoch </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, loss_logger):\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):   \n",
    "        \n",
    "        # Forward pass of model\n",
    "        outputs = # TO DO\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = # TO DO\n",
    "        \n",
    "        # Zero gradients\n",
    "        # TO DO\n",
    "        \n",
    "        # Backprop loss\n",
    "        # TO DO\n",
    "        \n",
    "        # Optimization Step\n",
    "        # TO DO\n",
    "        \n",
    "        loss_logger.append(loss.item())\n",
    "\n",
    "    return model, optimizer, loss_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create a function that will evaluate our network's performance on the test set </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, loss_logger):\n",
    "    with torch.no_grad():\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(tqdm(test_loader, desc=\"Testing\", leave=False)):   \n",
    "            \n",
    "            # Forward pass of model\n",
    "            outputs = # TO DO           \n",
    "            \n",
    "            # Calculate the accuracy of the model\n",
    "            # You'll need to accumulate the accuracy over multiple steps\n",
    "            \n",
    "            # TO DO\n",
    "            # Number of correctly predicted outputs\n",
    "            \n",
    "            correct_predictions += # TO DO\n",
    "            # Total number of predictions made\n",
    "            total_predictions += # TO DO\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = # TO DO\n",
    "            loss_logger.append(loss.item())\n",
    "            \n",
    "        acc = (correct_predictions/total_predictions) * 100.0\n",
    "        return loss_logger, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model for N epochs\n",
    "We call our training and testing functions in a loop, while keeping track of the losses and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists for the train/test losses and the test accuracy\n",
    "train_loss = []\n",
    "test_loss  = []\n",
    "test_acc   = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(n_epochs, desc=\"Epoch\", leave=False):\n",
    "    # Call the trainging function to perform an epoch of training\n",
    "    model, optimizer, train_loss = # TO DO\n",
    "    \n",
    "    # Call the testing function to work out the test loss and accuracy!\n",
    "    test_loss, acc = # TO DO\n",
    "    test_acc.append(acc)\n",
    "\n",
    "print(\"Final Accuracy: %.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Training and Test losses\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the Test Accuracy\n",
    "# TO DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
