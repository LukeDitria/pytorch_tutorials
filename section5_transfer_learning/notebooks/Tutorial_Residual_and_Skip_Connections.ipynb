{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Residual and Skip Connections</h1>\n",
    "As we've seen up until now neural networks can learn a lot of interesting things! But much of the data has been of a very simple nature. In this lab we are going to try and train with data that is a bit more complicated, the CIFAR10 dataset. CIFAR10 images are much more complicated then MNIST images and even though they are only 3x32x32 they have about 4x as much data as MNIST! Now image using high resolution images!<br>\n",
    "So let's just bigger neural networks right? In general there are two ways we can increase the size of the neural networks we have seen up until now, by increasing the width (parameters per layer) and the depth (number of layers).<br>\n",
    "So which is better?<br>\n",
    "Well..... it's complicated<br>\n",
    "Via empirical studies it is easy to show that by increasing the model's width the network's performance on a validation set does increase, up until a point then the model with a huge number of parameters starts to overfit on the training set and performance on the validation set DECREASES never reaching even close to 100%. Instead it has been shown that increasing the DEPTH of our model is far more effective. The verdict is STILL out on why this is but theories include:<br>\n",
    "-Every layer performs independent \"operations\" (like steps in a program) more steps are better<br>\n",
    "-Information is \"distilled\" layer to layer so each layer receives a refined version of the input and so cannot overfit<br>\n",
    "-Adding a new layer creates more paths for the data to flow to the output then does adding more width\n",
    "\n",
    "So we'll just add more layers!! Well... it's not that simple\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1000/1*aqmUx_ONo8KqKNEYsjM8eA.png)\n",
    "\n",
    "[Why ResNets?](https://mc.ai/what-are-deep-residual-networks-or-why-resnets-are-important/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader as dataloader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from Trainer import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of our mini batches\n",
    "batch_size = 256\n",
    "\n",
    "# How many iterations of our dataset\n",
    "num_epochs = 20\n",
    "\n",
    "# Optimizer learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Initialise best valid accuracy \n",
    "best_valid_acc = 0\n",
    "\n",
    "# Where to load/save the dataset from \n",
    "data_set_root = \"../../datasets\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_from_checkpoint = False\n",
    "save_dir = '../data/Models'\n",
    "\n",
    "model_name = 'Custom_ResNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU_indx if GPU is avaliable\n",
    "gpu_indx = 0\n",
    "device = torch.device(gpu_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create a transform for the input data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a composition of transforms\n",
    "# transforms.Compose will perform the transforms in order\n",
    "# NOTE some transform only take in a PIL image, others only a Tensor\n",
    "# EG Resize and ToTensor take in a PIL Image, Normalize takes in a Tensor\n",
    "# Refer to documentation\n",
    "# https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "transform = transforms.Compose([transforms.Resize(32),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create the training, testing and validation data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the CIFAR10 dataset!\n",
    "train_data = datasets.CIFAR10(data_set_root, train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(data_set_root, train=False, download=True, transform=transform)\n",
    "\n",
    "# We are going to split the train dataset into a train and validation set\n",
    "validation_split = 0.9\n",
    "\n",
    "# Determine the number of samples for each split\n",
    "n_train_examples = int(len(train_data) * validation_split)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "# The function random_split will take our dataset and split it randomly and give us dataset\n",
    "# that are the sizes we gave it\n",
    "# Note: we can split it into to more then two pieces!\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [n_train_examples, n_valid_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating Deep Networks</h2>\n",
    "So we'll just make our Networks deeper!<br>\n",
    "Well, it's not that simple, not only does adding more layers mean our model is more sequential (rather than parallel, meaning forward and backward passes are slower) but we now face the problem of \n",
    "<a href=\"https://towardsdatascience.com/vanishing-gradient-in-deep-neural-network-83953217c59f\">Vanishing Gradients</a>. <br>\n",
    "When we create larger and larger networks, something funny happens when we try and train them, the gradients that are back propagated from the output become tiny (near zero) for layers near the top. They seem to \"vanish\"! But why!? Well in most models gradients become smaller as they backpropagate through a network. This is easiest to understand by looking at our networks parameters and thinking about how gradients are back propagated. In general gradients are back propagated by multiplying together the weights of layers sequentially. As the weights of our models are tiny (much less then one in magnitude) multiplying many of them together gives us a VERY small result. This problem becomes worse the deeper it is! As a result the top layers of our network barely move from their random initialisations and in effect aren't trained!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Enter the Skip and Residual Connection!</h3>\n",
    "Skip and Residual connection allow us to have our deep networks and train them too!<br>\n",
    "So what are they?<br>\n",
    "In simple terms we take the output of some layer and \"skip\" some number of layers and combine it with the hidden layer of a much later layer. One result of this is that, during backpropagation, the gradients have a shorter minimum path to the input layers, reducing the impact of the vanishing gradient!<br>\n",
    "There are a couple of ways to combine hidden layers together, by adding them together or concatenating the tensors.<br>\n",
    "Adding the hidden layers together (often called a Residual Connection) means that the size of the layers must be the same which for the networks we've seen until now has not been the case (size usually decreases). However with residual connections we don't necessarily need to add the hidden layers directly. For example, we can take a hidden layer and skip two layers, by passing it through a single layer (that will transform it to the right size) halving the length of the path for gradients.<br>\n",
    "Concatenating hidden layers involves simply \"sticking together\" the tensors. <br>\n",
    "Residual and Skip Connections not only help with the vanishing gradient problem but also helps information from the input penetrate deeper into the network.\n",
    "\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1140/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)\n",
    "A simple \"Identity\" resdual connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modules in Modules</h3>\n",
    "To simplify the creation os our residual and skip networks we will create seperate nn.modules of the skip and residual \"blocks\" and then create our \"top level\" network with these!<br>\n",
    "NOTE: For simplicity all these blocks return an output the same size as their input though this does not have to be the case!\n",
    "NOTE: We also introduce <a href=\"https://youtu.be/DtEq44FTPM4?si=7KDVCRr_fzbQ2C7H\">Batch Normalisation</a> layers here for more info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvBlock with no Residual connection\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        # Call the __init__ function of the parent nn.module class\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channels, channels//2,  kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels//2)\n",
    "        self.conv2 = nn.Conv2d(channels//2, channels,  kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# First block demonstraights a simple identity residual connection\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        # Call the __init__ function of the parent nn.module class\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channels, channels//2,  kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels//2)\n",
    "        self.conv2 = nn.Conv2d(channels//2, channels,  kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Res Skip BEFORE ReLU\n",
    "        x0 = x \n",
    "        \n",
    "        # Activation function BEFORE Conv Input\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # Res addition on \"raw\" layer outputs\n",
    "        return x + x0\n",
    "    \n",
    "    \n",
    "# Second block demonstrates how we can use a \"side layer\" in our residual block to \n",
    "# Change the shape of the tensors so they match later layers\n",
    "# The channels change in this case but you could also create one where the feature map size changes\n",
    "class ResDownBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out=None):\n",
    "        # Call the __init__ function of the parent nn.module class\n",
    "        super(ResDownBlock, self).__init__()\n",
    "        \n",
    "        if channels_out is None:\n",
    "            channels_out = channels_in\n",
    "            \n",
    "        # How to handle channel width change\n",
    "        self.conv1 = nn.Conv2d(channels_in, channels_out, kernel_size=3, stride=1, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(channels_out, channels_out, kernel_size=3, stride=1, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels_out)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channels_in, channels_out, kernel_size=3, stride=1, padding = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(x)\n",
    "        x0 = self.conv3(x)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # Res addition on \"raw\" layer outputs\n",
    "        return x + x0\n",
    "\n",
    "    \n",
    "# Third block is a simple skip connection\n",
    "# The layers downsamples to half the input size channel size\n",
    "# and then concatenates the first hidden layer (x1) to the last output (x1) along the channels\n",
    "# creating a tensor that is the same shape as the input\n",
    "class SkipBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        # Call the __init__ function of the parent nn.module class\n",
    "        super(SkipBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channels, channels//2,  kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels//2)\n",
    "        self.conv2 = nn.Conv2d(channels//2, channels//2,  kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x1 = self.conv1(x)\n",
    "        x2 = F.relu(self.bn1(x1))\n",
    "        x3 = self.conv2(x2)\n",
    "        \n",
    "        # Skip concatenation of \"raw\" layer outputs\n",
    "        return torch.cat((x1, x2),1)\n",
    "\n",
    "    \n",
    "# We will use the above blocks to create a \"Deep\" neural network with many layers!\n",
    "class Deep_CNN(nn.Module):\n",
    "    def __init__(self, channels_in, num_blocks=2, ch_width=32, layer_type=ResBlock):\n",
    "        # Call the __init__ function of the parent nn.module class\n",
    "        super(Deep_CNN, self).__init__()\n",
    "        \n",
    "        # Downsample the image/feature map size from 32x32 to 8x8\n",
    "        self.conv1 = nn.Conv2d(channels_in, ch_width//4, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch_width//4)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(ch_width//4, ch_width//2, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(ch_width//2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(ch_width//2, ch_width, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Define a nn.Sequential list of ResBlocks if num_blocks > 0\n",
    "        # else use nn.Identity as a \"do nothing\" block\n",
    "        if num_blocks > 0:\n",
    "            self.layers = self.create_blocks(num_blocks, layer_type, channels = ch_width)\n",
    "        else:\n",
    "            self.layers = nn.Identity()\n",
    "\n",
    "        self.linear1 = nn.Linear(ch_width * 4 * 4, 10)\n",
    "\n",
    "        # This function will create a nn.Sequential block from a list of Pytorch layers\n",
    "        # A forward pass though the Sequential block will perform a forward pass\n",
    "        # though the layers in the order they appear in the list\n",
    "    def create_blocks(self, num_blocks, block_type, channels):\n",
    "        blocks = []\n",
    "        \n",
    "        # We will add some number of the res/skip blocks!\n",
    "        for _ in range(num_blocks):\n",
    "            blocks.append(block_type(channels))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass input through conv layers\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # No ReLU on the output so each ResBlock gets the \"raw\" layer output\n",
    "        # So the Res skip can happen before any activation function!\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        # Pass through the block of res/skip blocks!\n",
    "        x = F.relu(self.layers(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Flatten it for the final linear layer!\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # Ouput the class acitvations!\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating our Network</h3>\n",
    "When creating an instance of our network we will also specify the type of block we will use!<br>\n",
    "The next bit of code should be familiar to you, try experimenting with the different layer types and see the different results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of our network\n",
    "# Set channels_in to the number of channels of the dataset images\n",
    "res_net = Deep_CNN(channels_in=3, num_blocks=3, ch_width=64, layer_type=ResBlock).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have a look at our network structure!\n",
    "# res_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer(model=res_net, device=device, loss_fun=nn.CrossEntropyLoss(), \n",
    "                             batch_size=batch_size, learning_rate=learning_rate, \n",
    "                             save_dir=save_dir, model_name=model_name, \n",
    "                             start_from_checkpoint=start_from_checkpoint)\n",
    "valid_acc = 0\n",
    "train_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.set_data(train_set=train_data, test_set=test_data, val_set=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "images, labels = next(iter(model_trainer.test_loader))\n",
    "out = torchvision.utils.make_grid(images[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how many Parameter's our Model has!\n",
    "num_params = 0\n",
    "for param in model_trainer.model.parameters():\n",
    "    num_params += param.flatten().shape[0]\n",
    "print(\"This model has %d (approximately %d Million) Parameters!\" % (num_params, num_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell implements our training loop\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "pbar = trange(model_trainer.start_epoch, num_epochs, leave=False, desc=\"Epoch\")    \n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Accuracy: Train %.2f%%, Val %.2f%%' % (train_acc * 100, valid_acc * 100))\n",
    "    \n",
    "    # Call the training function and pass training dataloader etc\n",
    "    model_trainer.train_model()\n",
    "    \n",
    "    # Call the modules evaluate function for train and validation set\n",
    "    train_acc = model_trainer.evaluate_model(train_test_val=\"train\")\n",
    "    valid_acc = model_trainer.evaluate_model(train_test_val=\"val\")\n",
    "    \n",
    "    # Check if the current validation accuracy is greater than the previous best\n",
    "    # If so, then save the model\n",
    "    if valid_acc > model_trainer.best_valid_acc:\n",
    "        model_trainer.save_checkpoint(epoch, valid_acc)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The highest validation accuracy was %.2f%%\" %(model_trainer.best_valid_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training time %.2f seconds\" %(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10, 5))\n",
    "train_x = np.linspace(0, num_epochs, len(model_trainer.train_loss_logger))\n",
    "_ = plt.plot(train_x, model_trainer.train_loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(model_trainer.train_acc_logger))\n",
    "_ = plt.plot(train_x, model_trainer.train_acc_logger, c = \"y\")\n",
    "valid_x = np.linspace(0, num_epochs, len(model_trainer.val_acc_logger))\n",
    "_ = plt.plot(valid_x, model_trainer.val_acc_logger, c = \"k\")\n",
    "\n",
    "_ = plt.title(\"Accuracy\")\n",
    "_ = plt.legend([\"Training accuracy\", \"Validation accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the evaluate function and pass the evaluation/test dataloader etc\n",
    "test_acc = model_trainer.evaluate_model(train_test_val=\"test\")\n",
    "print(\"The Test Accuracy is: %.2f%%\" %(test_acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
