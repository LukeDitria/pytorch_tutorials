{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Gradient Decent Revisit with Pytorch</h1> <br>\n",
    "<img src=\"../data/Linear_Regression.gif\" width=\"1200\" align=\"center\">\n",
    "Animation of our \"model\" at each step when training with gradient descent\n",
    "\n",
    "<b>With our new knowledge of Pytorch let's create a new implementation of gradient decent!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading the data </h3>\n",
    "Lets load some \"toy\" data that we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can load your data using this cell\n",
    "npzfile = np.load(\"../data/toy_data_two_moon.npz\") # toy_data.npz or toy_data_two_circles.npz\n",
    "\n",
    "# The compressed Numpy file is split up into 4 parts\n",
    "# Lets convert them to Pytorch Float Tensors\n",
    "# Train inputs and target outputs\n",
    "x_train = torch.FloatTensor(npzfile['arr_0'])\n",
    "y_train = torch.FloatTensor(npzfile['arr_2'])\n",
    "\n",
    "# Test inputs and target outputs\n",
    "x_test = torch.FloatTensor(npzfile['arr_1'])\n",
    "y_test = torch.FloatTensor(npzfile['arr_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Let's plot our data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see what the data looks like\n",
    "plt.subplot(121)\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], marker='o', c=y_train[:,0], s=25, edgecolor='k')\n",
    "plt.title(\"Train data\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(x_test[:, 0], x_test[:, 1], marker='o', c=y_test[:,0], s=25, edgecolor='k')\n",
    "plt.title(\"Test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is randomly sampled from an odd looking distribution, the colour of the dots (as represented by y_test[:,0], a one or zero) is what the output of our model SHOULD be (aka the \"Ground Truth Data\"). Note that each data point is a vector of two values (the \"x and y\" values), therefore there will only be two parameters in our linear model. <br>\n",
    "<b>Note we do NOT need to add ones to our data for a bias term as Pytorch's Linear layer has a bias term by default</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training a model with GD </h2>\n",
    "\n",
    "[Gradient Descent, Step-by-Step by StatQuest](https://youtu.be/sDv4f4s2SB8?si=iClqYh2v3I7uf9WR)\n",
    "\n",
    "In doing so, we need a function to <br>\n",
    "1- compute the loss with respect to the inputs and the parameters of the model <br>\n",
    "2- compute the gradient of the model with respect to its parameters $\\theta$\n",
    "\n",
    "We recall the loss of the linear regression as\n",
    "\\begin{align}\n",
    "L(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\|\\theta^\\top \\boldsymbol{x}_i - y_i\\|^2\n",
    "\\end{align}\n",
    "\n",
    "Now it is easy to see that\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{m} \\sum_{i=1}^m 2(\\theta^\\top \\boldsymbol{x}_i - y_i)\\boldsymbol{x}_i\n",
    "\\end{align}\n",
    "\n",
    "Instead of calculating the gradient by hand, we'll just use Pytorch's auto-grad!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our linear model - 2 inputs, 1 output (bias is included in linear layer)\n",
    "linear = nn.Linear(2, 1) \n",
    "# Define our loss function - MSE\n",
    "loss_function = nn.MSELoss()\n",
    "# Create our optimizer - lr = 0.1\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can perform multiple iterations of GD to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times we iterate over the dataset\n",
    "max_epoch = 100\n",
    "\n",
    "loss_log = [] # keep track of the loss values\n",
    "acc = [] # keep track of the accuracy \n",
    "for epoch in range(max_epoch):\n",
    "    with torch.no_grad():\n",
    "        y_test_hat = linear(x_test)\n",
    "        class_pred = (y_test_hat >= 0).float()\n",
    "        acc.append(float(sum(class_pred == y_test))/ float(y_test.shape[0]))\n",
    "\n",
    "    # Perform a training step\n",
    "    y_train_hat = linear(x_train)\n",
    "    loss = loss_function(y_train_hat, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_log.append(loss.item())\n",
    "    \n",
    "print(\"Accuracy of linear model(GD): %.2f%% \" %(acc[-1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc)\n",
    "plt.title(\"Model accuracy per iteration\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)\n",
    "plt.title(\"Model loss iteration\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression </h1>\n",
    "We can see that with binary classification our goal is possition the decision boundary such that it seperates the two classes. However by simply using linear regression with binary labels, we are forcing our model to try and position the decision boundary at a fixed distance from each data point, EVEN if it's on the \"correct\" side of the boundary. For example if a data point (label 1) was on the correct side of the boundary (positive side)\n",
    "BUT it was a large distance away from the boundary, the model would be penalised and the boundary would be moved. This is an obvious problem if there is a large spread of values for a given class and can lead to our model to never converge to an optimal solution. <br>\n",
    "\n",
    "<h3>Sigmoid</h3>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\" width=\"300\" align=\"center\">\n",
    "\n",
    "Instead we think about moving the descision boundary *as far as possible* from the positive points on one side and *as far as possible* from the negative points on the other side. To do this we can utilize the Sigmoid function, which maps all real numbers to the range 0 to 1: $$ R \\to [0, 1] $$ <br>\n",
    "By putting the Sigmoid function on the output of our model we can see that to reach the target value 1, the raw model ouput would have to be $+\\infty$ and 0 would be $-\\infty$. The optimal solution would therefore achieve our goal of maximising the distance between the data points and the descision boundary.<br>\n",
    "While this solves one problem, it introduces another, the sigmoid function's gradient (derivative) is very flat for values witha large magnitude. If we were to use the MSE loss to train a model with a Sigmoid on the output, training would not only be slow, but may not progress at all! To see why, consider the case where our model as made a very large incorrect prediction, the gradient in this case would be very very small, the update to our model would therefore also be very small. <br>\n",
    "To convince yourself of this try adding the sigmoid function (torch.sigmoid()) to the output of the above model and train!\n",
    "\n",
    "<h3>Cross-Entropy loss</h3>\n",
    "We therefore introduce a new loss function the <a href=\"https://en.wikipedia.org/wiki/Cross-entropy#Cross-entropy_loss_function_and_logistic_regression\">Cross-Entropy loss</a>:\n",
    "\n",
    "\\begin{align}\n",
    "L(\\hat y, y) &= -y\\ln (\\hat y) - (1 - y)\\ln (1 - \\hat y)\\\\\n",
    "\\\\\n",
    "Where&\\\\\n",
    "y &= Target \\: value\\\\\n",
    "\\hat y &= Predicted \\: value\\\\\n",
    "\\end{align}\n",
    "\n",
    "We obtain this loss from the maximum log-likelihood of a multinomial (or binomial in the binary case) distribution. That is, instead of simply regressing to a value, we our model is now produces a probability distribution for the likelihood that a given input belongs to a given class. For the single output binary logistic case we get a single probability that the input belongs the the possitive (1) class. <br>\n",
    "NOTE that using a Softmax output (rather than Sigmoid) with a Cross-Entropy loss is very common for multiclass classification but can also be used for binary classification (more on this later).\n",
    "\n",
    "<h3>It's still a linear model!</h3>\n",
    "It's important to realise that even though our model now has the non-linear Sigmoid function on it's output, this model is still only linear, an therefore can only produce a linear descision boundary! We've simply changed that way we think about our model and how we train it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create a Logistic Regression Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our linear model - 2 inputs, 1 output (bias is included in linear layer)\n",
    "logistic_linear = nn.Linear(2, 1) \n",
    "\n",
    "# Define our loss function - Binary Cross entropy With logits\n",
    "# By using the \"with logits\" version Pytorch will assume the outputs given are the RAW\n",
    "# model outputs without the Sigmoid (logits - log odd probabilities, it is the inverse of the sigmoid function!)\n",
    "# Pytorch incorporates the Sigmoid into the loss function for numerical stability!\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create our optimizer - lr = 0.1\n",
    "logistic_optimizer = torch.optim.SGD(logistic_linear.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times we iterate over the dataset\n",
    "max_epoch = 100\n",
    "\n",
    "logistic_loss_log = [] # keep track of the loss values\n",
    "logistic_acc = [] # keep track of the accuracy \n",
    "for epoch in range(max_epoch):\n",
    "    with torch.no_grad():\n",
    "        y_test_hat = logistic_linear(x_test)\n",
    "        \n",
    "        # The descision boundary is at 0.5 (between 0 and 1) AFTER the sigmoid\n",
    "        # The input to the Sigmoid function that gives 0.5 is 0!\n",
    "        # Therefore the descision boundary for the RAW output is at 0!!\n",
    "        class_pred = (y_test_hat > 0).float()\n",
    "        logistic_acc.append(float(sum(class_pred == y_test))/ float(y_test.shape[0]))\n",
    "        \n",
    "    # Perform a training step\n",
    "    y_train_hat = logistic_linear(x_train)\n",
    "    loss = loss_function(y_train_hat, y_train)\n",
    "    \n",
    "    logistic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    logistic_optimizer.step()\n",
    "\n",
    "    logistic_loss_log.append(loss.item())\n",
    "    \n",
    "print(\"Accuracy of linear model(GD): %.2f%% \" %(logistic_acc[-1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logistic_acc)\n",
    "plt.title(\"Model accuracy per iteration\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xlabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logistic_loss_log)\n",
    "plt.title(\"Model loss iteration\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xlabel(\"BCE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
