{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sb01NHS5PMS8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader as dataloader\n",
    "import torchvision.models as models\n",
    "\n",
    "# You'll need to install albumentations!\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from PIL import Image, ImageOps\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "from Trainer import ModelTrainer\n",
    "from Datasets import CUB200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUaeH517PMS_"
   },
   "outputs": [],
   "source": [
    "# The size of our mini batches\n",
    "batch_size = 64\n",
    "\n",
    "# How many itterations of our dataset\n",
    "num_epochs = 60\n",
    "\n",
    "# Optimizer learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# You'll need to Download Dataset found here\n",
    "# https://www.kaggle.com/datasets/wenewone/cub2002011\n",
    "# Unzip and rename to cub_200\n",
    "# Where to load/save the dataset from \n",
    "data_set_root = \"../../datasets/cub_200\"\n",
    "\n",
    "# What to resize our images to \n",
    "image_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVGVcxx0PMTB"
   },
   "outputs": [],
   "source": [
    "start_from_checkpoint = False\n",
    "\n",
    "save_dir = '../data/Models'\n",
    "model_name = 'UNet_CUB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRJXAwTXPMTD"
   },
   "outputs": [],
   "source": [
    "# Set device to GPU_indx if GPU is avaliable\n",
    "gpu_indx = 0\n",
    "device = torch.device(gpu_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "keIwAFK-PMTG"
   },
   "outputs": [],
   "source": [
    "# Only include the augmentations if you can use the v2 transforms that will augment \n",
    "# both the image and bounding boxes (you'll need to modify the dataset class too!)\n",
    "\n",
    "train_transform = A.Compose([A.SmallestMaxSize(max_size=image_size),\n",
    "                             A.RandomCrop(height=image_size, width=image_size),\n",
    "                             A.HorizontalFlip(p=0.5),\n",
    "                             A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "                             A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "                             A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "                             A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225]),\n",
    "                            ToTensorV2()], \n",
    "                            bbox_params=A.BboxParams(format='coco',\n",
    "                                                     min_area=0, min_visibility=0.0, \n",
    "                                                     label_fields=['class_labels']))\n",
    "\n",
    "transform = A.Compose([A.SmallestMaxSize(max_size=image_size),\n",
    "                       A.CenterCrop(height=image_size, width=image_size),\n",
    "                       A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                   std=[0.229, 0.224, 0.225]),\n",
    "                       ToTensorV2()], \n",
    "                      bbox_params=A.BboxParams(format='coco',\n",
    "                                               min_area=0, min_visibility=0.0, \n",
    "                                               label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection over Union (IoU)\n",
    "To train our model to correctly predict the bounding box we're going to use logistic regression. However to calulate the performance of our model (and the accuracy of the predicted bounding boxes) we're going to use the [Intersection over Union](https://medium.com/analytics-vidhya/iou-intersection-over-union-705a39e7acef) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module class that will return the IoU for a batch of outputs\n",
    "class MaskIOU(nn.Module):\n",
    "\n",
    "    def mask_intersection_over_union(self, pred_bbox, target_bbox):\n",
    "\n",
    "        # compute the area of intersection rectangle\n",
    "        interArea = (pred_bbox * target_bbox).sum(dim=[1, 2])\n",
    "\n",
    "        area1 = pred_bbox.sum(dim=[1, 2])\n",
    "        area2 = target_bbox.sum(dim=[1, 2])\n",
    "\n",
    "        # compute the intersection over union by taking the intersection\n",
    "        # area and dividing it by the sum of prediction + ground-truth\n",
    "        # areas - the interesection area\n",
    "        iou = interArea / (area1 + area2 - interArea + 1e-5)\n",
    "\n",
    "        # return the intersection over union value\n",
    "        return iou\n",
    "\n",
    "    def forward(self, predictions, data):\n",
    "        \"\"\"\n",
    "        data: list of data, index 0 is the input image index [0] is the target\n",
    "        predictions: raw output of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        pred_mask = predictions.argmax(1)\n",
    "        target_mask = data[1].to(pred_mask.device)\n",
    "        \n",
    "        return self.mask_intersection_over_union(pred_mask, target_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7L2lrkdPMTM"
   },
   "source": [
    "# Create the training, testing and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29503,
     "status": "ok",
     "timestamp": 1568947936500,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "5FyAAqHWPMTM",
    "outputId": "d566a865-6439-47d3-a195-6b897199d923"
   },
   "outputs": [],
   "source": [
    "# Define our Datasets\n",
    "# You'll need to download the dataset from Kaggle\n",
    "# https://www.kaggle.com/datasets/wenewone/cub2002011\n",
    "# Unzip it (and the directories it contains) into the datasets directory \n",
    "# and rename the top-level directory cub_200\n",
    "\n",
    "train_data = CUB200(data_set_root, image_size=image_size, transform=train_transform, \n",
    "                    test_train=0, return_masks=True)\n",
    "\n",
    "test_data = CUB200(data_set_root, image_size=image_size, transform=transform, \n",
    "                   test_train=1, return_masks=True)\n",
    "\n",
    "# Split trainging data into train and validation set with 90/10% traning/validation split\n",
    "validation_split = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data)*validation_split)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [n_train_examples, n_valid_examples],\n",
    "                                                       generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGdAgnKgPMTc"
   },
   "source": [
    "<h2>The U-Net</h2>\n",
    "The U-Net was developed specifically for image segmentation, the intuition being that the \"autoencoder-like\" structure will extract class information from the input image and the skip connections allow image \"structure\" information (contained in the feature maps) to jump the bottle-neck. This means that the network does not have to \"learn\" how to extract and compress the structure of the image leading to sharper edges and higher quality results.\n",
    "<img src=\"https://miro.medium.com/max/1200/1*f7YOaE4TWubwaFF7Z1fzNw.png\" width=\"750\" align=\"center\">\n",
    "\n",
    "[U-Net](https://towardsdatascience.com/u-net-b229b32b4a71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Unet\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(UnetDown, self).__init__()\n",
    "        \n",
    "        model = [nn.BatchNorm2d(input_size),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Conv2d(input_size, output_size, kernel_size=3, stride=1, padding=1),\n",
    "                 nn.BatchNorm2d(output_size),\n",
    "                 nn.ReLU(),\n",
    "                 nn.MaxPool2d(2),\n",
    "                 nn.Conv2d(output_size, output_size, kernel_size=3, stride=1, padding=1)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        return self.model(x)\n",
    "      \n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(UnetUp, self).__init__()\n",
    "\n",
    "        model = [nn.BatchNorm2d(input_size),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Conv2d(input_size, output_size, kernel_size=3, stride=1, padding=1),\n",
    "                 nn.BatchNorm2d(output_size),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "                 nn.Conv2d(output_size, output_size, kernel_size=3, stride=1, padding=1)]\n",
    "          \n",
    "        self.model = nn.Sequential(*model)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "            \n",
    "         \n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out=2):\n",
    "        super(Unet, self).__init__()\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(channels_in, 64, \n",
    "                                 kernel_size=3, stride=1, padding=1)   # H X W --> H X W\n",
    "        \n",
    "        self.down1 = UnetDown(64, 64)  #  H   X W   --> H/2 X W/2\n",
    "        self.down2 = UnetDown(64, 128)  #  H/2 X W/2 --> H/4 X W/4\n",
    "        self.down3 = UnetDown(128, 128)  #  H/4 X W/4 --> H/8 X W/8\n",
    "        self.down4 = UnetDown(128, 256)  # H/8 X W/8 --> H/16 X W/16\n",
    "\n",
    "        self.up4 = UnetUp(256, 128)  #    H/16 X W/16 --> H/8 X W/8\n",
    "        self.up5 = UnetUp(128 * 2, 128)  # H/8 X W/8 --> H/4 X W/4\n",
    "        self.up6 = UnetUp(128 * 2, 64)  # H/4 X W/4 --> H/2 X W/2\n",
    "        self.up7 = UnetUp(64 * 2, 64)  # H/2 X W/2 --> H   X W\n",
    "        \n",
    "        self.conv_out = nn.Conv2d(64 * 2, channels_out, \n",
    "                                  kernel_size=3, stride=1, padding=1)  # H X W --> H X W\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.conv_in(x)  # 16 x H x W\n",
    "        \n",
    "        x1 = self.down1(x0)  # 32 x H/2 x W/2\n",
    "        x2 = self.down2(x1)  # 64 x H/4 x W/4\n",
    "        x3 = self.down3(x2)  # 64 x H/8 x W/8\n",
    "        x4 = self.down4(x3)  # 128 x H/16 x W/16\n",
    "\n",
    "        # Bottle-neck --> 128 x H/16 x W/16\n",
    "\n",
    "        x5 = self.up4(x4)  # 64 x H/8 x W/8\n",
    "        \n",
    "        x5_ = torch.cat((x5, x3), 1)  # 128 x H/8 x W/8\n",
    "        x6 = self.up5(x5_)  # 32 x H/4 x W/4\n",
    "        \n",
    "        x6_ = torch.cat((x6, x2), 1)  # 64 x H/4 x W/4\n",
    "        x7 = self.up6(x6_)  # 16 x H/2 x W/2\n",
    "        \n",
    "        x7_ = torch.cat((x7, x1), 1)  # 64 x H/2 x W/2\n",
    "        x8 = self.up7(x7_)  # 16 x H x W\n",
    "        \n",
    "        x8_ = F.relu(torch.cat((x8, x0), 1))  # 32 x H x W        \n",
    "        return self.conv_out(x8_)  # Co x H x W\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34657,
     "status": "ok",
     "timestamp": 1568947941813,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "JQPwhQuaPMTd",
    "outputId": "000d7dfc-5cd1-4afc-ef52-d7c1c7cc2754"
   },
   "outputs": [],
   "source": [
    "unet = Unet(channels_in=3, channels_out=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer(model=unet.to(device), output_size=-1, device=device, \n",
    "                             loss_fun=nn.CrossEntropyLoss(), batch_size=batch_size, \n",
    "                             learning_rate=learning_rate, save_dir=save_dir, model_name=model_name,\n",
    "                             elav_metric=MaskIOU(), start_from_checkpoint=start_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.set_data(train_set=train_data, test_set=test_data, val_set=valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set a Learning Rate Scheduler\n",
    "We can dynamically change the <a href=\"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\">learning rate</a> during training to help our model converge to a better minimum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.set_lr_schedule(optim.lr_scheduler.StepLR(model_trainer.optimizer, \n",
    "                                                        step_size=1, \n",
    "                                                        gamma=0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30371,
     "status": "ok",
     "timestamp": 1568947937416,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "2ET6pMrYPMTa",
    "outputId": "9131cd8c-eab1-4a66-d4fc-1314ae775814"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "images, mask, bbox, labels = next(iter(model_trainer.train_loader))\n",
    "out = torchvision.utils.make_grid(images[0:16], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid((mask[0:16]).unsqueeze(1).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how many Parameter's our Model has!\n",
    "num_params = 0\n",
    "for param in model_trainer.model.parameters():\n",
    "    num_params += param.flatten().shape[0]\n",
    "print(\"This model has %d (approximately %d Million) Parameters!\" % (num_params, num_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model!\n",
    "Our full training method is now fully contained within the trainner class! Simply run the run_training method and specify how many epochs it should train for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1568948678396,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "K27MEsO5PMT-",
    "outputId": "0c03f2f2-e250-4fad-dae5-b0dbaad8bda4"
   },
   "outputs": [],
   "source": [
    "model_trainer.run_training(num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The highest validation IoU was %.2f\" %(model_trainer.best_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1749,
     "status": "ok",
     "timestamp": 1568948455980,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "HoLp_P3xPMUE",
    "outputId": "b241900f-ff45-42f0-dc33-14b48126836f"
   },
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10,5))\n",
    "_ = plt.plot(model_trainer.train_loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10,5))\n",
    "_ = plt.plot(model_trainer.train_acc_logger, c = \"y\")\n",
    "_ = plt.plot(model_trainer.val_acc_logger, c = \"k\")\n",
    "\n",
    "_ = plt.title(\"Average IoU\")\n",
    "_ = plt.legend([\"Training IoU\", \"Validation IoU\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, mask, bbox, labels = next(iter(model_trainer.test_loader))\n",
    "model_trainer.eval()\n",
    "with torch.no_grad():\n",
    "    pred_out = model_trainer(images.to(device)).argmax(1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(images[0:16], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid((mask[0:16]).unsqueeze(1).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(pred_out[0:16].unsqueeze(1).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_F2Qy9WPMUG"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1568948469315,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "dKMx57tEPMUH",
    "outputId": "7590031a-2a9e-4701-9799-320155e5efd6"
   },
   "outputs": [],
   "source": [
    "# Call the evaluate function and pass the evaluation/test dataloader etc\n",
    "test_acc = model_trainer.evaluate_model(train_test_val=\"test\")\n",
    "print(\"The Test Average IoU is: %.2f\" %(test_acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet18_STL10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
