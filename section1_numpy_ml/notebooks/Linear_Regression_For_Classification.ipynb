{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Linear Regression For Classification </h1> <br>\n",
    "<img src=\"../data/Linear_Regression.gif\" width=\"1200\" align=\"center\">\n",
    "Animation of our \"model\" at each step when training with gradient descent\n",
    "\n",
    "<b>With our new knowledge of Python and Numpy, lets explore some linear models</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading the data </h3>\n",
    "Lets load some \"toy\" data that we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading our data\n",
    "npzfile = np.load(\"../data/toy_data_two_moon.npz\")\n",
    "\n",
    "#The compressed Numpy file is split up into 4 parts\n",
    "#Train inputs (x) and target outputs (y)\n",
    "X_train = npzfile['arr_0']\n",
    "y_train = npzfile['arr_2']\n",
    "#Test inputs (x) and target outputs (y)\n",
    "X_test = npzfile['arr_1']\n",
    "y_test = npzfile['arr_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Let's plot our data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see what the data looks like\n",
    "fig = plt.figure(figsize = (20, 10))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=y_train[:,0], s=100, edgecolor='k')\n",
    "plt.title(\"Train data\")\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=y_test[:,0], s=100, edgecolor='k')\n",
    "plt.title(\"Test data\")\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is randomly sampled from an odd looking distribution, the colour of the dots (as represented by y_test[:,0], a one or zero) is what the output of our model SHOULD be (aka the \"Ground Truth Data\"). Note that each data point is a vector of two values (the \"x and y\" values), therefore there will only be two parameters in our linear model. <br>\n",
    "In order to include a bias term let's augment the data by adding a \"one\" to every input datapoint. This gives us three values for every datapoint, the parameter associated with the constant \"one\" input is the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bias = np.c_[X_train,np.ones([y_train.size,1])]\n",
    "X_test_bias = np.c_[X_test, np.ones([y_test.size,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training a linear regression model in closed form</h2>\n",
    "We recall that the parameters of a linear regression can be obtained in closed-form. If $\\boldsymbol{X} \\in \\mathbb{R}^{n \\times m}$ denotes the matrix of input data (each column is a training sample) and  $\\boldsymbol{Y} \\in \\mathbb{R}^{p \\times m}$ is the matrix of desired outputs, then the parameters of the model is obtained as\n",
    "$\\theta^\\top = (\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our model, we need the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes the model inputs and parameters and returns the outout of the model, rounded to 1 or 0\n",
    "#AKA we threshold the output to 1 or 0 at 0.5\n",
    "\n",
    "#####To Do! #####\n",
    "#create a function called \"predict\" that will take in the model weights and and data inputs and\n",
    "#calculate the output of the model and threshold it!#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the values of $\\theta$ using the closed form solution defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_lin = ###To Do! Using the closed-from equation above calculate the values of Theta###\n",
    "print(\"The values of Theta are:\\n\", theta_lin[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cacluate the thresholded outputs of the model\n",
    "y_test_hat_lin = ####To Do! call the function you wrote \"predict\" to get the output for the TEST input data\n",
    "\n",
    "acc_lin = ####To Do! - write some code to caclulate the percentage accuracy of the model##\n",
    "print(\"Accuracy of linear model(closed form): %.2f%% \" %(acc_lin*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the models predicted output labels (1/0) lets vislualise the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=y_test_hat_lin[:,0], s=25, edgecolor='k')\n",
    "plt.title(\"Test data, linear model estimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data is 2D we can visualise our model as a line. We need to first transform it from the \"general form\" of the line equation: <br>\n",
    "$0 = Ax + By + C$ <br>\n",
    "To the more familiar form: <br>\n",
    "$y = mx + C$ <br>\n",
    "aka:\n",
    "$y = -\\frac{(Ax + C)}{B}$ <br>\n",
    "\n",
    "In our case:<br>\n",
    "$0.5 = \\theta_0x + \\theta_1y + \\theta_2$ <br>\n",
    "$y = -\\frac{(\\theta_0x + \\theta_2 - 0.5)}{\\theta_1}$ <br>\n",
    "\n",
    "NOTE the line equation should equal 0.5 not 0 in our case due to where we thresholded the output, the line is therefore the \"decision boundary\" not stictly the line given by our model<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = ####To Do!create a vector of 100 points from -2 to 2.5\n",
    "y_values = ####To Do! Using the equation above calculate the values of y using the theta values#####\n",
    "plt.plot(x_values, y_values)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=y_test_hat_lin[:,0], s=25, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training a model with GD </h2>\n",
    "In doing so, we need a function to <br>\n",
    "1- compute the loss with respect to the inputs and the parameters of the model <br>\n",
    "2- compute the gradient of the model with respect to its parameters $\\theta$\n",
    "\n",
    "We recall the loss of the linear regression as\n",
    "\\begin{align}\n",
    "L(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\|\\theta^\\top \\boldsymbol{x}_i - y_i\\|^2\n",
    "\\end{align}\n",
    "\n",
    "Now it is easy to see that\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{m} \\sum_{i=1}^m 2(\\theta^\\top \\boldsymbol{x}_i - y_i)\\boldsymbol{x}_i\n",
    "\\end{align}\n",
    "\n",
    "The function below implements this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###To Do! Create a function caled \"compute_grad_loss\" that takes in:\n",
    "#Input data\n",
    "#Output targets\n",
    "#Model weights\n",
    "\n",
    "#and Returns (in this order):\n",
    "#the loss\n",
    "#the gradient vector\n",
    "\n",
    "#NOTE - be careful of tensor shapes!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can perform multiple itteration of GD to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta is the parameters of the linear model\n",
    "# To learn them, we randomly initilize them below\n",
    "theta = ###To Do! randomly initialise the values of theta###\n",
    "lr = ##To Do! try some different learning rates!##\n",
    "\n",
    "#number of times we itterate over the dataset (epoch)\n",
    "max_epoch = ##To Do! try different numbers of epochs, untill your model converges!##\n",
    "\n",
    "loss = [] #keep track of the loss values\n",
    "acc = [] #keep track of the accuracy \n",
    "for epoch in range(max_epoch):\n",
    "    y_test_hat = ###To Do! Call the function \"predict\" to get the current output predictions for the TEST data!\n",
    "    acc.append(##To Do! - write some code to caclulate the percentage accuracy of the model##)\n",
    "\n",
    "    # call the compute_grad_loss that is implemented above to \n",
    "    # measure the loss and the gradient\n",
    "    tmpLoss, grad_vec = ###To Do! Call the function compute_grad_loss passing in the TRAINING data etc\n",
    "    loss.append(tmpLoss)\n",
    "    #update the theta parameter according to the GD here\n",
    "    theta = ###To Do!\n",
    "    \n",
    "print(\"Accuracy of linear model(GD): %.2f%% \" %(acc[-1]*100))\n",
    "print(\"The values of Theta are:\\n\", theta[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc)\n",
    "plt.title(\"Model accuracy per itteration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
