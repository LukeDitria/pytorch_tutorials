{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Linear Regression For Classification </h1> <br>\n",
    "<img src=\"../data/Linear_Regression.gif\" width=\"1200\" align=\"center\">\n",
    "Animation of our \"model\" at each step when training with gradient descent\n",
    "\n",
    "<b>With our new knowledge of Python and Numpy, lets explore some linear models</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading the data </h3>\n",
    "Lets load some \"toy\" data that we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can load your data using this cell\n",
    "npzfile = np.load(\"../data/toy_data_two_moon.npz\") # toy_data.npz or toy_data_two_circles.npz\n",
    "\n",
    "#The compressed Numpy file is split up into 4 parts\n",
    "#Train inputs and target outputs\n",
    "#Test inputs and target outputs\n",
    "X_train = npzfile['arr_0']\n",
    "X_test = npzfile['arr_1']\n",
    "y_train = npzfile['arr_2']\n",
    "y_test = npzfile['arr_3']\n",
    "\n",
    "# remember that each row in X_train and X_test is a sample. so X_train[1,:] is the first training sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Let's plot our data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see what the data looks like\n",
    "fig = plt.figure(figsize = (20, 10))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=y_train[:,0], s=100, edgecolor='k')\n",
    "plt.title(\"Train data\")\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=y_test[:,0], s=100, edgecolor='k')\n",
    "plt.title(\"Test data\")\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is randomly sampled from an odd looking distribution, the colour of the dots (as represented by y_test[:,0], a one or zero) is what the output of our model SHOULD be (aka the \"Ground Truth Data\"). Note that each data point is a vector of two values (the \"x and y\" values), therefore there will only be two parameters in our linear model. <br>\n",
    "In order to include a bias term let's augment the data by adding a \"one\" to every input datapoint. This gives us three values for every datapoint, the parameter associated with the constant \"one\" input is the bias term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bias = np.c_[X_train,np.ones([y_train.size,1])]\n",
    "X_test_bias = np.c_[X_test, np.ones([y_test.size,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training a linear regression model in closed form</h2>\n",
    "We recall that the parameters of a linear regression can be obtained in closed-form. If $\\boldsymbol{X} \\in \\mathbb{R}^{m \\times n}$ denotes the matrix of input data (each column is a training sample) and  $\\boldsymbol{Y} \\in \\mathbb{R}^{m \\times p}$ is the matrix of desired outputs, then the parameters of the model is obtained as\n",
    "$\\theta^\\top = (\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our model, we need the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes the model inputs and parameters and returns the outout of the model, rounded to 1 or 0\n",
    "#AKA we threshold the output to 1 or 0 at 0.5\n",
    "def predict(X, theta):\n",
    "    return (np.matmul(X , theta) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the values of $\\theta$ using the closed form solution defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_lin = np.matmul(np.linalg.inv(np.matmul(X_train_bias.T,X_train_bias)),np.matmul(X_train_bias.T,y_train))\n",
    "print(\"The values of Theta are:\\n\", theta_lin[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_lin = predict(X_test_bias,theta_lin)\n",
    "acc_lin = float(sum(y_test_hat_lin == y_test))/ float(len(y_test))\n",
    "print(\"Accuracy of linear model(closed form): %.2f%% \" %(acc_lin*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the models predicted output labels (1/0) lets vislualise the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=y_test_hat_lin[:,0], s=25, edgecolor='k')\n",
    "plt.title(\"Test data, linear model estimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data is 2D we can visualise our model as a line. We need to first transform it from the \"general form\" of the line equation: <br>\n",
    "$0 = Ax + By + C$ <br>\n",
    "To the more familiar form: <br>\n",
    "$y = mx + C$ <br>\n",
    "aka:\n",
    "$y = -\\frac{(Ax + C)}{B}$ <br>\n",
    "\n",
    "In our case:<br>\n",
    "$0.5 = \\theta_0x + \\theta_1y + \\theta_2$ <br>\n",
    "$y = -\\frac{(\\theta_0x + \\theta_2 - 0.5)}{\\theta_1}$ <br>\n",
    "\n",
    "Note its 0.5 not 0 in our case due to where we thresholded the output, the line is therefore the \"decision boundary\" not stictly the line given by our model<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-2, 2.5, 100)\n",
    "y_values = -((theta_lin[0,0])*x_values + theta_lin[2,0] - 0.5)/theta_lin[1,0]\n",
    "plt.plot(x_values, y_values)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=y_test_hat_lin[:,0], s=25, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training a model with Gradient Decent GD </h2>\n",
    "\n",
    "[Gradient Descent, Step-by-Step by StatQuest](https://youtu.be/sDv4f4s2SB8?si=iClqYh2v3I7uf9WR)\n",
    "\n",
    "To do so, we need a function to <br>\n",
    "1- compute the loss with respect to the inputs and the parameters of the model <br>\n",
    "2- compute the gradient of the model with respect to its parameters $\\theta$\n",
    "\n",
    "The loss of the linear regression as\n",
    "\\begin{align}\n",
    "L(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\|\\theta^\\top \\boldsymbol{x}_i - y_i\\|^2\n",
    "\\end{align}\n",
    "\n",
    "Now it is easy to see that\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{m} \\sum_{i=1}^m 2(\\theta^\\top \\boldsymbol{x}_i - y_i)\\boldsymbol{x}_i\n",
    "\\end{align}\n",
    "\n",
    "The function below implements this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_loss(X, y, theta):\n",
    "    #this function will get X, a set of samples (each sample is a row in X),\n",
    "    #the corresponding labels in the array y and the current parameter of the regression model theta\n",
    "    \n",
    "    h = np.matmul(X , theta)\n",
    "\n",
    "    loss = np.mean((h - y)**2)\n",
    "    grad_vec = np.matmul(X.T , (h - y))/y.shape[0]\n",
    "    return loss, grad_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can perform multiple itteration of GD to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta is the parameters of the linear model\n",
    "# To learn them, we randomly initilize them below\n",
    "theta = np.random.randn(X_test_bias.shape[1],1)\n",
    "lr = 0.5\n",
    "\n",
    "#number of times we itterate over the dataset\n",
    "max_epoch = 50 \n",
    "\n",
    "loss = [] #keep track of the loss values\n",
    "acc = [] #keep track of the accuracy \n",
    "for epoch in range(max_epoch):\n",
    "    y_test_hat = predict(X_test_bias,theta)\n",
    "\n",
    "    acc.append(float(sum(y_test_hat == y_test))/ float(len(y_test)))\n",
    "\n",
    "    # call the compute_grad_loss that is implemented above to \n",
    "    # measure the loss and the gradient\n",
    "    tmpLoss, grad_vec = compute_grad_loss(X_train_bias, y_train, theta)\n",
    "    loss.append(tmpLoss)\n",
    "    #update the theta parameter according to the GD here\n",
    "    theta -= lr*grad_vec \n",
    "    \n",
    "print(\"Accuracy of linear model(GD): %.2f%% \" %(acc[-1]*100))\n",
    "print(\"The values of Theta are:\\n\", theta[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc)\n",
    "plt.title(\"Model accuracy per itteration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
