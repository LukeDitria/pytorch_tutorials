{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Variational Autoencoder </h1>\n",
    "Traditional Autoencoders form dense representations with not a lot of meaningful \"structure\". This is fine when all you want to do is compress an input and then reconstruct it, but what if you then want to generate new images by sampling the representation space (latent space)? Small movements is the latent space lead to large and discontinuous jumps in the reconstructed output. We need to apply an additional loss to force the created latent space to be smooth and nicely structured.\n",
    "<img src=\"https://miro.medium.com/max/1687/1*22cSCfmktNIwH5m__u2ffA.png\" width=\"1200\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhWb2qkq6Idq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as Datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vyfSkLIu6Id3"
   },
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "imageSize = 28\n",
    "lr = 1e-4\n",
    "# Number of Training epochs\n",
    "nepoch = 10\n",
    "# The size of the Latent Vector\n",
    "latent_size = 128\n",
    "root = \"../../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ab2W41mB6Id6"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "GPU_indx  = 0\n",
    "device = torch.device(GPU_indx if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create an MNIST dataset and dataloader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6195,
     "status": "ok",
     "timestamp": 1570409783041,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "RJUrSrOl6Id-",
    "outputId": "a37cfcb0-da67-4107-fc85-893028c5d2cf"
   },
   "outputs": [],
   "source": [
    "# Define our transform\n",
    "# We'll upsample the images to 32x32 as it's easier to contruct our network\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_set = Datasets.MNIST(root=root, train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batchSize,shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = Datasets.MNIST(root=root, train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batchSize, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QSYKspHT6IeB"
   },
   "source": [
    "## KL Divergence penalty (loss)\n",
    "\n",
    "The KL divergance between two normal distributions where:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x) = N(\\mu_p,\\sigma_p)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "q(x) = N(\\mu_q,\\sigma_q)\n",
    "\\end{equation*}\n",
    "                                \n",
    "                                \n",
    "Is                               \n",
    "\\begin{equation*}\n",
    "KL(p,q) = \\ln(\\frac{\\sigma_q}{\\sigma_p}) + \\frac{\\sigma_p^2 + (\\mu_p - \\mu_q)^2}{2\\sigma_q^2} - \\frac{1}{2}\n",
    "\\end{equation*}\n",
    "\n",
    "With a VAE using the Encoder we produce a $\\sigma$ and a $\\mu$ per dimension and sample from normal distribution (once per dimension) with the given $\\sigma$ and $\\mu$. We then pass this sampled vector to the Decoder which will try and reconstruct the original image.<br>\n",
    "The KL penalty (or loss) tries to force the distribution from the encoder to be that of a unit gaussian where $\\sigma=1$ and $\\mu =0$ (also known as a Standard Normal Distribution).<br>\n",
    "To do this we create a loss using the KL Divergence (a value that is always positive) between the distribution produced by the encoder and that of a unit gaussian.<br>\n",
    "\n",
    "\n",
    "So the above becomes:\n",
    "\\begin{equation*}\n",
    "p(x) = N(\\mu_p,\\sigma_p)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "q(x) = N(0,1)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "KL(p,q) = \\ln(\\frac{1}{\\sigma_p}) + \\frac{\\sigma_p^2 + (\\mu_p - 0)^2}{2*1^2} - \\frac{1}{2}\n",
    "\\end{equation*}\n",
    "\n",
    "Which we can simplify to:\n",
    "\\begin{equation*}\n",
    "KL(p,q) = -\\frac{1}{2}(2\\ln(\\sigma_p) - \\sigma_p^2 - \\mu_p^2 + 1)\n",
    "\\end{equation*}\n",
    "\n",
    "If we minimise this we bring our distribution closer to a unit gaussian <br><br>\n",
    "Note: $\\sigma$ must always be $\\ge0$, instead of forcing this on our network we usually use $\\ln(\\sigma^2)$ or the \"log variance\" in its place (literally the log of the variance $\\sigma^2$ which simplifies to $2\\ln(\\sigma)$ which we can see in our equation above). This value is continuous in the range of $-\\infty$ to $\\infty$\n",
    "\n",
    "<img src=\"../data/VAE_3d.gif\" width=\"1200\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n7ZMUFXx6IeC"
   },
   "source": [
    "## Why do we even bother??\n",
    "VAEs are generative models, that is we use them to create new data once trained, do to this with a VAE we simply sample from the distribution learnt at training time. <br>\n",
    "There is more then meets the eye with what is actually going in a VAE and what happens when we implement a KL penalty, for a good explanation we won't be able to beat check-out [this](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf) great write-up.\n",
    "\n",
    "[Also this](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxLSEi7e6IeD"
   },
   "outputs": [],
   "source": [
    "def vae_loss(recon, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(recon, x)\n",
    "    \n",
    "    # Here is our KL divergance loss implemented in code\n",
    "    # We will use the mean across the dimensions instead of the sum (which is common and would require different scaling)\n",
    "    kl_loss = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).mean()\n",
    "    \n",
    "    # We'll tune the \"force\" of KL divergance loss to get a good result \n",
    "    loss = recon_loss + 0.1 * kl_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgqQSvQg6IeG"
   },
   "source": [
    "## VAE Network\n",
    "The structure is very similar to a vanilla Auto Encoder with the addition of a $\\sigma$ output on the encoder.<br>\n",
    "It is functionally different as we sample from a standard normal distribution and scale it with $\\sigma$ and shift with $\\mu$.<br> This functionally the same as sampling from $N(\\mu,\\sigma)$<br>\n",
    "Note: we only do this during training, during test time we just use $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-TN2KYN6IeH"
   },
   "outputs": [],
   "source": [
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels, ch = 32, z = 32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, ch, 4, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch)\n",
    "        self.conv2 = nn.Conv2d(ch, 2*ch, 4, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(2*ch)\n",
    "        self.conv3 = nn.Conv2d(2*ch, 4*ch, 4, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(4*ch)\n",
    "\n",
    "        # Instead of flattening (and then having to unflatten) out our feature map and \n",
    "        # putting it through a linear layer we can just use a conv layer\n",
    "        # where the kernal is the same size as the feature map \n",
    "        # (in practice it's the same thing)\n",
    "        self.conv_mu = nn.Conv2d(4*ch, z, 4, 1)\n",
    "        self.conv_logvar = nn.Conv2d(4*ch, z, 4, 1)\n",
    "\n",
    "    # This function will sample from our distribution\n",
    "    def sample(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        mu = self.conv_mu(x)\n",
    "        logvar = self.conv_logvar(x)\n",
    "        x = self.sample(mu, logvar)\n",
    "        \n",
    "        return x, mu, logvar\n",
    "    \n",
    "# Decoder is the same as a Vanilla Auto Encoder \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels, ch = 32, z = 32):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(z, 4*ch, 4, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(4*ch)\n",
    "        self.conv2 = nn.ConvTranspose2d(4*ch, 2*ch, 4, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(2*ch)\n",
    "        self.conv3 = nn.ConvTranspose2d(2*ch, ch, 4, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(ch)\n",
    "        self.conv4 = nn.ConvTranspose2d(ch, channels, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, channel_in, ch = 16, z = 32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(channels = channel_in, ch = ch, z = z)\n",
    "        self.decoder = Decoder(channels = channel_in, ch = ch, z = z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoding, mu, logvar = self.encoder(x)\n",
    "        \n",
    "        # Only sample during training or when we want to generate new images\n",
    "        # just use mu otherwise\n",
    "        if self.training:\n",
    "            x = self.decoder(encoding)\n",
    "        else:\n",
    "            x = self.decoder(mu)\n",
    "            \n",
    "        return x, mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualize our data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6489,
     "status": "ok",
     "timestamp": 1570409783350,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "74l6KlI06IeK",
    "outputId": "8d7a863d-8de8-4ae0-c4c1-403ddb67eae5"
   },
   "outputs": [],
   "source": [
    "# Get a test image\n",
    "dataiter = iter(test_loader)\n",
    "test_images = dataiter.next()[0]\n",
    "\n",
    "# View the shape\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7133,
     "status": "ok",
     "timestamp": 1570409784001,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "GBE2TPmy6IeN",
    "outputId": "8344a523-4b6a-4adb-ecdd-c59fd617b137"
   },
   "outputs": [],
   "source": [
    "# Visualize the data!!!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(test_images[0:8], normalize=True)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create Network and Optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XuZmm4Jx6IeQ"
   },
   "outputs": [],
   "source": [
    "# Create our network\n",
    "vae_net = VAE(channel_in = 1, z = latent_size).to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.Adam(vae_net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Create loss logger\n",
    "loss_log = []\n",
    "train_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Network output</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7126,
     "status": "ok",
     "timestamp": 1570409784003,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "iteROyrA6IeT",
    "outputId": "51104e6b-af60-44b9-a7f6-5885749d92af"
   },
   "outputs": [],
   "source": [
    "# Pass through a test image to make sure everything is working\n",
    "recon_data, mu, logvar = vae_net(test_images.to(device))\n",
    "\n",
    "# View the Latent vector shape\n",
    "mu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Start training!</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1570410601202,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "bVFA6W6L6IeY",
    "outputId": "99cd69e0-8ff6-466b-fb59-4e4ba9f33271"
   },
   "outputs": [],
   "source": [
    "pbar = trange(0, nepoch, leave=False, desc=\"Epoch\")    \n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Loss: %.4f' % train_loss)\n",
    "    for i, data in enumerate(tqdm(train_loader, leave=False, desc=\"Training\")):\n",
    "\n",
    "        image = data[0].to(device)\n",
    "\n",
    "        # Forward pass the image in the data tuple\n",
    "        recon_data, mu, logvar = vae_net(image)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = vae_loss(recon_data, image, mu, logvar)\n",
    "        \n",
    "        # Log the loss\n",
    "        loss_log.append(loss.item())\n",
    "        train_loss = loss.item()\n",
    "\n",
    "        # Take a training step\n",
    "        vae_net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0ZrSDsR6Ief"
   },
   "source": [
    "## Results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 824609,
     "status": "ok",
     "timestamp": 1570410601500,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "vTr64GEm6Iej",
    "outputId": "4d715e03-42e0-4277-ec5a-1e5577c2b240"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7GW3ruNa6Ieo"
   },
   "source": [
    "Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 825552,
     "status": "ok",
     "timestamp": 1570410602450,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "yqC3cmVx6Ieo",
    "outputId": "8665a80d-bc7d-4be4-d8fa-89716e7391c7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(data[0][0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPRXvoNr6Ies"
   },
   "source": [
    "Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 825546,
     "status": "ok",
     "timestamp": 1570410602451,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "kX95LD-u6Iet",
    "outputId": "40c30d2b-837b-4143-a540-024b14789389"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(recon_data.detach().cpu()[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xBrRk9Hp6Iev"
   },
   "source": [
    "Random Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 826092,
     "status": "ok",
     "timestamp": 1570410603004,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "5Dm8T70o6Iex",
    "outputId": "dd0e50e3-780b-4694-d550-cbd3c258ba2b"
   },
   "outputs": [],
   "source": [
    "rand_samp = vae_net.decoder(mu + torch.randn_like(mu))\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(rand_samp.detach().cpu()[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-_CImBn6Ie0"
   },
   "source": [
    "# Interpolation in Latent Space\n",
    "Now that we've trained our VAE we can explore the \"MNIST Latent Space\" it has created. <br>\n",
    "We first use our validation images and class labels to find the mean latent vector for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 830953,
     "status": "ok",
     "timestamp": 1570410607873,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "7_ZEGuhF6Ie1",
    "outputId": "2ca91ef6-c10c-4b05-f27f-b74a366aaeaa"
   },
   "outputs": [],
   "source": [
    "# Initialise the class means to 0\n",
    "class_means = torch.zeros(10, latent_size)\n",
    "\n",
    "vae_net.eval()\n",
    "# Loop through all the data in the validation set\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        images, labels = data\n",
    "        recon_data, mu, _ = vae_net(images.to(device))\n",
    "        \n",
    "        # For each batch sum up the latent vectors of the same class\n",
    "        # (Use a matrix of one hot coded vectors to make it easy)\n",
    "        class_matrix = F.one_hot(labels, 10).t().type(torch.FloatTensor)\n",
    "        class_means += torch.matmul(class_matrix, mu.squeeze().detach().cpu())\n",
    "\n",
    "# In the validation set each class has 1000 images so to find the mean vectors we divide by 1000\n",
    "class_means /= 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFTYB8Xg6Ie5"
   },
   "source": [
    "Recondstruct the means using the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3qMLKiO6Ie7"
   },
   "outputs": [],
   "source": [
    "# Reshape the mean classes to the appropriate shape \n",
    "class_means = class_means.view(10, latent_size, 1, 1)\n",
    "\n",
    "# We only need to pass the latent vectors through our decoder\n",
    "recon_means = vae_net.decoder(class_means.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhGtytFf6IfD"
   },
   "source": [
    "Plot out our class means!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 832301,
     "status": "ok",
     "timestamp": 1570410609232,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "1mZo8Y6_6IfJ",
    "outputId": "e80d67df-b7de-42f8-9bdc-3116d4a1f4e1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "out = vutils.make_grid(recon_means.detach().cpu(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wuiSJmX76IfW"
   },
   "source": [
    "Interpolate between two class means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZl3Vg5t6IfZ"
   },
   "outputs": [],
   "source": [
    "# Pick the two classes to move between\n",
    "start_class = 1\n",
    "end_class = 8\n",
    "\n",
    "# Number of interpolation steps\n",
    "num_steps = 100\n",
    "steps = torch.linspace(0,1,num_steps)\n",
    "\n",
    "# Get the vector pointing from one class to the other\n",
    "diff_vector = class_means[end_class] - class_means[start_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vh1pWb866Ifi"
   },
   "source": [
    "Take \"num_steps\" from the \"start_class\" to the \"end_class\" along the \"diff_vector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBiC3_WZ6Ifk"
   },
   "outputs": [],
   "source": [
    "latent_steps = class_means[start_class] + (steps.view(num_steps, 1, 1, 1) * diff_vector.view(1, latent_size, 1, 1))\n",
    "recon_steps = vae_net.decoder(latent_steps.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1VigySL6Ifr"
   },
   "source": [
    "Visulise the interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 832289,
     "status": "ok",
     "timestamp": 1570410609233,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "dvkc6DZX6Ift",
    "outputId": "9bb9921f-8945-4528-d7a1-38c31e993daf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(recon_steps.detach().cpu(), normalize=True)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZZDqGWP6Ifw"
   },
   "source": [
    "Animate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1570410637193,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "XkoT838E6Ifx",
    "outputId": "0151c9e0-3ce5-4d23-b1d3-e42572ca13d6"
   },
   "outputs": [],
   "source": [
    "for i in range(num_steps):\n",
    "    plt.imshow(((recon_steps[i, 0] + 1) / 2).detach().cpu())\n",
    "    plt.pause(0.01)\n",
    "    clear_output(True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VAE.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
