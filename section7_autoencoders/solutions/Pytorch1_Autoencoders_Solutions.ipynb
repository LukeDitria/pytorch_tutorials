{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Autoencoder </h1>\n",
    "Autoencoders are a fairly straightforward network structure, characterised by a \"bottleneck\" where the input is \"compressed\" before being upsampled again. This network can be used to create compressed representations of images by training the model to reconstruct the input on the output. It could also be used for our segmentation problem! However in segmentation, we don't really want our network to compress our image, we want it to do some \"work\" and then give us a segmented version of the input!\n",
    "<img src=\"https://miro.medium.com/max/3148/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png\" width=\"750\" align=\"center\">\n",
    "\n",
    "[Autoencoders](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhWb2qkq6Idq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as Datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vyfSkLIu6Id3"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Define learning rate\n",
    "lr = 1e-4\n",
    "\n",
    "# Number of Training epochs\n",
    "nepoch = 10\n",
    "\n",
    "# Dataset location\n",
    "root = \"../../datasets\"\n",
    "\n",
    "# Scale for the added image noise\n",
    "noise_scale = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ab2W41mB6Id6"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "gpu_indx  = 0\n",
    "device = torch.device(gpu_indx if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create an MNIST dataset and dataloader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6195,
     "status": "ok",
     "timestamp": 1570409783041,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "RJUrSrOl6Id-",
    "outputId": "a37cfcb0-da67-4107-fc85-893028c5d2cf"
   },
   "outputs": [],
   "source": [
    "# Define our transform\n",
    "# We'll upsample the images to 32x32 as it's easier to contruct our network\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_set = Datasets.MNIST(root=root, train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size,shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = Datasets.MNIST(root=root, train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transpose Convolution</h3>\n",
    "The AE model introduces a new layer-type the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\">Transpose convolution</a> (sometimes called \"Deconvolution\")<br>\n",
    "The transpose convolution is a \"learnable upsampling\" method and is essentially the opposite of a convolution! We take a single feature (pixel) in our feature map and replicate it and multiply by a kernel, any overlapping sections are added together. The easiest way to understand them is with the following animation (where the blue square is the input and green is the output).\n",
    "<img src=\"https://miro.medium.com/max/986/1*yoQ62ckovnGYV2vSIq9q4g.gif\" width=\"750\" align=\"center\">\n",
    "\n",
    "[Blog: Transposed Convolutions explained](https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8)<br>\n",
    "[Blog: Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgqQSvQg6IeG"
   },
   "source": [
    "## AE Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We split up our network into two parts, the Encoder and the Decoder\n",
    "# class DownBlock(nn.Module):\n",
    "#     def __init__(self, channels_in, channels_out):\n",
    "#         super(DownBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(channels_in, channels_out, 3, 2, 1)\n",
    "#         self.bn1 = nn.BatchNorm2d(channels_out)\n",
    "        \n",
    "#         self.conv2 = nn.Conv2d(channels_out, channels_out, 3, 1, 1)\n",
    "#         self.bn2 = nn.BatchNorm2d(channels_out)\n",
    "        \n",
    "#         self.conv3 = nn.Conv2d(channels_in, channels_out, 3, 2, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x_skip = self.conv3(x)\n",
    "        \n",
    "#         x = F.relu(self.bn1(self.conv1(x)))\n",
    "#         x = self.conv2(x) + x_skip\n",
    "        \n",
    "#         return F.relu(self.bn2(x))\n",
    "    \n",
    "# # We split up our network into two parts, the Encoder and the Decoder\n",
    "# class UpBlock(nn.Module):\n",
    "#     def __init__(self, channels_in, channels_out):\n",
    "#         super(UpBlock, self).__init__()\n",
    "#         self.bn1 = nn.BatchNorm2d(channels_in)\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(channels_in, channels_in, 3, 1, 1)\n",
    "#         self.bn2 = nn.BatchNorm2d(channels_in)\n",
    "\n",
    "#         self.conv2 = nn.ConvTranspose2d(channels_in, channels_out, 3, 2, 1, 1)\n",
    "        \n",
    "#         self.conv3 = nn.ConvTranspose2d(channels_in, channels_out, 3, 2, 1, 1)\n",
    "\n",
    "#     def forward(self, x_in):\n",
    "#         x = F.relu(self.bn2(x_in))\n",
    "        \n",
    "#         x_skip = self.conv3(x)\n",
    "        \n",
    "#         x = F.relu(self.bn2(self.conv1(x)))\n",
    "#         return self.conv2(x) + x_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We split up our network into two parts, the Encoder and the Decoder\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, channels, ch=32, z=32):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.conv_1 = nn.Conv2d(channels, ch, 3, 1, 1)\n",
    "\n",
    "#         self.conv_block1 = DownBlock(ch, ch)\n",
    "#         self.conv_block2 = DownBlock(ch, ch * 2)\n",
    "#         self.conv_block3 = DownBlock(ch * 2, ch * 4)\n",
    "\n",
    "#         self.conv_out = nn.Conv2d(4 * ch, z, 4, 1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv_1(x))\n",
    "        \n",
    "#         x = self.conv_block1(x)\n",
    "#         x = self.conv_block2(x)\n",
    "#         x = self.conv_block3(x)\n",
    "\n",
    "#         return self.conv_out(x)\n",
    "    \n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, channels, ch = 32, z = 32):\n",
    "#         super(Decoder, self).__init__()\n",
    "        \n",
    "#         self.conv1 = nn.ConvTranspose2d(z, 4 * ch, 4, 1)\n",
    "        \n",
    "#         self.conv_block1 = UpBlock(4 * ch, 2 * ch)\n",
    "#         self.conv_block2 = UpBlock(2 * ch, ch)\n",
    "#         self.conv_block3 = UpBlock(ch, ch)\n",
    "#         self.bn1 = nn.BatchNorm2d(ch)\n",
    "\n",
    "#         self.conv_out = nn.Conv2d(ch, channels, 3, 1, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "        \n",
    "#         x = self.conv_block1(x)\n",
    "#         x = self.conv_block2(x)\n",
    "#         x = F.relu(self.bn1(self.conv_block3(x)))\n",
    "\n",
    "#         return torch.tanh(self.conv_out(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-TN2KYN6IeH"
   },
   "outputs": [],
   "source": [
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels, ch=32, z=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, ch, 3, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch)\n",
    "        self.conv2 = nn.Conv2d(ch, 2 * ch, 3, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(2 * ch)\n",
    "        self.conv3 = nn.Conv2d(2 * ch, 4 * ch, 3, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(4 * ch)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(4 * ch, z, 4, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        return self.conv_out(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels, ch = 32, z = 32):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(z, 4 * ch, 4, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(4 * ch)\n",
    "        self.conv2 = nn.ConvTranspose2d(4 * ch, 2 * ch, 3, 2, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(2 * ch)\n",
    "        self.conv3 = nn.ConvTranspose2d(2 * ch, ch, 3, 2, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(ch)\n",
    "        self.conv4 = nn.ConvTranspose2d(ch, ch, 3, 2, 1, 1)\n",
    "        self.bn4 = nn.BatchNorm2d(ch)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(ch, channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn3(self.conv4(x)))\n",
    "\n",
    "        return torch.tanh(self.conv_out(x))\n",
    "    \n",
    "class AE(nn.Module):\n",
    "    def __init__(self, channel_in, ch=16, z=32):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = Encoder(channels=channel_in, ch=ch, z=z)\n",
    "        self.decoder = Decoder(channels=channel_in, ch=ch, z=z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoding = self.encoder(x)\n",
    "        x = self.decoder(encoding)\n",
    "        return x, encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualize our data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6489,
     "status": "ok",
     "timestamp": 1570409783350,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "74l6KlI06IeK",
    "outputId": "8d7a863d-8de8-4ae0-c4c1-403ddb67eae5"
   },
   "outputs": [],
   "source": [
    "# Get a test image\n",
    "dataiter = iter(test_loader)\n",
    "test_images = dataiter.next()[0]\n",
    "# View the shape\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7133,
     "status": "ok",
     "timestamp": 1570409784001,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "GBE2TPmy6IeN",
    "outputId": "8344a523-4b6a-4adb-ecdd-c59fd617b137"
   },
   "outputs": [],
   "source": [
    "# Visualize the data!!!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(test_images[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-noising Autoencoder\n",
    "While an Autoencoder can be used to simply compress the input into a lower-dimentional space lets also see how we can use it to remove some noise from an image!<br>\n",
    "We're going to simulate some [salt-and-pepper noise!](https://en.wikipedia.org/wiki/Salt-and-pepper_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data!!!\n",
    "plt.figure(figsize = (20, 10))\n",
    "random_sample = (torch.bernoulli((1 - noise_scale) * torch.ones_like(test_images)) * 2) - 1\n",
    "noisy_test_img = random_sample * test_images\n",
    "\n",
    "out = vutils.make_grid(noisy_test_img[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create Network and Optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XuZmm4Jx6IeQ"
   },
   "outputs": [],
   "source": [
    "# The size of the Latent Vector\n",
    "latent_size = 128\n",
    "\n",
    "# Create our network\n",
    "ae_net = AE(channel_in=1, z=latent_size).to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.Adam(ae_net.parameters(), lr=lr)\n",
    "\n",
    "# MSE loss for reconstruction!\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "loss_log = []\n",
    "train_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Network output</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7126,
     "status": "ok",
     "timestamp": 1570409784003,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "iteROyrA6IeT",
    "outputId": "51104e6b-af60-44b9-a7f6-5885749d92af"
   },
   "outputs": [],
   "source": [
    "# Pass through a test image to make sure everything is working\n",
    "recon_data, encoding = ae_net(test_images.to(device))\n",
    "\n",
    "# View the Latent vector shape\n",
    "encoding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Start training!</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1570410601202,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "bVFA6W6L6IeY",
    "outputId": "99cd69e0-8ff6-466b-fb59-4e4ba9f33271"
   },
   "outputs": [],
   "source": [
    "pbar = trange(0, nepoch, leave=False, desc=\"Epoch\")    \n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Loss: %.4f' % train_loss)\n",
    "    for i, data in enumerate(tqdm(train_loader, leave=False, desc=\"Training\")):\n",
    "\n",
    "        image = data[0].to(device)\n",
    "        \n",
    "        random_sample = (torch.bernoulli((1 - noise_scale) * torch.ones_like(image)) * 2) - 1\n",
    "        noisy_img = random_sample * image\n",
    "        \n",
    "        # Forward pass the image in the data tuple\n",
    "        recon_data, _ = ae_net(noisy_img)\n",
    "        \n",
    "        # Calculate the MSE loss\n",
    "        loss = loss_func(recon_data, image)\n",
    "        \n",
    "        # Log the loss\n",
    "        loss_log.append(loss.item())\n",
    "        train_loss = loss.item()\n",
    "        \n",
    "        # Take a training step\n",
    "        ae_net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0ZrSDsR6Ief"
   },
   "source": [
    "## Results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 824609,
     "status": "ok",
     "timestamp": 1570410601500,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "vTr64GEm6Iej",
    "outputId": "4d715e03-42e0-4277-ec5a-1e5577c2b240"
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(loss_log)\n",
    "_ = plt.title(\"MSE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 825552,
     "status": "ok",
     "timestamp": 1570410602450,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "yqC3cmVx6Ieo",
    "outputId": "8665a80d-bc7d-4be4-d8fa-89716e7391c7"
   },
   "outputs": [],
   "source": [
    "# Ground Truth\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(test_images[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Input\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(noisy_test_img[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 825546,
     "status": "ok",
     "timestamp": 1570410602451,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "kX95LD-u6Iet",
    "outputId": "40c30d2b-837b-4143-a540-024b14789389"
   },
   "outputs": [],
   "source": [
    "# Reconstruction\n",
    "plt.figure(figsize = (20,10))\n",
    "recon_data, _ = ae_net(noisy_test_img.to(device))\n",
    "out = vutils.make_grid(recon_data.detach().cpu()[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VAE.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
