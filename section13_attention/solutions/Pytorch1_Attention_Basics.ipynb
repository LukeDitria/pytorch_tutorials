{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7483c6",
   "metadata": {},
   "source": [
    "# Attention Basics!\n",
    "In this tutorial we explore the Attention Mechanism, what it is, how it works and what it is for!\n",
    "<br>\n",
    "[Corresponding Video](https://youtu.be/2hheJ4BBrkY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8491bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd054b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory of the dataset\n",
    "data_set_root = \"../../datasets\"\n",
    "\n",
    "# Define transformations to be applied to the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize image data with mean 0.5 and standard deviation 0.5\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "dataset = datasets.MNIST(data_set_root, train=True, download=True, transform=transform)\n",
    "\n",
    "# Specify the number of examples to select randomly\n",
    "num_of_examples = 100\n",
    "\n",
    "# Randomly select indices from the dataset\n",
    "rand_perm = torch.randperm(dataset.data.shape[0])[:num_of_examples]\n",
    "\n",
    "# Extract and concatenate the images of randomly selected examples into a tensor\n",
    "dataset_tensor = torch.cat([dataset.__getitem__(i)[0].reshape(1, -1) for i in rand_perm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise the images\n",
    "out = torchvision.utils.make_grid(dataset_tensor.reshape(-1, 1, 28, 28), 10, normalize=True, pad_value=0.5)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca862285",
   "metadata": {},
   "source": [
    "## Indexing a Dataset\n",
    "At this point we would be familiar with indexing a tensor/array with an integer index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our index value\n",
    "q_index = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df99a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise the image at this index!\n",
    "plt.figure(figsize = (5,5))\n",
    "_ = plt.imshow(dataset_tensor[q_index].reshape(28, 28).numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864443a",
   "metadata": {},
   "source": [
    "## Indexing a Dataset with Matrix Multiplication\n",
    "Did you know we can do the same thing, but using Matrix multiplication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the index into a one-hot-coded vector with the same length as the number of samples\n",
    "# This will be our \"query\" vector (q)\n",
    "q_one_hot_vec = F.one_hot(torch.tensor([q_index]), num_of_examples)\n",
    "\n",
    "# Create a unique one-hot-coded vector for every image in our dataset\n",
    "# These will be our \"key\" vectors (k)\n",
    "k_one_hot = F.one_hot(torch.arange(num_of_examples), num_of_examples)\n",
    "\n",
    "# Randomly shuffle the keys and dataset to demonstrate that we can find the target image\n",
    "# even in a randomly organized dataset\n",
    "rand_perm = torch.randperm(num_of_examples)\n",
    "k_one_hot = k_one_hot[rand_perm]\n",
    "dataset_tensor_random = dataset_tensor[rand_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24751fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply our key vector with the dataset\n",
    "# Perform matrix multiplication between the query vector (q_one_hot_vec) and the transpose of the key vectors (k_one_hot.t())\n",
    "index_map = torch.mm(q_one_hot_vec, k_one_hot.t()).float()\n",
    "\n",
    "# Perform matrix multiplication between the resulting index map and the randomly shuffled dataset\n",
    "output = torch.mm(index_map, dataset_tensor_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e7aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the image at the specified index\n",
    "plt.figure(figsize=(5, 5))\n",
    "_ = plt.imshow(output.reshape(28, 28).numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460e23c",
   "metadata": {},
   "source": [
    "## Attention as a \"Soft\" Look-up\n",
    "What if we don't use \"hard\" one-hot coded vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size for each of the vectors\n",
    "vec_size = 512\n",
    "\n",
    "# Create a random query vector\n",
    "q_random_vec = torch.randn(1, vec_size)\n",
    "\n",
    "# Create a random key vector for each image in the dataset\n",
    "random_keys = torch.randn(num_of_examples, vec_size)\n",
    "\n",
    "# Calculate an \"attention map\" by performing matrix multiplication between the \n",
    "# query vector and the transpose of the key vectors\n",
    "attention_map = torch.mm(q_random_vec, random_keys.t()).float()\n",
    "\n",
    "# Calculate the Softmax over the attention map to obtain a probability distribution\n",
    "attention_map = F.softmax(attention_map, 1)\n",
    "\n",
    "# Use the attention map to perform a soft \"indexing\" over the dataset by \n",
    "# multiplying it with the dataset tensor\n",
    "output = torch.mm(attention_map, dataset_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac24f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The largest Softmax value is %f\" % attention_map.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise the image we get as a result!\n",
    "plt.figure(figsize = (5,5))\n",
    "_ = plt.imshow(output.reshape(28, 28).numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a9d7e",
   "metadata": {},
   "source": [
    "## Multiple Queries\n",
    "We can also perform multiple queries at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1101de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size for each of the vectors\n",
    "vec_size = 32\n",
    "\n",
    "# Number of Queries\n",
    "num_q = 8\n",
    "\n",
    "# Create random query vectors\n",
    "q_random_vec = torch.randn(num_q, vec_size)\n",
    "\n",
    "# Create a random key vector for each image in the dataset\n",
    "random_keys = torch.randn(num_of_examples, vec_size)\n",
    "\n",
    "# Calculate an \"attention map\" by performing matrix multiplication between the \n",
    "# query vectors and the transpose of the key vectors\n",
    "attention_map = torch.mm(q_random_vec, random_keys.transpose(0, 1)).float()\n",
    "\n",
    "# Calculate the Softmax over the attention map to obtain a probability distribution\n",
    "attention_map = F.softmax(attention_map, -1)\n",
    "\n",
    "# Use the attention map to perform a soft \"indexing\" over the dataset by \n",
    "# multiplying it with the dataset tensor\n",
    "output = torch.mm(attention_map, dataset_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af670e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(output.reshape(num_q, 1, 28, 28), 8, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2fca92",
   "metadata": {},
   "source": [
    "## Multi-Headed Attention\n",
    "We can also perform Attention multiple times in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size for each of the vectors\n",
    "vec_size = 32\n",
    "\n",
    "# Number of Queries\n",
    "num_q = 8\n",
    "\n",
    "# Number of Heads\n",
    "num_heads = 4\n",
    "\n",
    "# Create random query vectors\n",
    "q_random_vec = torch.randn(num_heads, num_q, vec_size)\n",
    "\n",
    "# Create a random key vector for each image in the dataset\n",
    "random_keys = torch.randn(num_heads, num_of_examples, vec_size)\n",
    "\n",
    "# Calculate an \"attention map\" by performing batch matrix multiplication between the \n",
    "# query vectors and the transpose of the key vectors\n",
    "attention_map = torch.bmm(q_random_vec, random_keys.transpose(1, 2)).float()\n",
    "\n",
    "# Calculate the Softmax over the attention map to obtain a probability distribution\n",
    "attention_map = F.softmax(attention_map, 2)\n",
    "\n",
    "# Use the attention map to perform a soft \"indexing\" over the dataset by \n",
    "# multiplying it with the dataset tensor\n",
    "output = torch.bmm(attention_map, dataset_tensor.unsqueeze(0).expand(num_heads, num_of_examples, -1))\n",
    "\n",
    "# Reshape the output tensor for visualization\n",
    "out_reshape = output.reshape(num_heads, num_q, 28, 28).transpose(1, 2).reshape(num_heads, 1, 28, num_q*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab43da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(out_reshape, 1, normalize=True, pad_value=0.5)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7718c007",
   "metadata": {},
   "source": [
    "## Pytorch Multi-Head Attention\n",
    "Of course Pytorch has it's own implementation of Multi-Head Attention!<br>\n",
    "\n",
    "[Pytorch MultiheadAttention](https://pytorch.org/docs/2.1/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention)\n",
    "\n",
    "We'll go into more detail on how to use it in later examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b6432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size for each of the vectors\n",
    "vec_size = 32\n",
    "\n",
    "# Number of Heads\n",
    "num_heads = 8\n",
    "\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "\n",
    "# Create a batch of a single random query vector\n",
    "query = torch.randn(batch_size, 1, num_heads * vec_size)\n",
    "\n",
    "# Create random key and value vectors for each image in the dataset\n",
    "key = torch.randn(batch_size, num_of_examples, num_heads * vec_size)\n",
    "value = torch.randn(batch_size, num_of_examples, num_heads * vec_size)\n",
    "\n",
    "# Initialize a MultiheadAttention module with specified parameters\n",
    "multihead_attn = nn.MultiheadAttention(num_heads * vec_size, num_heads, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass through the Multi-Head Attention module\n",
    "# Returns the attention output and the attention weights\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value, average_attn_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the output of the forward pass from Multi-Head Attention module\n",
    "\n",
    "# Softmaxed \"attention mask\" shape\n",
    "print(\"Softmax Attention Mask:\", attn_output_weights.shape)\n",
    "\n",
    "# Attention output shape\n",
    "print(\"Attention Output:\", attn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a9749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each query/key/value vector is passed through a \"projection\" aka a learnable linear layer before\n",
    "# the attention mechanism \n",
    "# As they are all the same size Pytorch creates a single block of parameters splits it into 3 \n",
    "# Before doing a forward pass of each*\n",
    "print(\"Projection weight size\", multihead_attn.in_proj_weight.shape)\n",
    "\n",
    "# *Most of the time, doing a deep dive into the implementation Pytorch tries to do a lot of optimisation\n",
    "# to try and be efficient as possible depending on the use-case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9bb9b",
   "metadata": {},
   "source": [
    "## Train a Multi-Head Attention\n",
    "Lets train a model with attention that when given an image will try to find an image within its corresponding fixed \"dataset\" that closely matches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionTest(nn.Module):\n",
    "    def __init__(self, num_of_examples=100, embed_dim=784, num_heads=4):\n",
    "        super(AttentionTest, self).__init__()\n",
    "        \n",
    "        # Define an MLP for processing image data\n",
    "        self.img_mlp = nn.Sequential(\n",
    "            nn.Linear(784, embed_dim),   # Linear layer to embed image data into a lower-dimensional space\n",
    "            nn.LayerNorm(embed_dim),     # Layer normalization to normalize the embedded features\n",
    "            nn.ELU(),                    # ELU activation function for introducing non-linearity\n",
    "            nn.Linear(embed_dim, embed_dim)  # Another linear transformation for further processing\n",
    "        )\n",
    "        \n",
    "        # Define the Multi-Head Attention mechanism\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,     # Dimensionality of the embedding space\n",
    "            num_heads=num_heads,     # Number of attention heads\n",
    "            batch_first=True         # Whether the input is batch-first or sequence-first\n",
    "        )\n",
    "\n",
    "    def forward(self, img, values):\n",
    "        # Process the input image and values through the MLP layers\n",
    "        img_ = self.img_mlp(img)\n",
    "        values_ = self.img_mlp(values)\n",
    "\n",
    "        # Apply the Multi-Head Attention mechanism\n",
    "        attn_output, attn_output_weights = self.mha(img_, values_, values_)\n",
    "        \n",
    "        # Compute the output using the attention weights and the original values\n",
    "        output = torch.bmm(attn_output_weights, values)\n",
    "\n",
    "        return output, attn_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensionality of the embedding space\n",
    "embed_dim = 256\n",
    "\n",
    "# Define the number of attention heads\n",
    "num_heads = 1\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9361666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a DataLoader for training the model\n",
    "train_loader = dataloader.DataLoader(\n",
    "    dataset,                   # Dataset to load\n",
    "    shuffle=True,              # Shuffle the data for each epoch\n",
    "    batch_size=batch_size,     # Batch size for training\n",
    "    num_workers=4,             # Number of processes to use for data loading\n",
    "    drop_last=True             # Drop the last incomplete batch if it's smaller than the batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb795f",
   "metadata": {},
   "source": [
    "## Initialize Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d3f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the AttentionTest model\n",
    "mha_model = AttentionTest(\n",
    "    num_of_examples=num_of_examples,   # Number of examples in the dataset\n",
    "    embed_dim=embed_dim,               # Dimensionality of the embedding space\n",
    "    num_heads=num_heads                # Number of attention heads\n",
    ").to(device)                           # Move the model to the specified device\n",
    "\n",
    "# Define the Adam optimizer for training the model\n",
    "optimizer = optim.Adam(\n",
    "    mha_model.parameters(),  # Parameters to optimize\n",
    "    lr=1e-4                   # Learning rate\n",
    ")\n",
    "\n",
    "# List to store the training loss for each epoch\n",
    "loss_logger = []\n",
    "\n",
    "# Duplicate the data value tensor for each batch element and move it to the specified device\n",
    "values_tensor = dataset_tensor.unsqueeze(0).expand(batch_size, num_of_examples, -1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933ca68",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f5c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to training mode\n",
    "mha_model.train()\n",
    "\n",
    "# Loop through 10 epochs\n",
    "for _ in trange(10, leave=False):\n",
    "    # Iterate over the training data loader\n",
    "    for data, _train_loader in tqdm(train_loader, leave=False):\n",
    "        # Reshape the input data and move it to the specified device\n",
    "        q_img = data.reshape(data.shape[0], 1, -1).to(device)\n",
    "\n",
    "        # Perform forward pass through the Multi-Head Attention model\n",
    "        attn_output, attn_output_weights = mha_model(q_img, values_tensor)\n",
    "\n",
    "        # Calculate the mean squared error loss between the output and input images\n",
    "        loss = (attn_output - q_img).pow(2).mean()\n",
    "\n",
    "        # Zero the gradients, perform backward pass, and update model parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append the current loss value to the loss logger\n",
    "        loss_logger.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5cd0d6",
   "metadata": {},
   "source": [
    "## Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(loss_logger[100:])\n",
    "print(\"Minimum MSE loss %.4f\" % np.min(loss_logger))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd8061b",
   "metadata": {},
   "source": [
    "## Using Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bdb3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "mha_model.eval()\n",
    "\n",
    "# Perform forward pass without gradient computation\n",
    "with torch.no_grad():\n",
    "    # Reshape input data and move it to the specified device\n",
    "    q_img = data.reshape(data.shape[0], 1, -1).to(device)\n",
    "\n",
    "    # Perform forward pass through the Multi-Head Attention model\n",
    "    attn_output, attn_output_weights = mha_model(q_img, values_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f480e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given input, use the attention map to find the \"closest\" value-data matches\n",
    "index = 10\n",
    "top10 = attn_output_weights[index, 0].argsort(descending=True)[:10]\n",
    "top10_data = values_tensor[index, top10].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52032b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original image\n",
    "plt.figure(figsize=(3, 3))\n",
    "out = torchvision.utils.make_grid(q_img[index].cpu().reshape(-1, 1, 28, 28), 8, \n",
    "                                  normalize=True, pad_value=0.5)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 10 closest matches\n",
    "plt.figure(figsize=(10, 10))\n",
    "out = torchvision.utils.make_grid(top10_data.reshape(-1, 1, 28, 28), 10, normalize=True, pad_value=0.5)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the attention weights for the given input\n",
    "_ = plt.plot(attn_output_weights[index, 0].cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the target and returned images\n",
    "target_img = q_img.reshape(batch_size, 1, 28, 28)\n",
    "indexed_img = attn_output.reshape(batch_size, 1, 28, 28)\n",
    "\n",
    "# Stack the images with the returned image on top\n",
    "img_pair = torch.cat((indexed_img, target_img), 2).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab48490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the pairs of images, with the returned image on top and the target on bottom\n",
    "plt.figure(figsize=(10, 10))\n",
    "out = torchvision.utils.make_grid(img_pair, 8, normalize=True, pad_value=0.5)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
