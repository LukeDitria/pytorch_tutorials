{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Adding Self-Attention to a Convolutional Neural Network</h1>\n",
    "<br>\n",
    "<img src=\"../data/SelfAttention.jpg\" width=\"800\" align=\"center\">\n",
    "<br><br>\n",
    "^^ <b>Self Attention </b>^^<br>\n",
    "<br>\n",
    "In this notebook we will see how we can add Attention to a simple Convolutional Neural Network. Each spatial location of the feature map within the network will be able to query every other location. As every input can query every other input in the \"sequence\" (here the input sequence is the sequence of spatial locations) we call this type of Attention \"Self\" Attention.\n",
    "<br>\n",
    "Why?\n",
    "<br>\n",
    "We know that Convolutions operate locally over the spatial dimensions of our input. This means the receptive field of a feature in the output feature map has a receptive field of a very small part of the input. In order for the output of the very last convolution layer to have a receptive field of the whole input, we need to stack many convolution layers, hence the structure of a CNN. However this means that many Convolution layers early in the network can only operate on a small part of the input. Self Attention provides a way (with minimal parameters) to \"Mix\" all of the spatial locations so that all features have information about the whole input early on in the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of our mini batches\n",
    "batch_size = 64\n",
    "# How many itterations of our dataset\n",
    "num_epochs = 50\n",
    "# Optimizer learning rate\n",
    "learning_rate = 1e-4\n",
    "# Where to load/save the dataset from \n",
    "data_set_root = \"../../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU_indx if GPU is avaliable\n",
    "gpu_indx = 0\n",
    "device = torch.device(gpu_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create a transform for the input data </h3>\n",
    "As we have seen, we often wish to perform some operations on data before we pass it through our model. Such operations could be, cropping or resizing images, affine transforms and data normalizations. Pytorch's torchvision module has a large number of such \"transforms\" which can be strung together sequentially using the \"Compose\" function. <br>\n",
    "\n",
    "Pytorch's inbuilt datasets take a transform as an input and will apply this transform to the data before passing it to you! This makes preprocessing data really easy! We will see more about data preprocessing in a later lab!\n",
    "\n",
    "[torchvision.transforms](https://pytorch.org/vision/0.9/transforms.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a composition of transforms\n",
    "# transforms.Compose will perform the transforms in order\n",
    "# NOTE: some transform only take in a PIL image, others only a Tensor\n",
    "# EG Resize and ToTensor take in a PIL Image, Normalize takes in a Tensor\n",
    "# Refer to documentation\n",
    "transform = transforms.Compose([transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])]) \n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                          std=[0.229, 0.224, 0.225])]) \n",
    "# Note: ToTensor() will scale unit8 and similar type data to a float and re-scale to 0-1\n",
    "# Note: We are normalizing with the dataset mean and std "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create the training, testing and validation data</h3>\n",
    "When training many machine learning systems it is best practice to have our TOTAL dataset split into three segments, the training set, testing set and validation set. Up until now we have only had a train/test set split and have used the test set to gauge the performance during training. Though for the most \"unbiased\" results we should really not use our test set until training is done! So if we want to evaluate our model on an \"unseen\" part of the dataset we need another split - the validation set. <br><br>\n",
    "<b>Training set</b>   - the data we train our model on<br>\n",
    "<b>Validation set</b> - the data we use to gauge model performance during training<br>\n",
    "<b>Testing set</b>   - the data we use to \"rate\" our trained model<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.CIFAR10(data_set_root, train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(data_set_root, train=False, download=True, transform=test_transform)\n",
    "\n",
    "# We are going to split the test dataset into a train and validation set 90%/10%\n",
    "validation_split = 0.9\n",
    "\n",
    "# Determine the number of samples for each split\n",
    "n_train_examples = int(len(train_data) * validation_split)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "# The function random_split will take our dataset and split it randomly and give us dataset\n",
    "# that are the sizes we gave it\n",
    "# Note: we can split it into to more then two pieces!\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [n_train_examples, n_valid_examples],\n",
    "                                                       generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# IMPORTANT TO KNOW!!!!!!!!!\n",
    "# Here we pass the random_split function a manual seed, this is very important as if we did not do this then \n",
    "# everytime we randomly split our training and validation set we would get different splits!!!\n",
    "# For example if we saved our model and reloaded it in the future to train some more, the dataset that we now use to\n",
    "# train with will undoubtably contain datapoints that WERE in the validation set initially!!\n",
    "# Our model would therefore be trained with both validation and training data -- very bad!!!\n",
    "# Setting the manual seed to the same value everytime prevents this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Check the lengths of all the datasets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create the dataloader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training, Validation and Evaluation/Test Datasets\n",
    "# It is best practice to separate your data into these three Datasets\n",
    "# Though depending on your task you may only need Training + Evaluation/Test or maybe only a Training set\n",
    "# (It also depends on how much data you have)\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataloader\n",
    "train_loader = dataloader.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = dataloader.DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader  = dataloader.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create the CNN with Self Attention</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, channels_in):\n",
    "        # Call the __init__ function of the parent nn.module class\n",
    "        super(CNN, self).__init__()\n",
    "        # Define Convolution Layers\n",
    "        self.conv1 = nn.Conv2d(channels_in, 64, 3, 1, 1, padding_mode='reflect')\n",
    "        \n",
    "        # Define Layer Normalization and Multi-head Attention layers\n",
    "        self.norm = nn.LayerNorm(64)\n",
    "        self.mha = nn.MultiheadAttention(64, num_heads=1, batch_first=True)\n",
    "        self.scale = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        # Define additional Convolution Layers\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Define Dropout and Fully Connected Layers\n",
    "        self.do = nn.Dropout(0.5)\n",
    "        self.fc_out = nn.Linear(128*4*4, 10)\n",
    "        \n",
    "    def use_attention(self, x):\n",
    "        # Reshape input for multi-head attention\n",
    "        bs, c, h, w = x.shape\n",
    "        x_att = x.reshape(bs, c, h * w).transpose(1, 2)  # BSxHWxC\n",
    "        \n",
    "        # Apply Layer Normalization\n",
    "        x_att = self.norm(x_att)\n",
    "        # Apply Multi-head Attention\n",
    "        att_out, att_map  = self.mha(x_att, x_att, x_att)\n",
    "        return att_out.transpose(1, 2).reshape(bs, c, h, w), att_map\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        # Apply self-attention mechanism and add to the input\n",
    "        x = self.scale * self.use_attention(x)[0] + x\n",
    "        \n",
    "        # Apply batch normalization and ReLU activation\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Additional convolutional layers\n",
    "        x = F.relu(self.bn1(self.conv2(x)))\n",
    "        x = F.relu(self.bn2(self.conv3(x)))\n",
    "        x = F.relu(self.bn3(self.conv4(x)))\n",
    "        \n",
    "        # Flatten the output and apply dropout\n",
    "        x = self.do(x.reshape(x.shape[0], -1))\n",
    "\n",
    "        # Fully connected layer for final output\n",
    "        return self.fc_out(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create our model and view the ouput! </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader itterable object\n",
    "dataiter = next(iter(test_loader))\n",
    "# Sample from the itterable object\n",
    "test_images, test_labels = dataiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(test_images, 8, normalize=True)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of our network\n",
    "# Set channels_in to the number of channels of the dataset images (1 channel for MNIST)\n",
    "model = CNN(channels_in = test_images.shape[1]).to(device)\n",
    "\n",
    "# View the network\n",
    "# Note that the layer order is simply the order in which we defined them, NOT the order of the forward pass\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in model.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass image through network\n",
    "out = model(test_images.to(device))\n",
    "# Check output\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Set up the optimizer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass our network parameters to the optimiser set our lr as the learning_rate\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Cross Entropy Loss\n",
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should perform a single training epoch using our training data\n",
    "def train(model, optimizer, loader, device, loss_fun, loss_logger):\n",
    "    \n",
    "    # Set Network in train mode\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(tqdm(loader, leave=False, desc=\"Training\")):\n",
    "        # Forward pass of image through network and get output\n",
    "        fx = model(x.to(device))\n",
    "        \n",
    "        # Calculate loss using loss function\n",
    "        loss = loss_fun(fx, y.to(device))\n",
    "\n",
    "        # Zero Gradents\n",
    "        optimizer.zero_grad()\n",
    "        # Backpropagate Gradents\n",
    "        loss.backward()\n",
    "        # Do a single optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the loss for plotting\n",
    "        loss_logger.append(loss.item())\n",
    "        \n",
    "    # Return the avaerage loss and acc from the epoch as well as the logger array       \n",
    "    return model, optimizer, loss_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should perform a single evaluation epoch, it WILL NOT be used to train our model\n",
    "def evaluate(model, device, loader):\n",
    "    \n",
    "    # Initialise counter\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # Set network in evaluation mode\n",
    "    # Layers like Dropout will be disabled\n",
    "    # Layers like Batchnorm will stop calculating running mean and standard deviation\n",
    "    # and use current stored values (More on these layer types soon!)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(tqdm(loader, leave=False, desc=\"Evaluating\")):\n",
    "            # Forward pass of image through network\n",
    "            fx = model(x.to(device))\n",
    "            \n",
    "            # Log the cumulative sum of the acc\n",
    "            epoch_acc += (fx.argmax(1) == y.to(device)).sum().item()\n",
    "            \n",
    "    # Return the accuracy from the epoch     \n",
    "    return epoch_acc / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_logger = []\n",
    "validation_acc_logger = []\n",
    "training_acc_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acc = 0\n",
    "train_acc = 0\n",
    "\n",
    "# This cell implements our training loop\n",
    "pbar = trange(0, num_epochs, leave=False, desc=\"Epoch\")    \n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Accuracy: Train %.2f%%, Val %.2f%%' % (train_acc * 100, valid_acc * 100))\n",
    "    \n",
    "    # Call the training function and pass training dataloader etc\n",
    "    model, optimizer, training_loss_logger = train(model=model, \n",
    "                                                   optimizer=optimizer, \n",
    "                                                   loader=train_loader, \n",
    "                                                   device=device, \n",
    "                                                   loss_fun=loss_fun, \n",
    "                                                   loss_logger=training_loss_logger)\n",
    "    \n",
    "    # Call the evaluate function and pass the dataloader for both validation and training\n",
    "    train_acc = evaluate(model=model, device=device, loader=train_loader)\n",
    "    valid_acc = evaluate(model=model, device=device, loader=valid_loader)\n",
    "    \n",
    "    # Log the train and validation accuracies\n",
    "    validation_acc_logger.append(valid_acc)\n",
    "    training_acc_logger.append(train_acc)\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(training_loss_logger))\n",
    "plt.plot(train_x, training_loss_logger)\n",
    "_ = plt.title(\"LeNet Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(training_acc_logger))\n",
    "plt.plot(train_x, training_acc_logger, c = \"y\")\n",
    "valid_x = np.linspace(0, num_epochs, len(validation_acc_logger))\n",
    "plt.plot(valid_x, validation_acc_logger, c = \"k\")\n",
    "\n",
    "plt.title(\"LeNet\")\n",
    "_ = plt.legend([\"Training accuracy\", \"Validation accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the evaluate function and pass the evaluation/test dataloader etc\n",
    "test_acc = evaluate(model=model, device=device, loader=test_loader)\n",
    "print(\"The total test accuracy is: %.2f%%\" %(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise the prediction for a few test images!\n",
    "\n",
    "with torch.no_grad():\n",
    "    fx = model(test_images[:8].to(device))\n",
    "    pred = fx.argmax(-1)\n",
    "    \n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(test_images[:8], 8, normalize=True)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "print(\"Predicted Values\\n\", list(pred.cpu().numpy()))\n",
    "print(\"True Values\\n\", list(test_labels[:8].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model and test_images are already defined and loaded\n",
    "with torch.no_grad():\n",
    "    x = model.conv1(test_images[:8].to(device))\n",
    "    _, att_map = model.use_attention(x)\n",
    "    \n",
    "# Index of the image you want to visualize\n",
    "img_idx = 6\n",
    "\n",
    "# Specify the dimensions for the attention map visualization\n",
    "x_dim = 5\n",
    "y_dim = 25\n",
    "\n",
    "assert x_dim < test_images.shape[3], \"x_dim must be less than \" + str(test_images.shape[3] - 1)\n",
    "assert y_dim < test_images.shape[2], \"y_dim must be less than \" + str(test_images.shape[2] - 1)\n",
    "\n",
    "# Plot the image and its corresponding attention map\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "# Plot the original image\n",
    "img_out = test_images[img_idx]\n",
    "img_out = (img_out - img_out.min())/(img_out.max() - img_out.min())\n",
    "axes[0].imshow(img_out.permute(1, 2, 0).cpu().numpy())\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "axes[0].scatter(x_dim, y_dim, color='red', marker='x')\n",
    "\n",
    "# Plot the attention map\n",
    "axes[1].imshow(att_map[img_idx, x_dim * y_dim].reshape(32, 32).cpu().numpy(), cmap='viridis')\n",
    "axes[1].set_title(\"Attention Map\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
