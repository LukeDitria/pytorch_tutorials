{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c131e741",
   "metadata": {},
   "source": [
    "# Image Captioning with Transformers\n",
    "### Encoder-Decoder Architecture\n",
    "\n",
    "<img src=\"../data/llm_architecture_comparison.png\" width=\"600\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "# Make sure you are using the lastest version!\n",
    "from transformers import AutoTokenizer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d70b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate for the optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Image size\n",
    "image_size = 128\n",
    "\n",
    "# Define the number of epochs for training\n",
    "nepochs = 1000\n",
    "\n",
    "# Define the batch size for mini-batch gradient descent\n",
    "batch_size = 64\n",
    "\n",
    "# Define the root directory of the dataset\n",
    "data_set_root='/media/luke/Quick_Storage/Data/coco_captions'\n",
    "train_set ='train2014'\n",
    "validation_set ='val2014'\n",
    "\n",
    "train_image_path = os.path.join(data_set_root, train_set)\n",
    "train_ann_file = '{}/annotations/captions_{}.json'.format(data_set_root, train_set)\n",
    "\n",
    "val_image_path = os.path.join(data_set_root, validation_set)\n",
    "val_ann_file = '{}/annotations/captions_{}.json'.format(data_set_root, validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383fcb4",
   "metadata": {},
   "source": [
    "## Data processing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabacd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleCaption(nn.Module):\n",
    "    def __call__(self, sample):\n",
    "        rand_index = random.randint(0, len(sample) - 1)\n",
    "        return sample[rand_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee64e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize(image_size),\n",
    "                                      transforms.RandomCrop(image_size),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                           std=[0.229, 0.224, 0.225]),\n",
    "                                      transforms.RandomErasing(p=0.5)]) \n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(image_size),\n",
    "                                transforms.CenterCrop(image_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])]) \n",
    "\n",
    "train_dataset = datasets.CocoCaptions(root=train_image_path,\n",
    "                                      annFile=train_ann_file,\n",
    "                                      transform=train_transform,\n",
    "                                      target_transform=SampleCaption())\n",
    "\n",
    "val_dataset = datasets.CocoCaptions(root=val_image_path,\n",
    "                                    annFile=val_ann_file,\n",
    "                                    transform=transform,\n",
    "                                    target_transform=SampleCaption())\n",
    "\n",
    "data_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "data_loader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e27751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader itterable object\n",
    "dataiter = next(iter(data_loader_val))\n",
    "# Sample from the itterable object\n",
    "test_images, test_captions = dataiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (3,3))\n",
    "out = torchvision.utils.make_grid(test_images[0:1], 1, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "caption = test_captions[0]\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ff20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3213ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(test_captions, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9fbf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokens['input_ids'][0]\n",
    "tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2662720",
   "metadata": {},
   "source": [
    "## Create Encoder-Decoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ac2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image_tensor, patch_size=16):\n",
    "    # Get the dimensions of the image tensor\n",
    "    bs, c, h, w = image_tensor.size()\n",
    "    \n",
    "    # Define the Unfold layer with appropriate parameters\n",
    "    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    # Apply Unfold to the image tensor\n",
    "    unfolded = unfold(image_tensor)\n",
    "    \n",
    "    # Reshape the unfolded tensor to match the desired output shape\n",
    "    # Output shape: BSxLxH, where L is the number of patches in each dimension\n",
    "    unfolded = unfolded.transpose(1, 2).reshape(bs, -1, c * patch_size * patch_size)\n",
    "    \n",
    "    return unfolded\n",
    "\n",
    "# sinusoidal positional embeds\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "    \n",
    "\n",
    "# Define a module for attention blocks\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, masking=True):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.masking = masking\n",
    "\n",
    "        # Multi-head attention mechanism\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    batch_first=True,\n",
    "                                                    dropout=0.25)\n",
    "\n",
    "    def forward(self, x_in, kv_in, key_mask=None):\n",
    "        # Apply causal masking if enabled\n",
    "        if self.masking:\n",
    "            bs, l, h = x_in.shape\n",
    "            mask = torch.triu(torch.ones(l, l, device=x_in.device), 1).bool()\n",
    "        else:\n",
    "            mask = None\n",
    "            \n",
    "        # Perform multi-head attention operation\n",
    "        return self.multihead_attn(x_in, kv_in, kv_in, attn_mask=mask, key_padding_mask=key_mask)[0]\n",
    "\n",
    "\n",
    "# Define a module for a transformer block with self-attention and optional causal masking\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, decoder=False, masking=True):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "        # Layer normalization for the input\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        # Self-attention mechanism\n",
    "        self.attn1 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=masking)\n",
    "        \n",
    "        # Layer normalization for the output of the first attention layer\n",
    "        if self.decoder:\n",
    "            self.norm2 = nn.LayerNorm(hidden_size)\n",
    "            # Self-attention mechanism for the decoder with no masking\n",
    "            self.attn2 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=False)\n",
    "        \n",
    "        # Layer normalization for the output before the MLP\n",
    "        self.norm_mlp = nn.LayerNorm(hidden_size)\n",
    "        # Multi-layer perceptron (MLP)\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size * 2),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(hidden_size * 2, hidden_size))\n",
    "                \n",
    "    def forward(self, x, input_key_mask=None, cross_key_mask=None, kv_cross=None):\n",
    "        # Perform self-attention operation\n",
    "        x = self.attn1(x, x, key_mask=input_key_mask) + x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # If decoder, perform additional cross-attention layer\n",
    "        if self.decoder:\n",
    "            x = self.attn2(x, kv_cross, key_mask=cross_key_mask) + x\n",
    "            x = self.norm2(x)\n",
    "\n",
    "        # Apply MLP and layer normalization\n",
    "        x = self.mlp(x) + x\n",
    "        return self.norm_mlp(x)\n",
    "\n",
    "    \n",
    "# Define a decoder module for the Transformer architecture\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Create an embedding layer for tokens\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        # Initialize the embedding weights\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "\n",
    "        # Initialize sinusoidal positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        \n",
    "        # Create multiple transformer blocks as layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads, decoder=True) for _ in range(num_layers)\n",
    "        ])\n",
    "                \n",
    "        # Define a linear layer for output prediction\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "        \n",
    "    def forward(self, input_seq, encoder_output, input_padding_mask=None, encoder_padding_mask=None):        \n",
    "        # Embed the input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        \n",
    "        # Pass the embeddings through each transformer block\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs, \n",
    "                           input_key_mask=input_padding_mask, \n",
    "                           cross_key_mask=encoder_padding_mask, \n",
    "                           kv_cross=encoder_output)\n",
    "        \n",
    "        return self.fc_out(embs)\n",
    "\n",
    "    \n",
    "# Define an Vision Encoder module for the Transformer architecture\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, patch_size=16, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(VisionEncoder, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size)\n",
    "        \n",
    "        seq_length = (image_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_size).normal_(std=0.02))\n",
    "        \n",
    "        # Create multiple transformer blocks as layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads, decoder=False, masking=False) for _ in range(num_layers)\n",
    "        ])\n",
    "                \n",
    "    def forward(self, image):  \n",
    "        bs = image.shape[0]\n",
    "\n",
    "        patch_seq = extract_patches(image, patch_size=self.patch_size)\n",
    "        patch_emb = self.fc_in(patch_seq)\n",
    "\n",
    "        # Add a unique embedding to each token embedding\n",
    "        embs = patch_emb + self.pos_embedding\n",
    "        \n",
    "        # Pass the embeddings through each transformer block\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs)\n",
    "        \n",
    "        return embs\n",
    "    \n",
    "    \n",
    "# Define an Vision Encoder-Decoder module for the Transformer architecture\n",
    "class VisionEncoderDecoder(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, num_emb, patch_size=16, \n",
    "                 hidden_size=128, num_layers=(3, 3), num_heads=4):\n",
    "        super(VisionEncoderDecoder, self).__init__()\n",
    "        \n",
    "        # Create an encoder and decoder with specified parameters\n",
    "        self.encoder = VisionEncoder(image_size=image_size, channels_in=channels_in, patch_size=patch_size,\n",
    "                               hidden_size=hidden_size, num_layers=num_layers[0], num_heads=num_heads)\n",
    "        \n",
    "        self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size, \n",
    "                               num_layers=num_layers[1], num_heads=num_heads)\n",
    "\n",
    "    def forward(self, input_image, target_seq, padding_mask):\n",
    "        # Generate padding masks for the target sequence\n",
    "        bool_padding_mask = padding_mask == 0\n",
    "\n",
    "        # Encode the input sequence\n",
    "        encoded_seq = self.encoder(image=input_image)\n",
    "        \n",
    "        # Decode the target sequence using the encoded sequence\n",
    "        decoded_seq = self.decoder(input_seq=target_seq, \n",
    "                                   encoder_output=encoded_seq, \n",
    "                                   input_padding_mask=bool_padding_mask)\n",
    "        return decoded_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a21795",
   "metadata": {},
   "source": [
    "## Initialise Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(1 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Embedding Size\n",
    "hidden_size = 768\n",
    "\n",
    "# Number of Transformer blocks for the (Encoder, Decoder)\n",
    "num_layers = (3, 6)\n",
    "\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 16\n",
    "\n",
    "# Size of the patches\n",
    "patch_size = 16\n",
    "\n",
    "# Create model\n",
    "caption_model = VisionEncoderDecoder(image_size=image_size, channels_in=test_images.shape[1], \n",
    "                                     num_emb=tokenizer.vocab_size, patch_size=patch_size, \n",
    "                                     num_layers=num_layers,hidden_size=hidden_size, \n",
    "                                     num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(caption_model.parameters(), lr=learning_rate)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "# Initialize the training loss logger\n",
    "training_loss_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in caption_model.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8eea79",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd56a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over epochs\n",
    "for epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):\n",
    "    # Set the model in training mode\n",
    "    caption_model.train()\n",
    "    steps = 0\n",
    "    # Iterate over the training data loader\n",
    "    for images, captions in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Tokenize and pre-process the captions\n",
    "        tokens = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        token_ids = tokens['input_ids'].to(device)\n",
    "        padding_mask = tokens['attention_mask'].to(device)\n",
    "        bs = token_ids.shape[0]\n",
    "        \n",
    "        # Shift the input sequence to create the target sequence\n",
    "        target_ids = torch.cat((token_ids[:, 1:], \n",
    "                                torch.zeros(bs, 1, device=device).long()), 1)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass\n",
    "            pred = caption_model(images, token_ids, padding_mask=padding_mask)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = (loss_fn(pred.transpose(1, 2), target_ids) * padding_mask).mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Log the training loss\n",
    "        training_loss_logger.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf7354",
   "metadata": {},
   "source": [
    "## Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger[100:])\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd144a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 512\n",
    "data = np.convolve(np.array(training_loss_logger), np.ones(window_size)/window_size, mode=\"valid\")\n",
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(data[10000:])\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f7b97",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564fde22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader itterable object\n",
    "dataiter = next(iter(data_loader_val))\n",
    "# Sample from the itterable object\n",
    "test_images, test_captions = dataiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an index within the batch\n",
    "index = 0\n",
    "test_image = test_images[index].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabbdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (3,3))\n",
    "out = torchvision.utils.make_grid(test_image, 1, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
    "print(test_captions[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Start-Of-Sentence token to the prompt to signal the network to start generating the caption\n",
    "sos_token = 101 * torch.ones(1, 1).long()\n",
    "\n",
    "# Set the temperature for sampling during generation\n",
    "temp = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_tokens = [sos_token]\n",
    "caption_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode the input image\n",
    "    with torch.cuda.amp.autocast():\n",
    "        # Forward pass\n",
    "        image_embedding = caption_model.encoder(test_image.to(device))\n",
    "\n",
    "    # Generate the answer tokens\n",
    "    for i in range(50):\n",
    "        input_tokens = torch.cat(log_tokens, 1)\n",
    "        \n",
    "        # Decode the input tokens into the next predicted tokens\n",
    "        data_pred = caption_model.decoder(input_tokens.to(device), image_embedding)\n",
    "        \n",
    "        # Sample from the distribution of predicted probabilities\n",
    "        dist = Categorical(logits=data_pred[:, -1] / temp)\n",
    "        next_tokens = dist.sample().reshape(1, 1)\n",
    "        \n",
    "        # Append the next predicted token to the sequence\n",
    "        log_tokens.append(next_tokens.cpu())\n",
    "        \n",
    "        # Break the loop if the End-Of-Caption token is predicted\n",
    "        if next_tokens.item() == 102:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ddad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of token indices to a tensor\n",
    "pred_text = torch.cat(log_tokens, 1)\n",
    "\n",
    "# Convert the token indices to their corresponding strings using the vocabulary\n",
    "pred_text_strings = tokenizer.decode(pred_text[0], skip_special_tokens=True)\n",
    "\n",
    "# Join the token strings to form the predicted text\n",
    "pred_text = \"\".join(pred_text_strings)\n",
    "\n",
    "# Print the predicted text\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e081bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (3,3))\n",
    "out = torchvision.utils.make_grid(test_image, 1, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc10290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
