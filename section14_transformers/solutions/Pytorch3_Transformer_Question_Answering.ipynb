{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c131e741",
   "metadata": {},
   "source": [
    "# Transformers QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import YahooAnswers\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchtext.data.functional import sentencepiece_tokenizer, load_sp_model\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d70b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "learning_rate = 1e-4\n",
    "\n",
    "nepochs = 100\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "max_len_q = 32\n",
    "max_len_a = 64\n",
    "\n",
    "data_set_root = \"../../datasets\"\n",
    "\n",
    "# We'll be using the YahooAnswers Dataset\n",
    "# Note that for torchtext these datasets are NOT Pytorch dataset classes \"YahooAnswers\" is a function that\n",
    "# returns a Pytorch DataPipe!\n",
    "\n",
    "# Pytorch DataPipes vvv\n",
    "# https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "\n",
    "# vvv Good Blog on the difference between DataSet and DataPipe\n",
    "# https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58\n",
    "\n",
    "# Depending on the dataset sometimes the dataset doesn't download and gives an error\n",
    "# and you'll have to download and extract manually \n",
    "# \"The datasets supported by torchtext are datapipes from the torchdata project, which is still in Beta status\"\n",
    "\n",
    "# Un-comment to triger the DataPipe to download the data vvv\n",
    "# dataset_train = YahooAnswers(root=data_set_root, split=\"train\")\n",
    "# data = next(iter(dataset_train))\n",
    "\n",
    "# Side-Note I've noticed that the WikiText dataset is no longer able to be downloaded :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## \"Train\" a Sentence Piece Tokenizer with the train data capping the vocab size to 20000 tokens\n",
    "# from torchtext.data.functional import generate_sp_model\n",
    "\n",
    "# with open(os.path.join(data_set_root, \"datasets/YahooAnswers/train.csv\")) as f:\n",
    "#     with open(os.path.join(data_set_root, \"datasets/YahooAnswers/data.txt\"), \"w\") as f2:\n",
    "#         for i, line in enumerate(f):\n",
    "#             text_only = \"\".join(line.split(\",\")[1:])\n",
    "#             filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # remove newline characters\n",
    "#             f2.write(filtered.lower() + \"\\n\")\n",
    "\n",
    "\n",
    "# generate_sp_model(os.path.join(data_set_root, \"datasets/YahooAnswers/data.txt\"), \n",
    "#                   vocab_size=20000, model_prefix='spm_user_ya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YahooQA(Dataset):\n",
    "    def __init__(self, num_datapoints, test_train=\"train\"):\n",
    "        self.df = pd.read_csv(os.path.join(data_set_root, \"datasets/YahooAnswers/\" + test_train + \".csv\"),\n",
    "                              names=[\"Class\", \"Q_Title\", \"Q_Content\", \"A\"])\n",
    "        \n",
    "        self.df.fillna('', inplace=True)\n",
    "        self.df['Q'] = self.df['Q_Title'] + ': ' + self.df['Q_Content']\n",
    "        self.df.drop(['Q_Title', 'Q_Content'], axis=1, inplace=True)\n",
    "        self.df['Q'] = self.df['Q'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "        self.df['A'] = self.df['A'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        question_text = self.df.loc[index][\"Q\"].lower()\n",
    "        answer_text = self.df.loc[index][\"A\"].lower()\n",
    "\n",
    "        return question_text, answer_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = YahooQA(num_datapoints=data_set_root, test_train=\"train\")\n",
    "dataset_test = YahooQA(num_datapoints=data_set_root, test_train=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model = load_sp_model(\"spm_user_ya.model\")\n",
    "tokenizer = sentencepiece_tokenizer(sp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "            \n",
    "vocab = build_vocab_from_iterator(yield_tokens(\"spm_user_ya.vocab\"), \n",
    "                                  specials= ['<pad>', '<soq>', '<eoq>', '<soa>', '<eoa>', '<unk>'], # special case tokens\n",
    "                                  special_first=True)\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_transform = T.SentencePieceTokenizer(\"spm_user_ya.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72346ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tranform = T.Sequential(\n",
    "    # Tokeniz with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"spm_user_ya.model\"),\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentance if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len_q),\n",
    "    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "    # 2 as seen in previous section\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0)\n",
    ")\n",
    "\n",
    "a_tranform = T.Sequential(\n",
    "    # Tokeniz with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"spm_user_ya.model\"),\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(3, begin=True),\n",
    "    # Crop the sentance if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len_a),\n",
    "    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "    # 2 as seen in previous section\n",
    "    T.AddToken(4, begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ac2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinusoidal positional embeds\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "    \n",
    "# Attention block with self-attention with/without causal masking\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, masking=True):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.masking = masking\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True)\n",
    "                \n",
    "    def forward(self, x_in, kv_in):\n",
    "        if self.masking:\n",
    "            bs, l, h = x_in.shape\n",
    "            mask = torch.triu(torch.ones(l, l, device=x_in.device), 1).bool()\n",
    "        else:\n",
    "            mask = None\n",
    "            \n",
    "        return self.multihead_attn(x_in, kv_in, kv_in, attn_mask=mask)[0]\n",
    "\n",
    "    \n",
    "# Transformer block with self-attention with/without causal masking\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, decoder=False, masking=True):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn1 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=masking)\n",
    "        \n",
    "        if self.decoder:\n",
    "            self.norm2 = nn.LayerNorm(hidden_size)\n",
    "            self.attn2 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=False)\n",
    "        \n",
    "        self.norm_mlp = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size * 4),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(hidden_size * 4, hidden_size))\n",
    "                \n",
    "    def forward(self, x, kv_cross=None):\n",
    "        x = self.attn1(x, x) + x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        if self.decoder:\n",
    "            x = self.attn2(x, kv_cross) + x\n",
    "            x = self.norm2(x)\n",
    "\n",
    "        x = self.mlp(x) + x\n",
    "        return self.norm_mlp(x)\n",
    "    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Create an embedding for each token\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads, decoder=False, masking=False) for _ in range(num_layers)\n",
    "        ])\n",
    "                \n",
    "    def forward(self, input_seq):        \n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add a unique embedding to each token embedding depending on it's position in the sequence\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            output = block(embs)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Create an embedding for each token\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads, decoder=True) for _ in range(num_layers)\n",
    "        ])\n",
    "                \n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "        \n",
    "    def forward(self, input_seq, encoder_output):        \n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add a unique embedding to each token embedding depending on it's position in the sequence\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            output = block(embs, kv_cross=encoder_output)\n",
    "        \n",
    "        return self.fc_out(output)\n",
    "\n",
    "    \n",
    "# \"Encoder-Decoder\" Style Transformer with self-attention\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=(3, 3), num_heads=4):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        \n",
    "        # Create an embedding for each token\n",
    "        self.encoder = Encoder(num_emb=num_emb, hidden_size=hidden_size, \n",
    "                               num_layers=num_layers[0], num_heads=num_heads)\n",
    "        \n",
    "        self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size, \n",
    "                               num_layers=num_layers[1], num_heads=num_heads)\n",
    "\n",
    "        \n",
    "    def forward(self, input_seq, target_seq):        \n",
    "        encoded_seq = self.encoder(input_seq)\n",
    "        decoded_seq = self.decoder(target_seq, encoded_seq)\n",
    "\n",
    "        return decoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "\n",
    "num_layers = (3, 6)\n",
    "num_heads = 16\n",
    "\n",
    "# Create model\n",
    "tf_generator = EncoderDecoder(num_emb=len(vocab), num_layers=num_layers, \n",
    "                              hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(tf_generator.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Custom transform that will randomly replace a token with <pad>\n",
    "# td = TokenDrop(prob=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in tf_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0443831",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd56a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):\n",
    "    tf_generator.train()\n",
    "    steps = 0\n",
    "    for q_text, a_text in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        q_text_tokens = q_tranform(list(q_text)).to(device)\n",
    "        a_text_tokens = a_tranform(list(a_text)).to(device)\n",
    "        a_input_text = a_text_tokens[:, 0:-1]\n",
    "        a_output_text = a_text_tokens[:, 1:]\n",
    "        \n",
    "        bs = q_text_tokens.shape[0]\n",
    "\n",
    "        pred = tf_generator(q_text_tokens, a_input_text)\n",
    "\n",
    "        loss = loss_fn(pred.transpose(1, 2), a_output_text)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss_logger.append(loss.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger[1000:])\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd144a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 512\n",
    "data = np.convolve(np.array(training_loss_logger), np.ones(window_size)/window_size, mode=\"valid\")\n",
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(data[10000:])\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_text, a_text = next(iter(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeeb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_prompt = [\"what is that largest ocean in the world?: \"]\n",
    "init_prompt = [q_text[0]]\n",
    "\n",
    "input_tokens = q_tranform(init_prompt).to(device)\n",
    "\n",
    "# Add Start-Of-Answer token to prompt the network to start generating the answer!\n",
    "# input_tokens = torch.cat((input_tokens, 3 * torch.ones(1, 1, device=device).long()), 1)\n",
    "soa_token = 3 * torch.ones(1, 1).long()\n",
    "print(input_tokens)\n",
    "print(vocab.lookup_tokens(input_tokens[0].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_tokens = [soa_token]\n",
    "tf_generator.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded_seq = tf_generator.encoder(input_tokens.to(device))\n",
    "\n",
    "    for i in range(100):\n",
    "        input_tokens = torch.cat(log_tokens, 1)\n",
    "        data_pred = tf_generator.decoder(input_tokens.to(device), encoded_seq)\n",
    "#         We can take the token with the highest prob\n",
    "#         input_tokens = data_pred[:, -1].argmax().reshape(1, 1)\n",
    "        \n",
    "        # Or sample from the distribution of probs!\n",
    "        dist = Categorical(logits=data_pred[:, -1]/temp)\n",
    "        next_tokens = dist.sample().reshape(1, 1)\n",
    "        \n",
    "        log_tokens.append(next_tokens.cpu())\n",
    "        \n",
    "        if next_tokens.item() == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ddad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_text = \"\".join(vocab.lookup_tokens(torch.cat(log_tokens, 1)[0].numpy()))\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765963d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_text.replace(\"‚ñÅ\", \" \").replace(\"<unk>\", \"\").replace(\"<eoa>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(F.softmax(data_pred[:, -1]/temp, -1).cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc3b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0dea41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
