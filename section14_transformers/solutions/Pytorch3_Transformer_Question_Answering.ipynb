{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c131e741",
   "metadata": {},
   "source": [
    "# Question Answering with Transformers\n",
    "### Encoder-Decoder Architecture\n",
    "\n",
    "Let's revisit Question Answering and see how we can use Attention to create a Transformer instead of an LSTM. The type of Transformer network we will be using is called a Encoder-Decoder Transformer and is the first type of Transformer Network that was first detailed in the paper \"Attention is all you need!\". In fact the Encoder only and Decoder only networks we've already seen came later! We looked at them first as they are simpler to understand.\n",
    "<br>\n",
    "Encoder-Decoder Transformers Combine an Encoder Network with Self-Attention and a Decoder Network with Masked Self-Attention. The Encoder will Encode the input sequence (in our example this is the question!) and the Decoder will perform next-token prediction (in our example this is the answer!). The two are combined with a Cross-Attention Layer. This one-way attention allows the Decoder embeddings to query the Encoder embeddings, giving the Decoder information about what the output sequence (answer) should look like! Cross-Attention is a very common way to \"condition\" a text-generating Decoder network! <br>\n",
    "What else could we condition the decoder with...\n",
    "\n",
    "<img src=\"../data/llm_architecture_comparison.png\" width=\"600\" align=\"center\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from torchtext.datasets import YahooAnswers\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchtext.data.functional import sentencepiece_tokenizer, load_sp_model\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d70b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate for the optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Define the number of epochs for training\n",
    "nepochs = 20\n",
    "\n",
    "# Define the batch size for mini-batch gradient descent\n",
    "batch_size = 128\n",
    "\n",
    "# Define the maximum length of input questions\n",
    "max_len_q = 32\n",
    "\n",
    "# Define the maximum length of output answers\n",
    "max_len_a = 64\n",
    "\n",
    "# Define the root directory of the dataset\n",
    "data_set_root = \"../../datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383fcb4",
   "metadata": {},
   "source": [
    "## Data processing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using the YahooAnswers Dataset\n",
    "# Note that for torchtext these datasets are NOT Pytorch dataset classes \"YahooAnswers\" is a function that\n",
    "# returns a Pytorch DataPipe!\n",
    "\n",
    "# Pytorch DataPipes vvv\n",
    "# https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "\n",
    "# vvv Good Blog on the difference between DataSet and DataPipe\n",
    "# https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58\n",
    "\n",
    "# Depending on the dataset sometimes the dataset doesn't download and gives an error\n",
    "# and you'll have to download and extract manually \n",
    "# \"The datasets supported by torchtext are datapipes from the torchdata project, which is still in Beta status\"\n",
    "\n",
    "# Un-comment to triger the DataPipe to download the data vvv\n",
    "# dataset_train = YahooAnswers(root=data_set_root, split=\"train\")\n",
    "# data = next(iter(dataset_train))\n",
    "\n",
    "# Side-Note I've noticed that the WikiText dataset is no longer able to be downloaded :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## \"Train\" a Sentence Piece Tokenizer with the train data capping the vocab size to 20000 tokens\n",
    "# from torchtext.data.functional import generate_sp_model\n",
    "\n",
    "# with open(os.path.join(data_set_root, \"datasets/YahooAnswers/train.csv\")) as f:\n",
    "#     with open(os.path.join(data_set_root, \"datasets/YahooAnswers/data.txt\"), \"w\") as f2:\n",
    "#         for i, line in enumerate(f):\n",
    "#             text_only = \"\".join(line.split(\",\")[1:])\n",
    "#             filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # remove newline characters\n",
    "#             f2.write(filtered.lower() + \"\\n\")\n",
    "\n",
    "\n",
    "# generate_sp_model(os.path.join(data_set_root, \"datasets/YahooAnswers/data.txt\"), \n",
    "#                   vocab_size=20000, model_prefix='spm_user_ya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YahooQA(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for handling the Yahoo Answers dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_datapoints, test_train=\"train\"):\n",
    "        # Read the dataset from the CSV file\n",
    "        self.df = pd.read_csv(os.path.join(data_set_root, \"datasets/YahooAnswers/\" + test_train + \".csv\"),\n",
    "                              names=[\"Class\", \"Q_Title\", \"Q_Content\", \"A\"])\n",
    "        \n",
    "        # Fill any missing values with empty strings\n",
    "        self.df.fillna('', inplace=True)\n",
    "        \n",
    "        # Combine question title and content into a single question\n",
    "        self.df['Q'] = self.df['Q_Title'] + ': ' + self.df['Q_Content']\n",
    "        \n",
    "        # Drop the now redundant question title and content columns\n",
    "        self.df.drop(['Q_Title', 'Q_Content'], axis=1, inplace=True)\n",
    "        \n",
    "        # Clean the question and answer text by removing unwanted characters\n",
    "        self.df['Q'] = self.df['Q'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "        self.df['A'] = self.df['A'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve the question and answer text and convert them to lowercase\n",
    "        question_text = self.df.loc[index][\"Q\"].lower()\n",
    "        answer_text = self.df.loc[index][\"A\"].lower()\n",
    "\n",
    "        # Return a tuple of the question and answer text\n",
    "        return question_text, answer_text\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of data points in the dataset\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing datasets using the YahooQA class\n",
    "dataset_train = YahooQA(num_datapoints=data_set_root, test_train=\"train\")\n",
    "dataset_test = YahooQA(num_datapoints=data_set_root, test_train=\"test\")\n",
    "\n",
    "# Create data loaders for training and testing datasets\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the tokenizer\n",
    "# Load the SentencePiece model\n",
    "sp_model = load_sp_model(\"spm_user_ya.model\")\n",
    "\n",
    "# Create a tokenizer using the loaded model\n",
    "tokenizer = sentencepiece_tokenizer(sp_model)\n",
    "\n",
    "# Iterate over tokens generated by the tokenizer\n",
    "for token in tokenizer([\"welcome to this tutorial!\"]):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to yield tokens from a file\n",
    "def yield_tokens(file_path):\n",
    "    # Open the file in UTF-8 encoding\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        # Iterate over each line in the file\n",
    "        for line in f:\n",
    "            # Yield the token split by tab character\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "            \n",
    "# Build vocabulary from the iterator of tokens\n",
    "# We will also add \"special\" tokens that we'll use to signal something to our model\n",
    "# <pad> is a padding token that is added to the end of a sentence to ensure \n",
    "# the length of all sequences in a batch is the same\n",
    "# <soq> signals the \"Start-Of-Question\" aka the start of the Question sequence\n",
    "# <eoq> signals the \"End-Of-Question\" aka the end of the Question sequence\n",
    "# <soa> signals the \"Start-Of-Answer\" aka the start of the Answer sequence\n",
    "# <eoa> signals the \"End-Of-Answer\" aka the end of the Answer sequence\n",
    "# <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(\"spm_user_ya.vocab\"),\n",
    "    # Define special tokens with special_first=True to place them at the beginning of the vocabulary\n",
    "    specials=['<pad>', '<soq>', '<eoq>', '<soa>', '<eoa>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "# Set default index for out-of-vocabulary tokens\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72346ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tranform = T.Sequential(\n",
    "    # Tokeniz with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"spm_user_ya.model\"),\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentance if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len_q),\n",
    "    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "    # 2 as seen in previous section\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0)\n",
    ")\n",
    "\n",
    "a_tranform = T.Sequential(\n",
    "    # Tokeniz with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"spm_user_ya.model\"),\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(3, begin=True),\n",
    "    # Crop the sentance if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len_a),\n",
    "    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "    # 2 as seen in previous section\n",
    "    T.AddToken(4, begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2662720",
   "metadata": {},
   "source": [
    "## Create Encoder-Decoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ac2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinusoidal positional embeds\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "# Define a module for attention blocks\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, masking=True):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.masking = masking\n",
    "\n",
    "        # Multi-head attention mechanism\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    batch_first=True,\n",
    "                                                    dropout=0.25)\n",
    "\n",
    "    def forward(self, x_in, kv_in, key_mask=None):\n",
    "        # Apply causal masking if enabled\n",
    "        if self.masking:\n",
    "            bs, l, h = x_in.shape\n",
    "            mask = torch.triu(torch.ones(l, l, device=x_in.device), 1).bool()\n",
    "        else:\n",
    "            mask = None\n",
    "            \n",
    "        # Perform multi-head attention operation\n",
    "        return self.multihead_attn(x_in, kv_in, kv_in, attn_mask=mask, key_padding_mask=key_mask)[0]\n",
    "\n",
    "\n",
    "# Define a module for a transformer block with self-attention and optional causal masking\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, is_decoder=False, masking=True):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        # Layer normalization for the input\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        # Self-attention mechanism\n",
    "        self.attn1 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=masking)\n",
    "        \n",
    "        # Layer normalization for the output of the first attention layer\n",
    "        if self.is_decoder:\n",
    "            self.norm2 = nn.LayerNorm(hidden_size)\n",
    "            # Self-attention mechanism for the decoder with no masking\n",
    "            self.attn2 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=False)\n",
    "        \n",
    "        # Layer normalization for the output before the MLP\n",
    "        self.norm_mlp = nn.LayerNorm(hidden_size)\n",
    "        # Multi-layer perceptron (MLP)\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size * 4),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(hidden_size * 4, hidden_size))\n",
    "                \n",
    "    def forward(self, x, input_key_mask=None, cross_key_mask=None, kv_cross=None):\n",
    "        # Perform self-attention operation\n",
    "        x = self.attn1(x, x, key_mask=input_key_mask) + x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # If decoder, perform additional cross-attention layer\n",
    "        if self.is_decoder:\n",
    "            x = self.attn2(x, kv_cross, key_mask=cross_key_mask) + x\n",
    "            x = self.norm2(x)\n",
    "\n",
    "        # Apply MLP and layer normalization\n",
    "        x = self.mlp(x) + x\n",
    "        return self.norm_mlp(x)\n",
    "    \n",
    "    \n",
    "# Define an encoder module for the Transformer architecture\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Create an embedding layer for tokens\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        # Initialize the embedding weights\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "\n",
    "        # Initialize sinusoidal positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        \n",
    "        # Create multiple transformer blocks as layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads, is_decoder=False, masking=False) for _ in range(num_layers)\n",
    "        ])\n",
    "                \n",
    "    def forward(self, input_seq, padding_mask=None):        \n",
    "        # Embed the input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        \n",
    "        # Pass the embeddings through each transformer block\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs, input_key_mask=padding_mask)\n",
    "        \n",
    "        return embs\n",
    "\n",
    "    \n",
    "# Define a decoder module for the Transformer architecture\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Create an embedding layer for tokens\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        # Initialize the embedding weights\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "\n",
    "        # Initialize sinusoidal positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        \n",
    "        # Create multiple transformer blocks as layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads, is_decoder=True, masking=True) for _ in range(num_layers)\n",
    "        ])\n",
    "                \n",
    "        # Define a linear layer for output prediction\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "        \n",
    "    def forward(self, input_seq, encoder_output, input_padding_mask=None, encoder_padding_mask=None):        \n",
    "        # Embed the input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        \n",
    "        # Pass the embeddings through each transformer block\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs,\n",
    "                         input_key_mask=input_padding_mask,\n",
    "                         cross_key_mask=encoder_padding_mask, \n",
    "                         kv_cross=encoder_output)\n",
    "        \n",
    "        return self.fc_out(embs)\n",
    "\n",
    "    \n",
    "# Define an Encoder-Decoder module for the Transformer architecture\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=(3, 3), num_heads=4):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        \n",
    "        # Create an encoder and decoder with specified parameters\n",
    "        self.encoder = Encoder(num_emb=num_emb, hidden_size=hidden_size, \n",
    "                               num_layers=num_layers[0], num_heads=num_heads)\n",
    "        \n",
    "        self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size, \n",
    "                               num_layers=num_layers[1], num_heads=num_heads)\n",
    "\n",
    "    def forward(self, input_seq, target_seq):\n",
    "        # Generate padding masks for input and target sequences\n",
    "        input_key_mask = input_seq == 0\n",
    "        output_key_mask = target_seq == 0\n",
    "\n",
    "        # Encode the input sequence\n",
    "        encoded_seq = self.encoder(input_seq=input_seq, \n",
    "                                   padding_mask=input_key_mask)\n",
    "        \n",
    "        # Decode the target sequence using the encoded sequence\n",
    "        decoded_seq = self.decoder(input_seq=target_seq, \n",
    "                                   encoder_output=encoded_seq, \n",
    "                                   input_padding_mask=output_key_mask, \n",
    "                                   encoder_padding_mask=input_key_mask)\n",
    "\n",
    "        return decoded_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a21795",
   "metadata": {},
   "source": [
    "## Initialise Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Embedding Size\n",
    "hidden_size = 512\n",
    "\n",
    "# Number of Transformer blocks for the (Encoder, Decoder)\n",
    "num_layers = (4, 4)\n",
    "\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8\n",
    "\n",
    "# Create model\n",
    "tf_generator = EncoderDecoder(num_emb=len(vocab), num_layers=num_layers, \n",
    "                              hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(tf_generator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Scaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "# Initialize the training loss logger\n",
    "training_loss_logger = []\n",
    "\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Checkpoint\n",
    "# cp = torch.load(\"qa_model.pt\")\n",
    "# tf_generator.load_state_dict(cp[\"model_state_dict\"])\n",
    "# optimizer.load_state_dict(cp[\"optimizer_state_dict\"])\n",
    "# training_loss_logger = cp[\"data_logger\"]\n",
    "# start_epoch = cp[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in tf_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8eea79",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd56a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over epochs\n",
    "for epoch in trange(start_epoch, nepochs, leave=False, desc=\"Epoch\"):\n",
    "    # Set the model in training mode\n",
    "    tf_generator.train()\n",
    "    \n",
    "    # Iterate over the training data loader\n",
    "    for q_text, a_text in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        # Convert question and answer text to tokens and move to device\n",
    "        q_text_tokens = q_tranform(list(q_text)).to(device)\n",
    "        a_text_tokens = a_tranform(list(a_text)).to(device)\n",
    "        a_input_text = a_text_tokens[:, 0:-1]\n",
    "        a_output_text = a_text_tokens[:, 1:]\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = tf_generator(q_text_tokens, a_input_text)\n",
    "\n",
    "            # Generate mask for output text\n",
    "            output_mask = (a_output_text != 0).float()\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = (loss_fn(pred.transpose(1, 2), a_output_text) * output_mask).sum()/output_mask.sum()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Log the training loss\n",
    "        training_loss_logger.append(loss.item())\n",
    "    \n",
    "    # Quick save of the model every epoch\n",
    "    torch.save({'epoch': epoch + 1,\n",
    "                'data_logger': training_loss_logger,\n",
    "                'model_state_dict': tf_generator.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                 }, \"qa_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf7354",
   "metadata": {},
   "source": [
    "## Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger[10:])\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f7b97",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564fde22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of question and answer text from the test data loader\n",
    "q_text, a_text = next(iter(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an index within the batch\n",
    "index = 0\n",
    "\n",
    "# Print the question text at the chosen index\n",
    "print(\"Question:\")\n",
    "print(q_text[index])\n",
    "\n",
    "# Print the original answer text at the chosen index\n",
    "print(\"\\nOriginal Answer:\")\n",
    "print(a_text[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the initial prompt with the question text from the test data loader\n",
    "# init_prompt = [q_text[index]]\n",
    "init_prompt = [\"why did the chicken cross the road\"]\n",
    "\n",
    "# Tokenize the initial prompt question text\n",
    "input_q_tokens = q_tranform(init_prompt).to(device)\n",
    "\n",
    "# Add the Start-Of-Answer token to the prompt to signal the network to start generating the answer\n",
    "soa_token = 3 * torch.ones(1, 1).long()\n",
    "\n",
    "# Print the token indices of the question text\n",
    "print(\"\\nQuestion token indices:\")\n",
    "print(input_q_tokens)\n",
    "\n",
    "# Look up the token strings corresponding to the token indices\n",
    "token_strings = vocab.lookup_tokens(input_q_tokens[0].cpu().numpy())\n",
    "\n",
    "# Print the token strings\n",
    "print(\"\\nQuestion token strings:\")\n",
    "print(token_strings)\n",
    "\n",
    "# Concatenate the token strings to form the question string and perform formatting\n",
    "question = \"\".join(token_strings).replace(\"▁\", \" \").replace(\"<soq>\", \"\").replace(\"<eoq>\", \"\")\n",
    "\n",
    "# Print the formatted question string\n",
    "print(\"\\nQuestion String:\")\n",
    "print(question)\n",
    "\n",
    "# Set the temperature for sampling during generation\n",
    "temp = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_tokens = [soa_token]\n",
    "tf_generator.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode the input question tokens\n",
    "    encoded_seq = tf_generator.encoder(input_q_tokens.to(device))\n",
    "\n",
    "    # Generate the answer tokens\n",
    "    for i in range(100):\n",
    "        input_tokens = torch.cat(log_tokens, 1)\n",
    "        \n",
    "        # Decode the input tokens into the next predicted tokens\n",
    "        data_pred = tf_generator.decoder(input_tokens.to(device), encoded_seq)\n",
    "        \n",
    "        # Sample from the distribution of predicted probabilities\n",
    "        dist = Categorical(logits=data_pred[:, -1] / temp)\n",
    "        next_tokens = dist.sample().reshape(1, 1)\n",
    "        \n",
    "        # Append the next predicted token to the sequence\n",
    "        log_tokens.append(next_tokens.cpu())\n",
    "        \n",
    "        # Break the loop if the End-Of-Answer token is predicted\n",
    "        if next_tokens.item() == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ddad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of token indices to a tensor\n",
    "pred_text = torch.cat(log_tokens, 1)\n",
    "\n",
    "# Convert the token indices to their corresponding strings using the vocabulary\n",
    "pred_text_strings = vocab.lookup_tokens(pred_text[0].numpy())\n",
    "\n",
    "# Join the token strings to form the predicted text\n",
    "pred_text = \"\".join(pred_text_strings)\n",
    "\n",
    "# Print the predicted text\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origional Question\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765963d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special tokens and subword tokenization markers from the predicted text\n",
    "answer_out = pred_text.replace(\"▁\", \" \").replace(\"<unk>\", \"\").replace(\"<soa>\", \"\").replace(\"<eoa>\", \"\")\n",
    "print(answer_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
