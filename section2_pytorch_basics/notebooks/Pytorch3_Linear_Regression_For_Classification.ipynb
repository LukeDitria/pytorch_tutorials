{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Gradient Decent Revisit with Pytorch</h1> <br>\n",
    "<img src=\"../data/numpy_linear_regression.gif\" width=\"1200\" align=\"center\">\n",
    "Animation of our \"model\" at each step when training with gradient descent\n",
    "\n",
    "<b>With our new knowledge of Pytorch let's create a new implementation of gradient decent!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading the data </h3>\n",
    "Lets load some \"toy\" data that we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can load your data using this cell\n",
    "npzfile = np.load(\"../data/toy_data_two_moon.npz\") # toy_data.npz or toy_data_two_circles.npz\n",
    "\n",
    "#The compressed Numpy file is split up into 4 parts\n",
    "#Lets convert them to Pytorch Float Tensors\n",
    "#Train inputs and target outputs\n",
    "X_train = torch.FloatTensor(npzfile['arr_0'])\n",
    "y_train = torch.FloatTensor(npzfile['arr_2'])\n",
    "#Test inputs and target outputs\n",
    "X_test = torch.FloatTensor(npzfile['arr_1'])\n",
    "y_test = torch.FloatTensor(npzfile['arr_3'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Let's plot our data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see what the data looks like\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=y_train[:,0], s=25, edgecolor='k')\n",
    "plt.title(\"Train data\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=y_test[:,0], s=25, edgecolor='k')\n",
    "plt.title(\"Test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is randomly sampled from an odd looking distribution, the colour of the dots (as represented by y_test[:,0], a one or zero) is what the output of our model SHOULD be (aka the \"Ground Truth Data\"). Note that each data point is a vector of two values (the \"x and y\" values), therefore there will only be two parameters in our linear model. <br>\n",
    "<b>Note we do NOT need to add ones to our data for a bias term as Pytorch's Linear layer has a bias term by default</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training a model with GD </h2>\n",
    "In doing so, we need a function to <br>\n",
    "1- compute the loss with respect to the inputs and the parameters of the model <br>\n",
    "2- compute the gradient of the model with respect to its parameters $\\theta$\n",
    "\n",
    "We recall the loss of the linear regression as\n",
    "\\begin{align}\n",
    "L(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\|\\theta^\\top \\boldsymbol{x}_i - y_i\\|^2\n",
    "\\end{align}\n",
    "\n",
    "Now it is easy to see that\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{m} \\sum_{i=1}^m 2(\\theta^\\top \\boldsymbol{x}_i - y_i)\\boldsymbol{x}_i\n",
    "\\end{align}\n",
    "\n",
    "<b>Instead of calculating the gradient by hand, we'll just use Pytorch's auto-grad!!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our linear model - 2 inputs, 1 output (bias is included in linear layer)\n",
    "linear = ##To Do##\n",
    "#Define our loss function - MSE\n",
    "loss_function = ##To Do##\n",
    "#Create our optimizer - lr = 0.1\n",
    "optimizer = ##To Do##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can perform multiple itteration of GD to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of times we itterate over the dataset\n",
    "max_epoch = ##To Do##\n",
    "\n",
    "loss_log = [] #keep track of the loss values\n",
    "acc = [] #keep track of the accuracy \n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    #Perform a training step\n",
    "    ######To Do######\n",
    "    y_train_hat = ##To Do##\n",
    "    loss_log.append(loss.item())\n",
    "    \n",
    "    #Perform a test step\n",
    "    ######To Do######\n",
    "    y_test_hat = ##To Do##\n",
    "    acc.append(##To Do##)\n",
    "    \n",
    "print(\"Accuracy of linear model(GD): %.2f%% \" %(acc[-1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc)\n",
    "plt.title(\"Model accuracy per itteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
