{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c131e741",
   "metadata": {},
   "source": [
    "# Text Generation, Many-to-Many a Different way...\n",
    "What if we want to generate a sentence based on a prompt which is another sentence? We will need to first encode the input sequence and then train our model to produce the target sentence sequentially. This is common in question-answering type tasks where we want our network to \"respond\" to a given question!\n",
    "\n",
    "[<img src=\"https://static.packt-cdn.com/products/9781789346640/graphics/assets/79db1776-f471-4fe6-89b0-67cbae844bfc.png\">](LSTM)\n",
    "<br>\n",
    "[Corresponding Tutorial Video](https://youtu.be/uKgidS3DYcU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from torchtext.datasets import YahooAnswers\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchtext.data.functional import sentencepiece_tokenizer, load_sp_model\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d70b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "# Learning rate for model optimization\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Number of epochs for training\n",
    "nepochs = 10\n",
    "\n",
    "# Batch size for training data loader\n",
    "batch_size = 32\n",
    "\n",
    "# Maximum length of input questions\n",
    "max_len_q = 32\n",
    "\n",
    "# Maximum length of output answers\n",
    "max_len_a = 64\n",
    "\n",
    "# Root directory of the dataset\n",
    "data_set_root = \"../../datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc74a0",
   "metadata": {},
   "source": [
    "## Dataset, Tokenizers and Vocab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using the YahooAnswers Dataset\n",
    "# Note that for torchtext these datasets are NOT Pytorch dataset classes \"YahooAnswers\" is a function that\n",
    "# returns a Pytorch DataPipe!\n",
    "\n",
    "# Pytorch DataPipes vvv\n",
    "# https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "\n",
    "# vvv Good Blog on the difference between DataSet and DataPipe\n",
    "# https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58\n",
    "\n",
    "# Depending on the dataset sometimes the dataset doesn't download and gives an error\n",
    "# and you'll have to download and extract manually \n",
    "# \"The datasets supported by torchtext are datapipes from the torchdata project, which is still in Beta status\"\n",
    "\n",
    "# Un-comment to triger the DataPipe to download the data vvv\n",
    "# dataset_train = YahooAnswers(root=data_set_root, split=\"train\")\n",
    "# data = next(iter(dataset_train))\n",
    "\n",
    "# Side-Note I've noticed that the WikiText dataset is no longer able to be downloaded :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment to \"Train\" a Sentence Piece Tokenizer with the train data capping the vocab size to 20000 tokens\n",
    "# from torchtext.data.functional import generate_sp_model\n",
    "\n",
    "# with open(os.path.join(data_set_root, \"datasets/YahooAnswers/train.csv\")) as f:\n",
    "#     with open(os.path.join(data_set_root, \"datasets/YahooAnswers/data.txt\"), \"w\") as f2:\n",
    "#         for i, line in enumerate(f):\n",
    "#             text_only = \"\".join(line.split(\",\")[1:])\n",
    "#             filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # remove newline characters\n",
    "#             f2.write(filtered.lower() + \"\\n\")\n",
    "\n",
    "\n",
    "# generate_sp_model(os.path.join(data_set_root, \"datasets/YahooAnswers/data.txt\"), \n",
    "#                   vocab_size=20000, model_prefix='spm_user_ya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YahooQA dataset class definition\n",
    "class YahooQA(Dataset):\n",
    "    def __init__(self, num_datapoints, test_train=\"train\"):\n",
    "        # Read the Yahoo Answers dataset CSV file based on the test_train parameter (train or test)\n",
    "        self.df = pd.read_csv(os.path.join(data_set_root, \"datasets/YahooAnswers/\" + test_train + \".csv\"),\n",
    "                              names=[\"Class\", \"Q_Title\", \"Q_Content\", \"A\"])\n",
    "        \n",
    "        # Fill missing values with empty string\n",
    "        self.df.fillna('', inplace=True)\n",
    "        \n",
    "        # Combine Q_Title and Q_Content columns into a single Q column (question)\n",
    "        self.df['Q'] = self.df['Q_Title'] + ' ' + self.df['Q_Content']\n",
    "        \n",
    "        # Drop Q_Title and Q_Content columns as they are no longer needed\n",
    "        self.df.drop(['Q_Title', 'Q_Content'], axis=1, inplace=True)\n",
    "        \n",
    "        # Replace special characters with whitespace in the Q and A columns\n",
    "        self.df['Q'] = self.df['Q'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "        self.df['A'] = self.df['A'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "\n",
    "    # Method to get a single item (question, answer pair) from the dataset\n",
    "    def __getitem__(self, index):\n",
    "        # Get the question and answer texts at the given index, converted to lowercase\n",
    "        question_text = self.df.loc[index][\"Q\"].lower()\n",
    "        answer_text = self.df.loc[index][\"A\"].lower()\n",
    "\n",
    "        return question_text, answer_text\n",
    "\n",
    "    # Method to get the length of the dataset\n",
    "    def __len__(self):\n",
    "        # Return the total number of question-answer pairs in the dataset\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create YahooQA dataset instances for training and testing\n",
    "dataset_train = YahooQA(num_datapoints=data_set_root, test_train=\"train\")\n",
    "dataset_test = YahooQA(num_datapoints=data_set_root, test_train=\"test\")\n",
    "\n",
    "# Create data loaders for training and testing datasets\n",
    "# DataLoader for training dataset\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "# DataLoader for testing dataset\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the tokenizer\n",
    "# Load the SentencePiece model\n",
    "sp_model = load_sp_model(\"spm_user_ya.model\")\n",
    "\n",
    "# Create a tokenizer using the loaded model\n",
    "tokenizer = sentencepiece_tokenizer(sp_model)\n",
    "\n",
    "# Iterate over tokens generated by the tokenizer\n",
    "for token in tokenizer([\"i am creating\"]):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to yield tokens from a file\n",
    "def yield_tokens(file_path):\n",
    "    # Open the file in UTF-8 encoding\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        # Iterate over each line in the file\n",
    "        for line in f:\n",
    "            # Yield the token split by tab character\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "            \n",
    "# Build vocabulary from the iterator of tokens\n",
    "# We will also add \"special\" tokens that we'll use to signal something to our model\n",
    "# <pad> is a padding token that is added to the end of a sentence to ensure \n",
    "# the length of all sequences in a batch is the same\n",
    "# <soq> signals the \"Start-Of-Question\" aka the start of the Question sequence\n",
    "# <eoq> signals the \"End-Of-Question\" aka the end of the Question sequence\n",
    "# <soa> signals the \"Start-Of-Answer\" aka the start of the Answer sequence\n",
    "# <eoa> signals the \"End-Of-Answer\" aka the end of the Answer sequence\n",
    "# <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(\"spm_user_ya.vocab\"),\n",
    "    # Define special tokens with special_first=True to place them at the beginning of the vocabulary\n",
    "    specials=['<pad>', '<soq>', '<eoq>', '<soa>', '<eoa>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "# Set default index for out-of-vocabulary tokens\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72346ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation pipeline for questions\n",
    "q_transform = T.Sequential(\n",
    "    # Tokenize sentences using pre-existing SentencePiece tokenizer model\n",
    "    T.SentencePieceTokenizer(\"spm_user_ya.model\"),\n",
    "    # Convert tokens to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    # Add <sos> token at the beginning of each sentence (index 1 in vocabulary)\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentence if it is longer than the max question length\n",
    "    T.Truncate(max_seq_len=max_len_q),\n",
    "    # Add <eos> token at the end of each sentence (index 2 in vocabulary)\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor and pad sentences with the <pad> token if shorter than max length\n",
    "    T.ToTensor(padding_value=0)\n",
    ")\n",
    "\n",
    "# Define transformation pipeline for answers\n",
    "a_transform = T.Sequential(\n",
    "    # Tokenize sentences using pre-existing SentencePiece tokenizer model\n",
    "    T.SentencePieceTokenizer(\"spm_user_ya.model\"),\n",
    "    # Convert tokens to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    # Add <sos> token at the beginning of each sentence (index 3 in vocabulary)\n",
    "    T.AddToken(3, begin=True),\n",
    "    # Crop the sentence if it is longer than the max answer length\n",
    "    T.Truncate(max_seq_len=max_len_a),\n",
    "    # Add <eos> token at the end of each sentence (index 4 in vocabulary)\n",
    "    T.AddToken(4, begin=False),\n",
    "    # Convert the list of lists to a tensor and pad sentences with the <pad> token if shorter than max length\n",
    "    T.ToTensor(padding_value=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b5ace6",
   "metadata": {},
   "source": [
    "## Create LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ac2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model class\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_emb, num_layers=1, emb_size=128, hidden_size=128):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(num_emb, emb_size)\n",
    "\n",
    "        # MLP layer for embedding\n",
    "        self.mlp_emb = nn.Sequential(\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.25\n",
    "        )\n",
    "\n",
    "        # MLP layer for output\n",
    "        self.mlp_out = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size // 2, num_emb)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_seq, hidden_in, mem_in):\n",
    "        # Embed input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        # Pass through MLP for embedding\n",
    "        input_embs = self.mlp_emb(input_embs)\n",
    "                \n",
    "        # Pass through LSTM layer\n",
    "        output, (hidden_out, mem_out) = self.lstm(input_embs, (hidden_in, mem_in))\n",
    "                \n",
    "        # Pass through MLP for output\n",
    "        return self.mlp_out(output), hidden_out, mem_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a949a",
   "metadata": {},
   "source": [
    "## Initialise Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define embedding size, hidden size, and number of layers for the LSTM model\n",
    "emb_size = 256\n",
    "hidden_size = 1024\n",
    "num_layers = 4\n",
    "\n",
    "# Create LSTM model instance\n",
    "lstm_qa = LSTM(num_emb=len(vocab), num_layers=num_layers, \n",
    "               emb_size=emb_size, hidden_size=hidden_size).to(device)\n",
    "\n",
    "# Initialize optimizer with Adam optimizer\n",
    "optimizer = optim.Adam(lstm_qa.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Define the loss function (Cross Entropy Loss)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# List to store training loss during each epoch\n",
    "training_loss_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in lstm_qa.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66200d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd56a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):    \n",
    "    # Set LSTM model to training mode\n",
    "    lstm_qa.train()\n",
    "    steps = 0\n",
    "    # Iterate over batches in training data loader\n",
    "    for q_text, a_text in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        # Transform both question and answer text\n",
    "        q_text_tokens = q_transform(list(q_text)).to(device)\n",
    "        a_text_tokens = a_transform(list(a_text)).to(device)\n",
    "        \n",
    "        # Inputs and outputs for the answer next-token prediction\n",
    "        a_input_text = a_text_tokens[:, :-1]\n",
    "        a_output_text = a_text_tokens[:, 1:]\n",
    "        \n",
    "        # Batch size\n",
    "        bs = q_text_tokens.shape[0]\n",
    "        \n",
    "        # Initialise the memory buffers\n",
    "        hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "\n",
    "        # Encode the whole question sequence\n",
    "        _, hidden, memory = lstm_qa(q_text_tokens, hidden, memory)\n",
    "\n",
    "        # Perform a \"next-token\" prediction on the answer sequence\n",
    "        # providing the model with the memory buffers from the question-encoding step\n",
    "        pred, hidden, memory = lstm_qa(a_input_text, hidden, memory)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(pred.transpose(1, 2), a_output_text)\n",
    "\n",
    "        # Zero gradients, perform backward pass, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log training loss\n",
    "        training_loss_logger.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e38ff58",
   "metadata": {},
   "source": [
    "## Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f645a9c",
   "metadata": {},
   "source": [
    "## Generate some text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a question and its corresponding answer from the test dataset\n",
    "q_text, a_text = next(iter(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an index from the test data loader\n",
    "index = 0\n",
    "\n",
    "# Display the selected question\n",
    "print(\"QUESTION:\")\n",
    "print(q_text[index])\n",
    "\n",
    "# Initialize the prompt with the selected question\n",
    "init_prompt = [q_text[index]]\n",
    "\n",
    "# Transform the initial prompt into tokens and move to device\n",
    "input_tokens = q_transform(init_prompt).to(device)\n",
    "\n",
    "# Add Start-Of-Answer token to prompt the network to start generating the answer\n",
    "input_tokens = torch.cat((input_tokens, 3 * torch.ones(1, 1, device=device).long()), 1)\n",
    "\n",
    "print(\"\\nINITIAL PROMPT TOKENS:\")\n",
    "print(input_tokens)\n",
    "print(\"VOCABULARY TOKENS:\")\n",
    "print(vocab.lookup_tokens(input_tokens[0].cpu().numpy()))\n",
    "\n",
    "# Temperature parameter for sampling\n",
    "temp = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text tokens\n",
    "log_tokens = []\n",
    "# Set LSTM model to evaluation mode\n",
    "lstm_qa.eval()\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():    \n",
    "    # Initialize hidden and memory tensors\n",
    "    hidden = torch.zeros(num_layers, 1, hidden_size, device=device)\n",
    "    memory = torch.zeros(num_layers, 1, hidden_size, device=device)\n",
    "    \n",
    "    # Iterate over a maximum of 100 tokens\n",
    "    for i in range(100):\n",
    "        # Forward pass through LSTM model\n",
    "        data_pred, hidden, memory = lstm_qa(input_tokens, hidden, memory)\n",
    "        \n",
    "        # Sample from the distribution of probabilities\n",
    "        dist = Categorical(logits=data_pred[:, -1, :]/temp)\n",
    "        input_tokens = dist.sample().reshape(1, 1)\n",
    "        \n",
    "        # Append sampled token to log_tokens list\n",
    "        log_tokens.append(input_tokens.cpu())\n",
    "        \n",
    "        # Check if the sampled token is the End-Of-Answer token\n",
    "        if input_tokens.item() == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ddad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of token indices into text using the vocabulary\n",
    "pred_text = \"\".join(vocab.lookup_tokens(torch.cat(log_tokens, 1)[0].numpy()))\n",
    "\n",
    "# Print the generated text\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765963d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the generated text by replacing special tokens and removing unwanted characters\n",
    "cleaned_text = pred_text.replace(\"‚ñÅ\", \" \").replace(\"<unk>\", \"\").replace(\"<eoa>\", \"\")\n",
    "\n",
    "# Print the cleaned text\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the next token probabilities \n",
    "plt.plot(F.softmax(data_pred[:, -1, :]/temp, -1).cpu().numpy().flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
